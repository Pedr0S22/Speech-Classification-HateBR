{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6b1b33d",
   "metadata": {},
   "source": [
    "# PLN Project 3 - LLM Prompt Engineering for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e19afa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package Importation\n",
    "\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1e7d67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Dataset ---\n",
      "Train size: 5600\n",
      "Test size: 1400\n"
     ]
    }
   ],
   "source": [
    "# Data Loading & Splitting\n",
    "\n",
    "print(\"--- Loading Dataset ---\")\n",
    "url = \"https://raw.githubusercontent.com/franciellevargas/HateBR/refs/heads/main/dataset/HateBR.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Map labels for clarity\n",
    "df['label_text'] = df['label_final'].map({0: 'Não ofensivo', 1: 'Ofensivo'})\n",
    "\n",
    "X = df['comentario']\n",
    "y = df['label_final']\n",
    "\n",
    "# 80% Train, 20% Test split, random_state=42\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "train_df = pd.DataFrame({'text': X_train, 'label': y_train, 'label_text': y_train.map({0: 'Não ofensivo', 1: 'Ofensivo'})})\n",
    "test_df = pd.DataFrame({'text': X_test, 'label': y_test, 'label_text': y_test.map({0: 'Não ofensivo', 1: 'Ofensivo'})})\n",
    "\n",
    "print(f\"Train size: {len(train_df)}\")\n",
    "print(f\"Test size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee7a1ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading & Loading Qwen2.5 GPU Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56f0ee61a864f6b8001b96e6da55ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Qwen2.5-7B-Instruct-Q4_K_M.gguf:   0%|          | 0.00/4.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes\n",
      "llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 4060 Laptop GPU) - 7099 MiB free\n",
      "llama_model_loader: loaded meta data with 38 key-value pairs and 339 tensors from models\\Qwen2.5-7B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2.5 7B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Qwen2.5\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/Qwen/Qwen2.5-7...\n",
      "llama_model_loader: - kv   8:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   9:                  general.base_model.0.name str              = Qwen2.5 7B\n",
      "llama_model_loader: - kv  10:          general.base_model.0.organization str              = Qwen\n",
      "llama_model_loader: - kv  11:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-7B\n",
      "llama_model_loader: - kv  12:                               general.tags arr[str,2]       = [\"chat\", \"text-generation\"]\n",
      "llama_model_loader: - kv  13:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  14:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv  15:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv  16:                     qwen2.embedding_length u32              = 3584\n",
      "llama_model_loader: - kv  17:                  qwen2.feed_forward_length u32              = 18944\n",
      "llama_model_loader: - kv  18:                 qwen2.attention.head_count u32              = 28\n",
      "llama_model_loader: - kv  19:              qwen2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  20:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  21:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  22:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  34:                      quantize.imatrix.file str              = /models_out/Qwen2.5-7B-Instruct-GGUF/...\n",
      "llama_model_loader: - kv  35:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  36:             quantize.imatrix.entries_count i32              = 196\n",
      "llama_model_loader: - kv  37:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type q4_K:  169 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151649 '<|box_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151648 '<|box_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151644 '<|im_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 22\n",
      "llm_load_vocab: token to piece cache size = 0.9310 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 152064\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 3584\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 28\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 7\n",
      "llm_load_print_meta: n_embd_k_gqa     = 512\n",
      "llm_load_print_meta: n_embd_v_gqa     = 512\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 18944\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.62 B\n",
      "llm_load_print_meta: model size       = 4.36 GiB (4.91 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen2.5 7B Instruct\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
      "llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
      "llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'\n",
      "llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'\n",
      "llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'\n",
      "llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'\n",
      "llm_load_print_meta: EOG token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOG token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'\n",
      "llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'\n",
      "llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading output layer to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CUDA0 model buffer size =  4168.09 MiB\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =   292.36 MiB\n",
      ".................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 2048\n",
      "llama_new_context_with_model: n_ctx_per_seq = 2048\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 1000000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   112.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  112.00 MiB, K (f16):   56.00 MiB, V (f16):   56.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.58 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   304.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    11.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 986\n",
      "llama_new_context_with_model: graph splits = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2.5 loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Qwen2.5 7B Instruct', 'general.architecture': 'qwen2', 'general.type': 'model', 'general.basename': 'Qwen2.5', 'general.finetune': 'Instruct', 'general.size_label': '7B', 'general.license': 'apache-2.0', 'qwen2.attention.head_count_kv': '4', 'general.license.link': 'https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/blob/main/LICENSE', 'general.base_model.count': '1', 'general.base_model.0.name': 'Qwen2.5 7B', 'general.base_model.0.organization': 'Qwen', 'general.base_model.0.repo_url': 'https://huggingface.co/Qwen/Qwen2.5-7B', 'qwen2.block_count': '28', 'qwen2.context_length': '32768', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'qwen2.embedding_length': '3584', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '151643', 'qwen2.feed_forward_length': '18944', 'qwen2.attention.head_count': '28', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.eos_token_id': '151645', 'general.file_type': '15', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'qwen2', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'quantize.imatrix.chunks_count': '128', 'quantize.imatrix.file': '/models_out/Qwen2.5-7B-Instruct-GGUF/Qwen2.5-7B-Instruct.imatrix', 'quantize.imatrix.entries_count': '196'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- messages[0]['content'] }}\n",
      "    {%- else %}\n",
      "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
      "    {%- endif %}\n",
      "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0]['role'] == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
      "    {%- else %}\n",
      "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {{- '<|im_start|>' + message.role }}\n",
      "        {%- if message.content %}\n",
      "            {{- '\\n' + message.content }}\n",
      "        {%- endif %}\n",
      "        {%- for tool_call in message.tool_calls %}\n",
      "            {%- if tool_call.function is defined %}\n",
      "                {%- set tool_call = tool_call.function %}\n",
      "            {%- endif %}\n",
      "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
      "            {{- tool_call.name }}\n",
      "            {{- '\", \"arguments\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- '}\\n</tool_call>' }}\n",
      "        {%- endfor %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- message.content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Load Model Qwen2.5 Intructions 7B\n",
    "print(\"\\nDownloading & Loading Qwen2.5 GPU Model\")\n",
    "\n",
    "model_folder = \"models\"\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/Qwen2.5-7B-Instruct-GGUF\",\n",
    "    filename=\"Qwen2.5-7B-Instruct-Q4_K_M.gguf\",\n",
    "    local_dir=model_folder,\n",
    "    force_download=True\n",
    "    )\n",
    "\n",
    "qwen = Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=2048,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Qwen2.5 loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75dbe9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading & Loading Llama 3.1 GPU Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4060 Laptop GPU, compute capability 8.9, VMM: yes\n",
      "llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 4060 Laptop GPU) - 7099 MiB free\n",
      "llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from models\\Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\n",
      "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "llm_load_vocab: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
      "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: tensor 'token_embd.weight' (q4_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading output layer to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CUDA0 model buffer size =  4403.49 MiB\n",
      "llm_load_tensors:   CPU_Mapped model buffer size =   281.81 MiB\n",
      ".......................................................................................\n",
      "llama_new_context_with_model: n_seq_max     = 1\n",
      "llama_new_context_with_model: n_ctx         = 2048\n",
      "llama_new_context_with_model: n_ctx_per_seq = 2048\n",
      "llama_new_context_with_model: n_batch       = 512\n",
      "llama_new_context_with_model: n_ubatch      = 512\n",
      "llama_new_context_with_model: flash_attn    = 0\n",
      "llama_new_context_with_model: freq_base     = 500000.0\n",
      "llama_new_context_with_model: freq_scale    = 1\n",
      "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   258.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    12.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Meta Llama 3.1 8B Instruct', 'general.architecture': 'llama', 'general.type': 'model', 'llama.block_count': '32', 'general.basename': 'Meta-Llama-3.1', 'general.finetune': 'Instruct', 'general.size_label': '8B', 'general.license': 'llama3.1', 'llama.context_length': '131072', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.rope.freq_base': '500000.000000', 'quantize.imatrix.entries_count': '224', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.pre': 'llama-bpe', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'quantize.imatrix.chunks_count': '125', 'quantize.imatrix.file': '/models_out/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.imatrix', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 3.1 loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load Model Llama3.1 Instruct 8B\n",
    "print(\"\\nDownloading & Loading Llama 3.1 GPU Model\")\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\",\n",
    "    filename=\"Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\",\n",
    "    local_dir=\"models\",\n",
    ")\n",
    "\n",
    "# Llama initiallization with GPU offloading\n",
    "llama = Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=2048,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Llama 3.1 loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4478f8",
   "metadata": {},
   "source": [
    "## Prompt Engineering Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6abefcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm(prompt, llm):\n",
    "    \"\"\"\n",
    "    Sends prompt using Qwen's ChatML format via llama.cpp\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that classifies text in Portuguese.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    output = llm.create_chat_completion(\n",
    "        messages=messages,\n",
    "        max_tokens=512,\n",
    "        temperature=0.05,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    return output['choices'][0]['message']['content'].strip()\n",
    "\n",
    "PATTERN_MAIN = re.compile(r'classificação:\\s*[*\"\\']*(não ofensivo|ofensivo)')\n",
    "PATTERN_FALLBACK = re.compile(r'(não ofensivo|ofensivo)[*\"\\'\\.]*$')\n",
    "\n",
    "def parse_response(response):\n",
    "    response = response.lower().strip()\n",
    "    \n",
    "    match = PATTERN_MAIN.search(response)\n",
    "    if match:\n",
    "        return 0 if match.group(1) == \"não ofensivo\" else 1\n",
    "\n",
    "    fallback_match = PATTERN_FALLBACK.search(response)\n",
    "    if fallback_match:\n",
    "        return 0 if fallback_match.group(1) == \"não ofensivo\" else 1\n",
    "        \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5e9c09",
   "metadata": {},
   "source": [
    "### Zero Shot Learning Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02c1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zero_shot_prompt(text):\n",
    "    return f\"\"\"A tarefa é classificar o seguinte comentário como 'Ofensivo' ou 'Não ofensivo'.\n",
    "Responda apenas com a classe correta, sem explicações.\n",
    "\n",
    "Comentário: \"{text}\"\n",
    "Classificação:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcd0f24",
   "metadata": {},
   "source": [
    "### Few Shot Learning Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c9a197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_few_shot_prompt(target_text, train_dataset):\n",
    "    \n",
    "    # Selected comments\n",
    "    selected_ids = [60, 6758, 6398, 1041]\n",
    "\n",
    "    # Filtering the dataset to get only the specific rows matching IDs\n",
    "    samples = train_dataset[train_dataset.index.isin(selected_ids)].copy()\n",
    "    \n",
    "    # order is mixed to avoid pattern bias\n",
    "    samples = samples.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    prompt = \"Classifique os comentários abaixo como 'Ofensivo' ou 'Não ofensivo'. Responda apenas com a classe correta, sem explicações.\\n\\n\"\n",
    "    \n",
    "    for _, row in samples.iterrows():\n",
    "        prompt += f\"Comentário: \\\"{row['text']}\\\"\\nClassificação: {row['label_text']}\\n\\n\"\n",
    "        \n",
    "    prompt += f\"Comentário: \\\"{target_text}\\\"\\nClassificação:\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a555d6",
   "metadata": {},
   "source": [
    "### Chain of Thoughts with Zero Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6b16d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cot_prompt_zs(target_text):\n",
    "    return f\"\"\"Analise o comentário passo a passo para determinar se é ofensivo.\n",
    "A última frase deve conter a classificação 'Ofensivo' ou 'Não Ofensivo'.\n",
    "\n",
    "Comentário: {target_text}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacd5daf",
   "metadata": {},
   "source": [
    "### Chain of Thoughts with Few Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a9260e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cot_prompt_fs(target_text):\n",
    "    return f\"\"\"Analise o comentário passo a passo para determinar se é ofensivo. A última frase deve conter a classificação 'Ofensivo' ou 'Não Ofensivo'.\n",
    "    \n",
    "Exemplo 1:\n",
    "Comentário: \"Falsa!!!! Que nem nota de $30!!!!🤪🤪🤪\"\n",
    "Raciocínio: O comentário acusa a pessoa de ser \"Falsa\" (um insulto ao caráter) e compara essa falsidade a uma \"nota de $30\" (algo que não existe e seria uma fraude óbvia).\n",
    "A intenção é insultar, ridicularizar e desqualificar o alvo de forma intensa (reforçada pelos pontos de exclamação e emojis de escárnio).\n",
    "Classificação: Ofensivo.\n",
    "\n",
    "Exemplo 2:\n",
    "Comentário: \"Na verdade querem aumentar para 28%.\"\n",
    "Raciocínio: O seu comentário é uma declaração factual que expressa uma informação ou crença sobre uma proposta de aumento percentual (possivelmente de imposto, taxa ou participação acionária, dependendo do contexto anterior).\n",
    "Classificação: Não Ofensivo.\n",
    "\n",
    "Exemplo 3:\n",
    "Comentário: \"Indignação PT financiar candidatura de prefeita bolsonarista em Cabo Frio . Uma atitude incompatível com essa foto .\"\n",
    "Raciocínio: O comentário é uma crítica política e a expressão de uma opinião negativa (indignação) sobre a estratégia de um partido, sem utilizar linguagem ofensiva, caluniosa ou vulgar dirigida a pessoas.\n",
    "Classificação: Não Ofensivo\n",
    "\n",
    "Exemplo 4:\n",
    "Comentário: \"lenantemo-nos todos e saiamos dessa emissora e desse devasso\"\n",
    "Raciocínio: O comentário contém um insulto direto e depreciativo (\"devasso\") que ataca a moral e o caráter de uma pessoa específica.\n",
    "Classificação: Ofensivo\n",
    "\n",
    "Sua vez:\n",
    "Comentário: \"{target_text}\"\n",
    "Raciocínio:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f80fcce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1400 [00:00<?, ?it/s]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.70 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.16 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   221 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5469.96 ms /   282 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    51 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1517.75 ms /   506 tokens\n",
      "  0%|          | 1/1400 [00:07<2:57:34,  7.62s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.13 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.54 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   196 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4823.22 ms /   260 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2553.94 ms /   551 tokens\n",
      "  0%|          | 2/1400 [00:15<2:59:26,  7.70s/it]Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.61 ms /    92 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   216 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.47 ms /   220 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   258 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6397.86 ms /   342 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 479 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   479 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   128 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3470.17 ms /   607 tokens\n",
      "  0%|          | 3/1400 [00:25<3:26:39,  8.88s/it]Llama.generate: 42 prefix-match hit, remaining 142 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   142 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.42 ms /   145 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 270 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   270 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     218.88 ms /   273 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   138 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   356 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8962.39 ms /   494 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 533 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   533 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   222 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5815.88 ms /   755 tokens\n",
      "  0%|          | 4/1400 [00:40<4:24:34, 11.37s/it]Llama.generate: 42 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   117 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     149.32 ms /   120 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 245 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   245 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.03 ms /   248 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   113 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   295 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7379.77 ms /   408 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 508 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   508 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   186 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4908.34 ms /   694 tokens\n",
      "  0%|          | 5/1400 [00:53<4:35:13, 11.84s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     127.70 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.23 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   239 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5888.38 ms /   315 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   262 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6768.40 ms /   733 tokens\n",
      "  0%|          | 6/1400 [01:06<4:44:11, 12.23s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.65 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.10 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   318 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7871.55 ms /   394 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   136 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3645.52 ms /   607 tokens\n",
      "  0%|          | 7/1400 [01:18<4:41:31, 12.13s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     122.88 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.61 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   274 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6710.20 ms /   331 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   197 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5093.01 ms /   649 tokens\n",
      "  1%|          | 8/1400 [01:30<4:41:21, 12.13s/it]Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     134.22 ms /    92 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.06 ms /   220 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   325 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8074.97 ms /   410 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 480 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   480 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   156 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4105.27 ms /   636 tokens\n",
      "  1%|          | 9/1400 [01:43<4:44:06, 12.25s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     123.51 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.88 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   332 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8165.88 ms /   392 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   186 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4830.25 ms /   641 tokens\n",
      "  1%|          | 10/1400 [01:56<4:51:33, 12.58s/it]Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     135.57 ms /    92 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.98 ms /   220 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   257 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6365.60 ms /   342 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 480 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   480 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   273 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7044.18 ms /   753 tokens\n",
      "  1%|          | 11/1400 [02:10<4:59:43, 12.95s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.38 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.60 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   206 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5050.42 ms /   279 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3013.53 ms /   581 tokens\n",
      "  1%|          | 12/1400 [02:18<4:27:52, 11.58s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.59 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.01 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   321 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7945.40 ms /   384 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   221 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5708.17 ms /   679 tokens\n",
      "  1%|          | 13/1400 [02:32<4:44:43, 12.32s/it]Llama.generate: 42 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     137.36 ms /   106 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 231 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   231 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.38 ms /   234 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   365 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9108.79 ms /   465 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 494 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   494 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   247 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6381.75 ms /   741 tokens\n",
      "  1%|          | 14/1400 [02:48<5:09:12, 13.39s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.54 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.70 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5333.76 ms /   285 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   147 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3869.03 ms /   608 tokens\n",
      "  1%|          | 15/1400 [02:58<4:42:34, 12.24s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     128.08 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.10 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   265 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6509.51 ms /   331 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   198 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5133.91 ms /   658 tokens\n",
      "  1%|          | 16/1400 [03:10<4:40:35, 12.16s/it]Llama.generate: 42 prefix-match hit, remaining 109 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   109 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.80 ms /   113 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 237 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   237 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     226.36 ms /   241 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   105 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   268 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6619.55 ms /   373 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 500 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   500 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   158 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4157.03 ms /   658 tokens\n",
      "  1%|          | 17/1400 [03:21<4:33:40, 11.87s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.41 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.69 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   213 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5220.06 ms /   282 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2321.61 ms /   547 tokens\n",
      "  1%|▏         | 18/1400 [03:29<4:06:15, 10.69s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     127.24 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.32 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   256 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6260.20 ms /   312 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1890.31 ms /   518 tokens\n",
      "  1%|▏         | 19/1400 [03:37<3:50:46, 10.03s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     146.17 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.12 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   237 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5819.71 ms /   296 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   185 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4798.28 ms /   638 tokens\n",
      "  1%|▏         | 20/1400 [03:48<3:57:16, 10.32s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     151.20 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.41 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   200 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4870.03 ms /   260 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3010.52 ms /   568 tokens\n",
      "  2%|▏         | 21/1400 [03:56<3:43:00,  9.70s/it]Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.23 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   218 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.37 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   221 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5438.88 ms /   308 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 481 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   481 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3264.27 ms /   603 tokens\n",
      "  2%|▏         | 22/1400 [04:06<3:38:42,  9.52s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.69 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.23 ms /   216 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   272 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6681.88 ms /   352 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   147 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3878.79 ms /   622 tokens\n",
      "  2%|▏         | 23/1400 [04:17<3:48:28,  9.96s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     124.06 ms /    61 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.24 ms /   189 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   237 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5832.39 ms /   292 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2061.89 ms /   523 tokens\n",
      "  2%|▏         | 24/1400 [04:25<3:36:21,  9.43s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     148.88 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.53 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   209 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5086.07 ms /   265 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2453.21 ms /   541 tokens\n",
      "  2%|▏         | 25/1400 [04:33<3:25:48,  8.98s/it]Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     134.14 ms /    86 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   211 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.62 ms /   214 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   246 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6040.92 ms /   324 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 474 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   474 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   120 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3244.16 ms /   594 tokens\n",
      "  2%|▏         | 26/1400 [04:42<3:30:10,  9.18s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     131.79 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.00 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   227 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5549.77 ms /   290 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    78 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2169.56 ms /   535 tokens\n",
      "  2%|▏         | 27/1400 [04:50<3:22:19,  8.84s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     134.23 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.17 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   315 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7748.00 ms /   373 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2803.51 ms /   557 tokens\n",
      "  2%|▏         | 28/1400 [05:01<3:36:13,  9.46s/it]Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     136.74 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.29 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6636.89 ms /   357 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 482 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   482 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   237 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6191.05 ms /   719 tokens\n",
      "  2%|▏         | 29/1400 [05:14<4:01:37, 10.57s/it]Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     165.26 ms /   103 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   227 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.54 ms /   230 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   301 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7469.21 ms /   396 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 490 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   490 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   181 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4769.94 ms /   671 tokens\n",
      "  2%|▏         | 30/1400 [05:27<4:15:32, 11.19s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     149.02 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.86 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   341 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8476.08 ms /   400 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   111 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3001.38 ms /   565 tokens\n",
      "  2%|▏         | 31/1400 [05:39<4:19:42, 11.38s/it]Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     135.64 ms /    91 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   216 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.06 ms /   219 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   335 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8380.90 ms /   419 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 479 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   479 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   197 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5219.98 ms /   676 tokens\n",
      "  2%|▏         | 32/1400 [05:53<4:37:06, 12.15s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.09 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.20 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   182 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4469.29 ms /   238 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   129 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3452.45 ms /   580 tokens\n",
      "  2%|▏         | 33/1400 [06:01<4:10:37, 11.00s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.68 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.71 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   266 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6576.49 ms /   337 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   154 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4060.56 ms /   619 tokens\n",
      "  2%|▏         | 34/1400 [06:12<4:10:42, 11.01s/it]Llama.generate: 42 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   114 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     145.17 ms /   117 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 242 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   242 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.94 ms /   245 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   111 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   290 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7193.89 ms /   401 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 505 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   505 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5656.80 ms /   723 tokens\n",
      "  2%|▎         | 35/1400 [06:25<4:25:38, 11.68s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.60 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.77 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   305 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7523.85 ms /   385 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   215 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5568.28 ms /   690 tokens\n",
      "  3%|▎         | 36/1400 [06:39<4:37:45, 12.22s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     148.33 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.46 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   115 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2808.02 ms /   174 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2198.15 ms /   532 tokens\n",
      "  3%|▎         | 37/1400 [06:44<3:51:01, 10.17s/it]Llama.generate: 42 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    93 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     158.52 ms /    97 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   221 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.99 ms /   225 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   281 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6983.37 ms /   370 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 484 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   484 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   172 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4506.95 ms /   656 tokens\n",
      "  3%|▎         | 38/1400 [06:56<4:02:34, 10.69s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     141.08 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.97 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   257 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6313.47 ms /   330 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   180 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4705.59 ms /   648 tokens\n",
      "  3%|▎         | 39/1400 [07:08<4:07:03, 10.89s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.83 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.06 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   233 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5766.84 ms /   303 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2647.02 ms /   563 tokens\n",
      "  3%|▎         | 40/1400 [07:16<3:52:39, 10.26s/it]Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     124.53 ms /    60 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.39 ms /   188 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 53 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    53 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   259 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6349.36 ms /   312 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 448 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   448 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   109 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2899.11 ms /   557 tokens\n",
      "  3%|▎         | 41/1400 [07:26<3:47:51, 10.06s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.93 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.58 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   358 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8904.78 ms /   432 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   245 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6308.00 ms /   714 tokens\n",
      "  3%|▎         | 42/1400 [07:42<4:25:13, 11.72s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.58 ms /    86 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.66 ms /   214 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   206 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5047.00 ms /   284 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   155 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4108.83 ms /   628 tokens\n",
      "  3%|▎         | 43/1400 [07:51<4:10:22, 11.07s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     129.18 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.56 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   277 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6804.31 ms /   352 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2100.44 ms /   545 tokens\n",
      "  3%|▎         | 44/1400 [08:00<3:57:49, 10.52s/it]Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     149.75 ms /    61 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.34 ms /   189 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 53 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    53 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   206 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5026.23 ms /   259 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 448 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   448 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2083.24 ms /   522 tokens\n",
      "  3%|▎         | 45/1400 [08:08<3:37:05,  9.61s/it]Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     134.86 ms /    95 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.96 ms /   223 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6286.85 ms /   344 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 483 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   483 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   211 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5479.95 ms /   694 tokens\n",
      "  3%|▎         | 46/1400 [08:20<3:53:54, 10.36s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     151.92 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.10 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   252 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6181.21 ms /   322 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5678.13 ms /   683 tokens\n",
      "  3%|▎         | 47/1400 [08:32<4:06:18, 10.92s/it]Llama.generate: 42 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   116 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     148.97 ms /   119 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 244 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   244 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.22 ms /   247 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   113 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   253 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6255.02 ms /   366 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 507 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   507 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   219 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5689.08 ms /   726 tokens\n",
      "  3%|▎         | 48/1400 [08:44<4:15:34, 11.34s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     130.71 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.88 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5601.49 ms /   291 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    95 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2638.89 ms /   551 tokens\n",
      "  4%|▎         | 49/1400 [08:53<3:56:44, 10.51s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     122.24 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.23 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   199 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4860.95 ms /   255 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2108.97 ms /   527 tokens\n",
      "  4%|▎         | 50/1400 [09:00<3:34:51,  9.55s/it]Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     158.43 ms /    96 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.54 ms /   223 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   312 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7712.69 ms /   401 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 483 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   483 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   173 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4558.01 ms /   656 tokens\n",
      "  4%|▎         | 51/1400 [09:13<3:55:35, 10.48s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     131.74 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.81 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   354 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8779.87 ms /   425 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   186 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4843.56 ms /   651 tokens\n",
      "  4%|▎         | 52/1400 [09:27<4:18:56, 11.53s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.26 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.45 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   263 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6493.80 ms /   339 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   117 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3148.81 ms /   588 tokens\n",
      "  4%|▍         | 53/1400 [09:37<4:08:41, 11.08s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     127.48 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.17 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   288 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7073.01 ms /   350 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   188 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4899.84 ms /   645 tokens\n",
      "  4%|▍         | 54/1400 [09:49<4:16:50, 11.45s/it]Llama.generate: 42 prefix-match hit, remaining 147 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   147 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.23 ms /   150 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 275 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   275 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     227.66 ms /   278 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 143 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   143 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   286 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7149.28 ms /   429 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 538 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   538 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   306 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7924.46 ms /   844 tokens\n",
      "  4%|▍         | 55/1400 [10:05<4:43:51, 12.66s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     124.17 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.73 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   279 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6888.97 ms /   336 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2416.22 ms /   541 tokens\n",
      "  4%|▍         | 56/1400 [10:14<4:23:18, 11.75s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.50 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.27 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   265 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6496.71 ms /   326 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2571.80 ms /   550 tokens\n",
      "  4%|▍         | 57/1400 [10:24<4:07:41, 11.07s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.78 ms /    89 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     215.41 ms /   217 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   258 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6388.63 ms /   339 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   231 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5962.11 ms /   707 tokens\n",
      "  4%|▍         | 58/1400 [10:37<4:18:51, 11.57s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     124.84 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.50 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   265 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6500.10 ms /   326 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   125 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3351.84 ms /   580 tokens\n",
      "  4%|▍         | 59/1400 [10:47<4:09:21, 11.16s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     143.54 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.69 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   317 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7822.39 ms /   388 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   149 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3932.96 ms /   615 tokens\n",
      "  4%|▍         | 60/1400 [10:59<4:15:33, 11.44s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     128.23 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.34 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   270 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6647.85 ms /   337 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   145 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3819.93 ms /   607 tokens\n",
      "  4%|▍         | 61/1400 [11:10<4:11:07, 11.25s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.68 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.09 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   286 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7033.60 ms /   351 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   155 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4086.66 ms /   615 tokens\n",
      "  4%|▍         | 62/1400 [11:21<4:12:31, 11.32s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.91 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.31 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   173 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4216.42 ms /   227 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2449.27 ms /   539 tokens\n",
      "  4%|▍         | 63/1400 [11:28<3:43:45, 10.04s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.50 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.86 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   221 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5412.73 ms /   290 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2507.47 ms /   554 tokens\n",
      "  5%|▍         | 64/1400 [11:37<3:32:00,  9.52s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     130.40 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.05 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   262 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6438.24 ms /   333 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   132 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3507.40 ms /   598 tokens\n",
      "  5%|▍         | 65/1400 [11:47<3:36:58,  9.75s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     130.93 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.32 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   329 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8142.71 ms /   400 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   270 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6972.85 ms /   736 tokens\n",
      "  5%|▍         | 66/1400 [12:02<4:14:52, 11.46s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     135.55 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.91 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   235 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5753.00 ms /   291 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   121 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3244.22 ms /   572 tokens\n",
      "  5%|▍         | 67/1400 [12:12<4:00:30, 10.83s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     147.74 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.35 ms /   216 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   309 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7682.76 ms /   391 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   282 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7256.59 ms /   758 tokens\n",
      "  5%|▍         | 68/1400 [12:27<4:30:07, 12.17s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.05 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.00 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   188 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4610.53 ms /   258 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2689.88 ms /   562 tokens\n",
      "  5%|▍         | 69/1400 [12:35<4:00:07, 10.82s/it]Llama.generate: 42 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   104 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.73 ms /   108 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 232 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   232 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     224.63 ms /   236 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   320 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7934.72 ms /   420 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 495 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   495 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   194 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5051.36 ms /   689 tokens\n",
      "  5%|▌         | 70/1400 [12:48<4:17:05, 11.60s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.63 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.46 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   213 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5233.91 ms /   269 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2451.87 ms /   541 tokens\n",
      "  5%|▌         | 71/1400 [12:56<3:53:15, 10.53s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     132.60 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.91 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7347.82 ms /   360 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   219 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5692.67 ms /   674 tokens\n",
      "  5%|▌         | 72/1400 [13:10<4:11:57, 11.38s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.28 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.61 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   261 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6408.15 ms /   327 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2686.32 ms /   561 tokens\n",
      "  5%|▌         | 73/1400 [13:19<3:59:08, 10.81s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.91 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.28 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   424 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10564.42 ms /   496 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   165 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4313.02 ms /   631 tokens\n",
      "  5%|▌         | 74/1400 [13:34<4:28:31, 12.15s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     148.10 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.18 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   291 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7143.21 ms /   351 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2734.09 ms /   556 tokens\n",
      "  5%|▌         | 75/1400 [13:44<4:15:36, 11.57s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.75 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.08 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   321 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7897.59 ms /   379 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2243.26 ms /   534 tokens\n",
      "  5%|▌         | 76/1400 [13:55<4:08:16, 11.25s/it]Llama.generate: 42 prefix-match hit, remaining 137 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   137 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.01 ms /   141 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 265 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   265 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     241.08 ms /   269 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   134 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   271 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6773.99 ms /   405 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 528 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   528 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   193 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5080.68 ms /   721 tokens\n",
      "  6%|▌         | 77/1400 [14:07<4:15:08, 11.57s/it]Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.40 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 326 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   326 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     237.95 ms /   329 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   346 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8697.58 ms /   541 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 589 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   589 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7938.15 ms /   892 tokens\n",
      "  6%|▌         | 78/1400 [14:24<4:51:27, 13.23s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     127.94 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.94 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   356 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8807.58 ms /   418 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   117 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3138.25 ms /   574 tokens\n",
      "  6%|▌         | 79/1400 [14:37<4:44:59, 12.94s/it]Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.85 ms /    99 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.95 ms /   226 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   330 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8208.19 ms /   421 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   486 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6117.06 ms /   722 tokens\n",
      "  6%|▌         | 80/1400 [14:51<4:56:22, 13.47s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.36 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.69 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   271 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6662.25 ms /   335 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   114 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3054.55 ms /   574 tokens\n",
      "  6%|▌         | 81/1400 [15:02<4:34:01, 12.46s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.57 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.12 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   215 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5269.91 ms /   277 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    93 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2545.46 ms /   550 tokens\n",
      "  6%|▌         | 82/1400 [15:10<4:05:59, 11.20s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     156.21 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.29 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   267 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6571.62 ms /   327 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2790.10 ms /   558 tokens\n",
      "  6%|▌         | 83/1400 [15:19<3:56:04, 10.76s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     158.66 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.01 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   245 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6003.02 ms /   305 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   176 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4590.18 ms /   631 tokens\n",
      "  6%|▌         | 84/1400 [15:30<3:57:11, 10.81s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.91 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.48 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   268 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6584.55 ms /   328 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   105 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2837.32 ms /   560 tokens\n",
      "  6%|▌         | 85/1400 [15:40<3:50:30, 10.52s/it]Llama.generate: 42 prefix-match hit, remaining 251 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   251 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.94 ms /   254 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 379 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   379 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     270.01 ms /   382 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 248 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   248 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   311 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7867.72 ms /   559 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 642 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   642 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   396 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10338.36 ms /  1038 tokens\n",
      "  6%|▌         | 86/1400 [15:59<4:44:13, 12.98s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     131.70 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.87 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   287 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7049.65 ms /   342 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   111 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3013.85 ms /   561 tokens\n",
      "  6%|▌         | 87/1400 [16:09<4:27:04, 12.20s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     134.27 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.52 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   289 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7176.57 ms /   367 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   192 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5000.91 ms /   664 tokens\n",
      "  6%|▋         | 88/1400 [16:22<4:28:58, 12.30s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.66 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.98 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   301 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7447.17 ms /   383 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   130 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3460.02 ms /   605 tokens\n",
      "  6%|▋         | 89/1400 [16:33<4:22:02, 11.99s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     127.89 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.18 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   324 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7991.85 ms /   383 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2277.72 ms /   534 tokens\n",
      "  6%|▋         | 90/1400 [16:44<4:12:42, 11.57s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     149.90 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.09 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   199 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4858.90 ms /   258 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   128 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3398.18 ms /   582 tokens\n",
      "  6%|▋         | 91/1400 [16:52<3:53:16, 10.69s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     148.25 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.71 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   226 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5544.85 ms /   284 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   116 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3110.74 ms /   569 tokens\n",
      "  7%|▋         | 92/1400 [17:01<3:42:03, 10.19s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     133.00 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.22 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   281 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6908.72 ms /   340 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    83 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2299.10 ms /   536 tokens\n",
      "  7%|▋         | 93/1400 [17:11<3:37:41,  9.99s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.98 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.50 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   216 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5283.24 ms /   274 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3260.10 ms /   575 tokens\n",
      "  7%|▋         | 94/1400 [17:20<3:30:45,  9.68s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     133.94 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.08 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   304 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7553.04 ms /   372 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   111 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3000.94 ms /   573 tokens\n",
      "  7%|▋         | 95/1400 [17:31<3:38:32, 10.05s/it]Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.56 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   218 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.59 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   266 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6557.92 ms /   352 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 481 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   481 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   151 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4010.22 ms /   632 tokens\n",
      "  7%|▋         | 96/1400 [17:42<3:44:24, 10.33s/it]Llama.generate: 42 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   130 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.99 ms /   133 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 258 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   258 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     220.90 ms /   261 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   127 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   370 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9224.47 ms /   497 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 521 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   521 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   260 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6758.22 ms /   781 tokens\n",
      "  7%|▋         | 97/1400 [17:58<4:23:47, 12.15s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     133.54 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.40 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   301 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7437.77 ms /   364 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   108 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2904.92 ms /   565 tokens\n",
      "  7%|▋         | 98/1400 [18:09<4:14:06, 11.71s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     138.21 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.78 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   297 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7315.24 ms /   369 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   138 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3656.65 ms /   605 tokens\n",
      "  7%|▋         | 99/1400 [18:20<4:11:23, 11.59s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     122.95 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.94 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   215 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5250.48 ms /   274 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2791.77 ms /   558 tokens\n",
      "  7%|▋         | 100/1400 [18:29<3:50:13, 10.63s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.75 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.41 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7454.98 ms /   374 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   161 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4220.82 ms /   627 tokens\n",
      "  7%|▋         | 101/1400 [18:41<3:59:14, 11.05s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     133.57 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.23 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   226 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5529.28 ms /   293 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   147 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3864.84 ms /   608 tokens\n",
      "  7%|▋         | 102/1400 [18:50<3:50:32, 10.66s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.59 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.79 ms /   216 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   312 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7739.28 ms /   393 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   163 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4282.53 ms /   638 tokens\n",
      "  7%|▋         | 103/1400 [19:03<4:01:45, 11.18s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     132.16 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.65 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   235 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5766.68 ms /   298 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3296.87 ms /   580 tokens\n",
      "  7%|▋         | 104/1400 [19:12<3:50:04, 10.65s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.12 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.63 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   228 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5572.53 ms /   294 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2712.46 ms /   560 tokens\n",
      "  8%|▊         | 105/1400 [19:21<3:37:09, 10.06s/it]Llama.generate: 42 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   134 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.82 ms /   138 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 262 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   262 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     245.55 ms /   266 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   130 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   216 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5352.53 ms /   346 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 525 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   525 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   169 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4468.18 ms /   694 tokens\n",
      "  8%|▊         | 106/1400 [19:31<3:38:26, 10.13s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.42 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.85 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   257 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6301.85 ms /   326 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   146 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3894.75 ms /   610 tokens\n",
      "  8%|▊         | 107/1400 [19:42<3:41:13, 10.27s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     130.60 ms /    61 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.19 ms /   189 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   309 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7597.74 ms /   363 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    87 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2383.28 ms /   536 tokens\n",
      "  8%|▊         | 108/1400 [19:52<3:41:22, 10.28s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.81 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.18 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   219 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5357.43 ms /   290 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   161 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4229.24 ms /   627 tokens\n",
      "  8%|▊         | 109/1400 [20:02<3:39:37, 10.21s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     125.01 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.93 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   295 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7277.24 ms /   350 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   139 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3664.09 ms /   590 tokens\n",
      "  8%|▊         | 110/1400 [20:13<3:46:17, 10.53s/it]Llama.generate: 42 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   121 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.03 ms /   125 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 249 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   249 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     224.58 ms /   253 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   119 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   282 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6998.98 ms /   401 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 512 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   512 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   176 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4663.45 ms /   688 tokens\n",
      "  8%|▊         | 111/1400 [20:25<3:56:13, 11.00s/it]Llama.generate: 42 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   105 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.63 ms /   109 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 233 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   233 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     223.35 ms /   237 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   102 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   233 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5740.90 ms /   335 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 496 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   496 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   167 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4375.15 ms /   663 tokens\n",
      "  8%|▊         | 112/1400 [20:36<3:53:06, 10.86s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     158.59 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.78 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   233 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5766.66 ms /   299 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   142 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3749.59 ms /   603 tokens\n",
      "  8%|▊         | 113/1400 [20:46<3:46:38, 10.57s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.51 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.62 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   226 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5527.19 ms /   287 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2759.84 ms /   559 tokens\n",
      "  8%|▊         | 114/1400 [20:55<3:34:20, 10.00s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.36 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.04 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   266 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6570.54 ms /   332 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   142 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3746.26 ms /   602 tokens\n",
      "  8%|▊         | 115/1400 [21:05<3:38:43, 10.21s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     149.20 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.93 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   280 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6863.24 ms /   337 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2454.89 ms /   541 tokens\n",
      "  8%|▊         | 116/1400 [21:15<3:35:13, 10.06s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.42 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.38 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   257 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6313.60 ms /   321 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2421.65 ms /   547 tokens\n",
      "  8%|▊         | 117/1400 [21:24<3:29:03,  9.78s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.98 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.87 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   314 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7749.69 ms /   381 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   164 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4283.10 ms /   626 tokens\n",
      "  8%|▊         | 118/1400 [21:36<3:45:42, 10.56s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     149.52 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.63 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   202 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4940.68 ms /   257 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2182.67 ms /   529 tokens\n",
      "  8%|▊         | 119/1400 [21:44<3:25:53,  9.64s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     129.75 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.96 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   281 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6910.01 ms /   347 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   154 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4044.46 ms /   614 tokens\n",
      "  9%|▊         | 120/1400 [21:55<3:36:18, 10.14s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     137.75 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.82 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   320 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7943.27 ms /   401 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   147 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3891.02 ms /   622 tokens\n",
      "  9%|▊         | 121/1400 [22:07<3:49:11, 10.75s/it]Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.68 ms /    96 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.84 ms /   224 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   249 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6139.85 ms /   337 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 483 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   483 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3053.82 ms /   595 tokens\n",
      "  9%|▊         | 122/1400 [22:17<3:41:43, 10.41s/it]Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     135.75 ms /    98 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.62 ms /   226 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   349 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8670.67 ms /   440 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   486 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   230 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5949.20 ms /   716 tokens\n",
      "  9%|▉         | 123/1400 [22:32<4:10:44, 11.78s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.52 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.66 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   190 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4636.06 ms /   250 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2370.25 ms /   541 tokens\n",
      "  9%|▉         | 124/1400 [22:39<3:42:38, 10.47s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.03 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.52 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   330 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8202.88 ms /   397 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   193 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5005.06 ms /   655 tokens\n",
      "  9%|▉         | 125/1400 [22:53<4:02:19, 11.40s/it]Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     141.85 ms /    93 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   218 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.46 ms /   221 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7490.66 ms /   390 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 481 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   481 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   165 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4367.89 ms /   646 tokens\n",
      "  9%|▉         | 126/1400 [23:05<4:07:19, 11.65s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.57 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.09 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   212 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5182.75 ms /   280 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   158 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4145.53 ms /   621 tokens\n",
      "  9%|▉         | 127/1400 [23:15<3:54:43, 11.06s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     129.70 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.22 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   326 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8041.35 ms /   397 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   193 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5021.71 ms /   660 tokens\n",
      "  9%|▉         | 128/1400 [23:28<4:09:27, 11.77s/it]Llama.generate: 42 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   105 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     148.42 ms /   108 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 233 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   233 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.71 ms /   236 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   327 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8148.57 ms /   428 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 496 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   496 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5918.50 ms /   725 tokens\n",
      "  9%|▉         | 129/1400 [23:43<4:26:16, 12.57s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     126.92 ms /    61 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.20 ms /   189 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   294 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7227.69 ms /   348 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2776.34 ms /   550 tokens\n",
      "  9%|▉         | 130/1400 [23:53<4:11:52, 11.90s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.82 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.27 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   275 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6766.25 ms /   348 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   108 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2910.73 ms /   576 tokens\n",
      "  9%|▉         | 131/1400 [24:03<4:00:01, 11.35s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     124.95 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.71 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   210 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5115.45 ms /   268 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2642.05 ms /   550 tokens\n",
      "  9%|▉         | 132/1400 [24:11<3:39:14, 10.37s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     158.49 ms /    89 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.80 ms /   217 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   171 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4224.80 ms /   251 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   141 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3738.04 ms /   617 tokens\n",
      " 10%|▉         | 133/1400 [24:20<3:26:19,  9.77s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     148.13 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.81 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   331 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8184.14 ms /   387 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2012.36 ms /   524 tokens\n",
      " 10%|▉         | 134/1400 [24:30<3:31:13, 10.01s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.84 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.32 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   242 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5970.46 ms /   313 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   133 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3543.69 ms /   599 tokens\n",
      " 10%|▉         | 135/1400 [24:40<3:30:23,  9.98s/it]Llama.generate: 42 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   104 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     141.48 ms /   107 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 232 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   232 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.37 ms /   235 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   265 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6592.38 ms /   366 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 495 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   495 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   171 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4476.55 ms /   666 tokens\n",
      " 10%|▉         | 136/1400 [24:52<3:39:25, 10.42s/it]Llama.generate: 42 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     149.73 ms /   106 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 231 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   231 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.27 ms /   234 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   308 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7652.96 ms /   409 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 494 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   494 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   226 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5845.09 ms /   720 tokens\n",
      " 10%|▉         | 137/1400 [25:05<4:01:05, 11.45s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     126.78 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.88 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   149 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3646.94 ms /   207 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2512.43 ms /   545 tokens\n",
      " 10%|▉         | 138/1400 [25:12<3:29:34,  9.96s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     129.72 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.03 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   306 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7541.65 ms /   372 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   109 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2978.42 ms /   569 tokens\n",
      " 10%|▉         | 139/1400 [25:23<3:35:06, 10.23s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     131.82 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.84 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   238 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5852.90 ms /   306 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   123 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3300.26 ms /   585 tokens\n",
      " 10%|█         | 140/1400 [25:32<3:30:17, 10.01s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.29 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.99 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5839.95 ms /   301 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   150 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3949.70 ms /   610 tokens\n",
      " 10%|█         | 141/1400 [25:42<3:31:09, 10.06s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     131.78 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.33 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   285 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7045.94 ms /   352 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   134 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3548.89 ms /   595 tokens\n",
      " 10%|█         | 142/1400 [25:53<3:36:30, 10.33s/it]Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     127.83 ms /    60 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.77 ms /   188 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 53 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    53 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   323 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7987.11 ms /   376 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 448 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   448 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2087.95 ms /   524 tokens\n",
      " 10%|█         | 143/1400 [26:04<3:36:51, 10.35s/it]Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.26 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.17 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7388.58 ms /   387 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 482 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   482 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   182 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4744.43 ms /   664 tokens\n",
      " 10%|█         | 144/1400 [26:16<3:50:23, 11.01s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.55 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.09 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   215 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5272.68 ms /   290 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3012.21 ms /   579 tokens\n",
      " 10%|█         | 145/1400 [26:25<3:35:35, 10.31s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     127.64 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.80 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   249 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6144.79 ms /   318 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   118 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3148.41 ms /   581 tokens\n",
      " 10%|█         | 146/1400 [26:35<3:31:09, 10.10s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.51 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.46 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   274 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6773.97 ms /   346 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   121 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3231.51 ms /   587 tokens\n",
      " 10%|█         | 147/1400 [26:45<3:32:46, 10.19s/it]Llama.generate: 42 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   107 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     139.89 ms /   110 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 235 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   235 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.73 ms /   238 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   304 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7556.89 ms /   407 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 498 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   498 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   158 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4154.86 ms /   656 tokens\n",
      " 11%|█         | 148/1400 [26:57<3:44:25, 10.76s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     131.65 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.87 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   253 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6256.06 ms /   316 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1831.18 ms /   522 tokens\n",
      " 11%|█         | 149/1400 [27:06<3:29:42, 10.06s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.14 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.43 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   283 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6989.25 ms /   349 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   160 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4195.93 ms /   621 tokens\n",
      " 11%|█         | 150/1400 [27:17<3:39:07, 10.52s/it]Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     158.50 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   211 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.64 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   251 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6206.93 ms /   330 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 474 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   474 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   116 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3112.00 ms /   590 tokens\n",
      " 11%|█         | 151/1400 [27:27<3:33:57, 10.28s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.72 ms /    91 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.45 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   281 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6958.40 ms /   364 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   249 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6447.70 ms /   727 tokens\n",
      " 11%|█         | 152/1400 [27:41<3:55:40, 11.33s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.98 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.71 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   199 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4867.19 ms /   256 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    95 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2567.08 ms /   547 tokens\n",
      " 11%|█         | 153/1400 [27:48<3:33:33, 10.28s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.96 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.00 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   249 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6133.35 ms /   323 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2613.40 ms /   563 tokens\n",
      " 11%|█         | 154/1400 [27:58<3:26:20,  9.94s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     129.58 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.04 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   221 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5433.86 ms /   291 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   147 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3872.80 ms /   610 tokens\n",
      " 11%|█         | 155/1400 [28:07<3:24:23,  9.85s/it]Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     136.84 ms /    91 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   216 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.87 ms /   219 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   322 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7974.92 ms /   406 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 479 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   479 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   268 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6898.49 ms /   747 tokens\n",
      " 11%|█         | 156/1400 [28:22<3:57:41, 11.46s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     123.66 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.78 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   284 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6981.78 ms /   344 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2077.33 ms /   529 tokens\n",
      " 11%|█         | 157/1400 [28:32<3:44:33, 10.84s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     133.34 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.42 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   228 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5643.60 ms /   295 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2669.58 ms /   559 tokens\n",
      " 11%|█▏        | 158/1400 [28:41<3:30:49, 10.18s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.00 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.57 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   279 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6856.43 ms /   352 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   130 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3442.86 ms /   597 tokens\n",
      " 11%|█▏        | 159/1400 [28:51<3:33:41, 10.33s/it]Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.21 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.31 ms /   217 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   247 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6070.52 ms /   329 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 477 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   477 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   171 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4473.06 ms /   648 tokens\n",
      " 11%|█▏        | 160/1400 [29:02<3:37:24, 10.52s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.08 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.59 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   295 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7275.38 ms /   369 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5640.81 ms /   687 tokens\n",
      " 12%|█▏        | 161/1400 [29:15<3:54:37, 11.36s/it]Llama.generate: 42 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    93 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.48 ms /    97 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   221 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.36 ms /   224 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   301 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7433.07 ms /   389 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 484 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   484 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   328 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8425.43 ms /   812 tokens\n",
      " 12%|█▏        | 162/1400 [29:32<4:24:39, 12.83s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.12 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     236.94 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   184 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4491.78 ms /   238 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2015.94 ms /   521 tokens\n",
      " 12%|█▏        | 163/1400 [29:39<3:47:57, 11.06s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     158.04 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.57 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5310.91 ms /   275 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   159 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4162.01 ms /   611 tokens\n",
      " 12%|█▏        | 164/1400 [29:48<3:40:14, 10.69s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     140.05 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.19 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   289 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7164.91 ms /   358 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   182 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4742.86 ms /   646 tokens\n",
      " 12%|█▏        | 165/1400 [30:01<3:49:44, 11.16s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.18 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.46 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   343 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8504.26 ms /   415 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   157 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4123.54 ms /   624 tokens\n",
      " 12%|█▏        | 166/1400 [30:14<4:00:49, 11.71s/it]Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.28 ms /    99 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.17 ms /   227 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   330 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8182.28 ms /   422 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   486 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   154 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4059.69 ms /   640 tokens\n",
      " 12%|█▏        | 167/1400 [30:26<4:06:27, 11.99s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.52 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.23 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   227 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5569.78 ms /   292 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2766.41 ms /   561 tokens\n",
      " 12%|█▏        | 168/1400 [30:35<3:46:01, 11.01s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     151.64 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.81 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   292 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7184.79 ms /   362 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   108 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2939.45 ms /   573 tokens\n",
      " 12%|█▏        | 169/1400 [30:46<3:42:36, 10.85s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     137.44 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.11 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   290 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7156.55 ms /   354 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   123 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3280.97 ms /   582 tokens\n",
      " 12%|█▏        | 170/1400 [30:56<3:42:02, 10.83s/it]Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     133.77 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.56 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   305 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7543.02 ms /   392 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 482 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   482 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   153 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4032.16 ms /   635 tokens\n",
      " 12%|█▏        | 171/1400 [31:08<3:48:35, 11.16s/it]Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.54 ms /    96 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.02 ms /   224 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   231 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5715.34 ms /   319 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 483 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   483 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   123 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3299.89 ms /   606 tokens\n",
      " 12%|█▏        | 172/1400 [31:18<3:37:42, 10.64s/it]Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.79 ms /   103 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   228 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.25 ms /   231 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 97 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    97 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   280 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6925.43 ms /   377 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 491 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   491 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   238 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6156.25 ms /   729 tokens\n",
      " 12%|█▏        | 173/1400 [31:31<3:54:52, 11.49s/it]Llama.generate: 42 prefix-match hit, remaining 304 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   304 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     264.67 ms /   308 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 432 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   432 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     348.22 ms /   436 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 301 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   301 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   263 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6668.66 ms /   564 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 695 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   695 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   361 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9482.48 ms /  1056 tokens\n",
      " 12%|█▏        | 174/1400 [31:48<4:27:16, 13.08s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.32 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     216.83 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   277 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6796.30 ms /   337 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2884.17 ms /   561 tokens\n",
      " 12%|█▎        | 175/1400 [31:58<4:08:42, 12.18s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.79 ms /    61 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.67 ms /   189 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   215 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5296.31 ms /   270 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2038.98 ms /   522 tokens\n",
      " 13%|█▎        | 176/1400 [32:06<3:41:06, 10.84s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     130.13 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.69 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   263 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6505.83 ms /   334 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   120 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3206.53 ms /   586 tokens\n",
      " 13%|█▎        | 177/1400 [32:16<3:36:09, 10.60s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     151.94 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.61 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   167 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4078.78 ms /   229 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   106 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2859.06 ms /   563 tokens\n",
      " 13%|█▎        | 178/1400 [32:23<3:15:55,  9.62s/it]Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.83 ms /   102 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   227 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.68 ms /   230 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   345 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8578.59 ms /   441 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 490 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   490 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   221 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5734.30 ms /   711 tokens\n",
      " 13%|█▎        | 179/1400 [32:38<3:46:42, 11.14s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.03 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.71 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   237 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5809.83 ms /   300 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3243.75 ms /   580 tokens\n",
      " 13%|█▎        | 180/1400 [32:47<3:36:10, 10.63s/it]Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     142.33 ms /   103 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   228 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.19 ms /   231 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 97 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    97 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   320 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7933.64 ms /   417 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 491 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   491 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   256 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6605.09 ms /   747 tokens\n",
      " 13%|█▎        | 181/1400 [33:02<4:02:07, 11.92s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.18 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     222.44 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   210 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5145.34 ms /   281 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   133 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3535.66 ms /   599 tokens\n",
      " 13%|█▎        | 182/1400 [33:11<3:44:41, 11.07s/it]Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     169.80 ms /    95 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.76 ms /   223 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   289 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7168.88 ms /   376 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 482 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   482 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   154 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4051.08 ms /   636 tokens\n",
      " 13%|█▎        | 183/1400 [33:23<3:47:55, 11.24s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     134.86 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.37 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   228 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5561.74 ms /   292 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2665.53 ms /   557 tokens\n",
      " 13%|█▎        | 184/1400 [33:31<3:31:33, 10.44s/it]Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.89 ms /    98 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.92 ms /   226 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   276 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6812.71 ms /   367 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   486 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   163 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4327.38 ms /   649 tokens\n",
      " 13%|█▎        | 185/1400 [33:43<3:37:54, 10.76s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     128.12 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.36 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   250 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6135.85 ms /   309 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2571.26 ms /   548 tokens\n",
      " 13%|█▎        | 186/1400 [33:52<3:27:18, 10.25s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.90 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.70 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5777.25 ms /   295 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2102.75 ms /   526 tokens\n",
      " 13%|█▎        | 187/1400 [34:00<3:14:59,  9.64s/it]Llama.generate: 42 prefix-match hit, remaining 106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   106 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.61 ms /   110 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 234 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   234 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     224.18 ms /   238 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   102 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6679.79 ms /   371 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 497 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   497 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   144 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3815.62 ms /   641 tokens\n",
      " 13%|█▎        | 188/1400 [34:11<3:22:31, 10.03s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.21 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.86 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   294 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7274.62 ms /   375 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   179 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4715.56 ms /   654 tokens\n",
      " 14%|█▎        | 189/1400 [34:24<3:36:31, 10.73s/it]Llama.generate: 42 prefix-match hit, remaining 113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   113 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.66 ms /   116 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 241 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   241 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.19 ms /   244 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 109 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   109 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   360 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9013.00 ms /   469 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 504 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   504 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   237 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6144.13 ms /   741 tokens\n",
      " 14%|█▎        | 190/1400 [34:39<4:05:26, 12.17s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     126.33 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.19 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5589.49 ms /   285 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2299.48 ms /   536 tokens\n",
      " 14%|█▎        | 191/1400 [34:47<3:41:20, 10.98s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.52 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.77 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   254 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6256.88 ms /   317 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   149 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3919.27 ms /   607 tokens\n",
      " 14%|█▎        | 192/1400 [34:58<3:38:34, 10.86s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     140.06 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.60 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   302 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7467.66 ms /   376 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   128 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3436.26 ms /   596 tokens\n",
      " 14%|█▍        | 193/1400 [35:09<3:40:47, 10.98s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     131.99 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.50 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   242 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5951.24 ms /   315 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   138 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3660.79 ms /   607 tokens\n",
      " 14%|█▍        | 194/1400 [35:19<3:34:25, 10.67s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     128.41 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.18 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   302 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7453.46 ms /   369 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   116 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3104.00 ms /   577 tokens\n",
      " 14%|█▍        | 195/1400 [35:30<3:35:37, 10.74s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     141.70 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.77 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   257 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6334.71 ms /   322 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   127 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3379.00 ms /   586 tokens\n",
      " 14%|█▍        | 196/1400 [35:40<3:31:27, 10.54s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.83 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.81 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   289 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7153.27 ms /   366 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   134 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3568.68 ms /   605 tokens\n",
      " 14%|█▍        | 197/1400 [35:51<3:34:50, 10.72s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.07 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.52 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   283 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7000.57 ms /   358 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   196 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5098.78 ms /   666 tokens\n",
      " 14%|█▍        | 198/1400 [36:04<3:45:17, 11.25s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.71 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.64 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   212 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5232.36 ms /   282 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   117 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3125.51 ms /   582 tokens\n",
      " 14%|█▍        | 199/1400 [36:12<3:30:07, 10.50s/it]Llama.generate: 42 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.95 ms /   106 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 231 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   231 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.67 ms /   234 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   261 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6464.40 ms /   360 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 494 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   494 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   207 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5392.22 ms /   701 tokens\n",
      " 14%|█▍        | 200/1400 [36:25<3:40:21, 11.02s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     130.09 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.60 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   355 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8856.24 ms /   416 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2470.45 ms /   546 tokens\n",
      " 14%|█▍        | 201/1400 [36:36<3:44:08, 11.22s/it]Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.88 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   218 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     215.34 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   221 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5446.17 ms /   307 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 481 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   481 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   126 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3366.16 ms /   607 tokens\n",
      " 14%|█▍        | 202/1400 [36:46<3:32:09, 10.63s/it]Llama.generate: 42 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.02 ms /    98 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 222 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   222 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     222.41 ms /   226 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   285 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7078.98 ms /   376 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 485 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   485 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   117 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3155.06 ms /   602 tokens\n",
      " 14%|█▍        | 203/1400 [36:56<3:32:06, 10.63s/it]Llama.generate: 42 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    93 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.46 ms /    97 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   221 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.88 ms /   224 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   279 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6917.35 ms /   368 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 484 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   484 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   170 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4469.03 ms /   654 tokens\n",
      " 15%|█▍        | 204/1400 [37:08<3:38:58, 10.99s/it]Llama.generate: 42 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    93 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.90 ms /    97 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   221 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.76 ms /   225 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6715.95 ms /   358 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 484 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   484 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   119 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3184.20 ms /   603 tokens\n",
      " 15%|█▍        | 205/1400 [37:18<3:34:51, 10.79s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.79 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.89 ms /   216 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   263 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6479.65 ms /   343 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2900.74 ms /   582 tokens\n",
      " 15%|█▍        | 206/1400 [37:28<3:28:48, 10.49s/it]Llama.generate: 42 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   134 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.45 ms /   138 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 262 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   262 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     222.67 ms /   265 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   130 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7598.84 ms /   433 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 525 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   525 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   178 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4740.42 ms /   703 tokens\n",
      " 15%|█▍        | 207/1400 [37:41<3:42:15, 11.18s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.85 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.43 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   315 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7794.25 ms /   380 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   114 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3095.36 ms /   573 tokens\n",
      " 15%|█▍        | 208/1400 [37:52<3:42:34, 11.20s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.27 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.93 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   223 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5498.23 ms /   285 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   148 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3915.42 ms /   605 tokens\n",
      " 15%|█▍        | 209/1400 [38:02<3:34:04, 10.78s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     134.89 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.70 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   224 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5485.77 ms /   283 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   124 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3313.45 ms /   578 tokens\n",
      " 15%|█▌        | 210/1400 [38:11<3:24:09, 10.29s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.84 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.54 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   369 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9171.65 ms /   431 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   126 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3356.78 ms /   583 tokens\n",
      " 15%|█▌        | 211/1400 [38:24<3:39:40, 11.09s/it]Llama.generate: 42 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   104 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.99 ms /   108 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 232 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   232 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     223.63 ms /   236 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   280 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6927.12 ms /   381 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 495 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   495 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   155 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4151.94 ms /   650 tokens\n",
      " 15%|█▌        | 212/1400 [38:36<3:42:07, 11.22s/it]Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     136.01 ms /    99 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 224 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   224 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.25 ms /   227 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    93 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   351 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8711.76 ms /   444 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 487 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   487 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   291 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7499.84 ms /   778 tokens\n",
      " 15%|█▌        | 213/1400 [38:52<4:13:44, 12.83s/it]Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.43 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   218 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     217.28 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5630.09 ms /   315 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 481 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   481 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   121 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3256.22 ms /   602 tokens\n",
      " 15%|█▌        | 214/1400 [39:01<3:52:34, 11.77s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.71 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.87 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   226 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5576.10 ms /   283 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1762.71 ms /   513 tokens\n",
      " 15%|█▌        | 215/1400 [39:09<3:28:21, 10.55s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.77 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.14 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   240 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5896.21 ms /   306 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   123 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3298.37 ms /   584 tokens\n",
      " 15%|█▌        | 216/1400 [39:19<3:22:32, 10.26s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.38 ms /    89 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.52 ms /   217 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   308 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7633.97 ms /   389 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   154 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4049.29 ms /   630 tokens\n",
      " 16%|█▌        | 217/1400 [39:31<3:33:05, 10.81s/it]Llama.generate: 42 prefix-match hit, remaining 165 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   165 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.71 ms /   169 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 293 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   293 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     262.44 ms /   297 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 161 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   161 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7515.99 ms /   460 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 556 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   556 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   224 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5874.92 ms /   780 tokens\n",
      " 16%|█▌        | 218/1400 [39:45<3:51:07, 11.73s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.65 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     235.08 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   194 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4746.09 ms /   258 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2515.97 ms /   551 tokens\n",
      " 16%|█▌        | 219/1400 [39:52<3:27:02, 10.52s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.65 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.18 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   297 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7328.41 ms /   353 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   125 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3307.12 ms /   577 tokens\n",
      " 16%|█▌        | 220/1400 [40:03<3:29:54, 10.67s/it]Llama.generate: 42 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.65 ms /   106 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 231 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   231 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.94 ms /   234 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   260 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6486.23 ms /   359 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 494 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   494 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   171 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4490.44 ms /   665 tokens\n",
      " 16%|█▌        | 221/1400 [40:15<3:33:47, 10.88s/it]Llama.generate: 42 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   129 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.06 ms /   132 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 257 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   257 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     221.94 ms /   260 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   126 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   310 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7786.66 ms /   436 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 520 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   520 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   195 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5140.93 ms /   715 tokens\n",
      " 16%|█▌        | 222/1400 [40:28<3:48:12, 11.62s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.65 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.44 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   365 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9071.19 ms /   431 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   144 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3859.86 ms /   605 tokens\n",
      " 16%|█▌        | 223/1400 [40:42<3:57:54, 12.13s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     158.44 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.26 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   248 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6101.73 ms /   310 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   127 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3444.10 ms /   583 tokens\n",
      " 16%|█▌        | 224/1400 [40:51<3:44:44, 11.47s/it]Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.93 ms /   103 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   227 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     222.11 ms /   231 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   211 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5230.89 ms /   306 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 490 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   490 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   155 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4132.68 ms /   645 tokens\n",
      " 16%|█▌        | 225/1400 [41:01<3:34:40, 10.96s/it]Llama.generate: 42 prefix-match hit, remaining 97 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    97 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.18 ms /   101 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 225 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   225 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     233.52 ms /   229 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    93 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6702.82 ms /   362 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 488 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   488 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   134 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3626.48 ms /   622 tokens\n",
      " 16%|█▌        | 226/1400 [41:12<3:33:19, 10.90s/it]Llama.generate: 42 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     143.72 ms /   101 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 226 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   226 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.29 ms /   229 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   274 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6781.30 ms /   369 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 489 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   489 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   219 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5732.18 ms /   708 tokens\n",
      " 16%|█▌        | 227/1400 [41:25<3:44:47, 11.50s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.71 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.27 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   226 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5553.10 ms /   289 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2567.98 ms /   550 tokens\n",
      " 16%|█▋        | 228/1400 [41:33<3:27:08, 10.60s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     130.24 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.11 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   284 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7044.84 ms /   353 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   267 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6896.98 ms /   731 tokens\n",
      " 16%|█▋        | 229/1400 [41:48<3:48:34, 11.71s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.54 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.64 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   338 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8438.60 ms /   403 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   120 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3219.40 ms /   579 tokens\n",
      " 16%|█▋        | 230/1400 [42:00<3:50:15, 11.81s/it]Llama.generate: 42 prefix-match hit, remaining 106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   106 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.44 ms /   109 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 234 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   234 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.09 ms /   237 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   344 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8607.67 ms /   445 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 497 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   497 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5670.43 ms /   715 tokens\n",
      " 16%|█▋        | 231/1400 [42:14<4:06:47, 12.67s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     147.26 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     243.46 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   158 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3860.81 ms /   217 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2446.39 ms /   544 tokens\n",
      " 17%|█▋        | 232/1400 [42:21<3:31:52, 10.88s/it]Llama.generate: 42 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   105 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.54 ms /   109 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 233 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   233 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     226.68 ms /   237 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   308 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7691.41 ms /   409 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 496 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   496 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6621.43 ms /   751 tokens\n",
      " 17%|█▋        | 233/1400 [42:36<3:54:20, 12.05s/it]Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.88 ms /    99 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 224 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   224 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.12 ms /   227 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   266 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6647.12 ms /   358 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 487 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   487 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   170 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4464.18 ms /   657 tokens\n",
      " 17%|█▋        | 234/1400 [42:47<3:50:50, 11.88s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     137.86 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.07 ms /   216 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   327 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8115.95 ms /   407 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   200 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5211.65 ms /   676 tokens\n",
      " 17%|█▋        | 235/1400 [43:01<4:01:06, 12.42s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.10 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.26 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   288 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7124.75 ms /   361 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   146 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3899.49 ms /   613 tokens\n",
      " 17%|█▋        | 236/1400 [43:12<3:55:03, 12.12s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.75 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.22 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   342 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8550.25 ms /   415 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   130 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3456.54 ms /   598 tokens\n",
      " 17%|█▋        | 237/1400 [43:25<3:56:39, 12.21s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.92 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.41 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   266 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6593.19 ms /   334 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   158 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4166.60 ms /   621 tokens\n",
      " 17%|█▋        | 238/1400 [43:36<3:50:10, 11.88s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.99 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.75 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   260 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6425.29 ms /   317 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    50 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1481.74 ms /   502 tokens\n",
      " 17%|█▋        | 239/1400 [43:44<3:29:02, 10.80s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.41 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.44 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   313 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7765.84 ms /   380 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   125 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3334.96 ms /   586 tokens\n",
      " 17%|█▋        | 240/1400 [43:56<3:32:47, 11.01s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.40 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.89 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   249 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6147.30 ms /   317 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   108 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2962.74 ms /   572 tokens\n",
      " 17%|█▋        | 241/1400 [44:05<3:23:53, 10.55s/it]Llama.generate: 42 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   104 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.32 ms /   108 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 232 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   232 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.72 ms /   235 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   264 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6581.94 ms /   365 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 495 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   495 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   243 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6353.62 ms /   738 tokens\n",
      " 17%|█▋        | 242/1400 [44:19<3:39:52, 11.39s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.29 ms /    89 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     216.76 ms /   217 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   216 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5313.87 ms /   298 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   114 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3063.83 ms /   590 tokens\n",
      " 17%|█▋        | 243/1400 [44:27<3:24:35, 10.61s/it]Llama.generate: 42 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   116 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.14 ms /   120 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 244 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   244 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     226.17 ms /   248 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   112 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   313 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7846.30 ms /   425 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 507 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   507 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   156 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4128.06 ms /   663 tokens\n",
      " 17%|█▋        | 244/1400 [44:40<3:34:52, 11.15s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.63 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.13 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   367 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9143.30 ms /   434 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   145 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3854.76 ms /   607 tokens\n",
      " 18%|█▊        | 245/1400 [44:53<3:47:39, 11.83s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     151.94 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.42 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   278 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6878.11 ms /   338 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2747.95 ms /   556 tokens\n",
      " 18%|█▊        | 246/1400 [45:03<3:36:49, 11.27s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.19 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.08 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   252 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6213.36 ms /   317 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   127 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3396.53 ms /   587 tokens\n",
      " 18%|█▊        | 247/1400 [45:13<3:29:32, 10.90s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.57 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.94 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   204 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5042.03 ms /   273 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   132 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3532.62 ms /   596 tokens\n",
      " 18%|█▊        | 248/1400 [45:22<3:18:21, 10.33s/it]Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     135.16 ms /    95 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.78 ms /   223 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   360 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8971.99 ms /   449 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 483 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   483 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   213 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5525.76 ms /   696 tokens\n",
      " 18%|█▊        | 249/1400 [45:37<3:44:11, 11.69s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.47 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.71 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5378.88 ms /   292 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   168 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4403.60 ms /   637 tokens\n",
      " 18%|█▊        | 250/1400 [45:47<3:35:24, 11.24s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.28 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.85 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   163 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3986.38 ms /   220 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2664.65 ms /   548 tokens\n",
      " 18%|█▊        | 251/1400 [45:54<3:11:20,  9.99s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     127.52 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.79 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   323 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7974.01 ms /   384 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   159 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4194.30 ms /   615 tokens\n",
      " 18%|█▊        | 252/1400 [46:07<3:25:35, 10.74s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.35 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.80 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   213 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5296.43 ms /   283 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2486.21 ms /   553 tokens\n",
      " 18%|█▊        | 253/1400 [46:15<3:10:41,  9.97s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.22 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.18 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   240 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5873.32 ms /   300 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   116 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3152.30 ms /   572 tokens\n",
      " 18%|█▊        | 254/1400 [46:24<3:07:09,  9.80s/it]Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.82 ms /   100 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 224 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   224 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     221.78 ms /   228 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   268 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6675.34 ms /   360 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 487 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   487 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   173 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4595.14 ms /   660 tokens\n",
      " 18%|█▊        | 255/1400 [46:36<3:17:46, 10.36s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     128.53 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.08 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   274 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6737.91 ms /   331 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   120 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3254.10 ms /   572 tokens\n",
      " 18%|█▊        | 256/1400 [46:46<3:17:24, 10.35s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     139.74 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.26 ms /   216 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   318 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7906.44 ms /   399 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   142 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3804.36 ms /   618 tokens\n",
      " 18%|█▊        | 257/1400 [46:59<3:27:05, 10.87s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.88 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.42 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   194 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4794.32 ms /   256 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2679.06 ms /   555 tokens\n",
      " 18%|█▊        | 258/1400 [47:06<3:09:44,  9.97s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.34 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.56 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   253 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6242.99 ms /   312 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2846.40 ms /   557 tokens\n",
      " 18%|█▊        | 259/1400 [47:16<3:06:38,  9.81s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     151.26 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.39 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   198 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4856.72 ms /   253 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3045.29 ms /   563 tokens\n",
      " 19%|█▊        | 260/1400 [47:24<2:57:43,  9.35s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     123.90 ms /    61 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.46 ms /   189 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   271 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6673.27 ms /   325 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2108.15 ms /   523 tokens\n",
      " 19%|█▊        | 261/1400 [47:33<2:56:14,  9.28s/it]Llama.generate: 42 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   124 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     169.83 ms /   128 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 252 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   252 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     229.71 ms /   256 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   120 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   298 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7495.73 ms /   418 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 515 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   515 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   172 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4611.24 ms /   687 tokens\n",
      " 19%|█▊        | 262/1400 [47:46<3:14:34, 10.26s/it]Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.25 ms /   100 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 224 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   224 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     220.77 ms /   228 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    93 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   297 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7385.00 ms /   390 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 487 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   487 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   277 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7207.51 ms /   764 tokens\n",
      " 19%|█▉        | 263/1400 [48:01<3:41:23, 11.68s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     130.14 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.70 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   275 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6779.39 ms /   339 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2190.51 ms /   536 tokens\n",
      " 19%|█▉        | 264/1400 [48:10<3:27:46, 10.97s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.70 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.69 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   263 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6469.45 ms /   323 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   140 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3714.37 ms /   595 tokens\n",
      " 19%|█▉        | 265/1400 [48:21<3:25:08, 10.84s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.92 ms /    91 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     215.36 ms /   219 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   257 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6352.55 ms /   340 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   125 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3398.11 ms /   603 tokens\n",
      " 19%|█▉        | 266/1400 [48:31<3:21:05, 10.64s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     133.45 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.77 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   314 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7777.85 ms /   386 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   195 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5079.60 ms /   661 tokens\n",
      " 19%|█▉        | 267/1400 [48:44<3:35:27, 11.41s/it]Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     135.05 ms /    91 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   216 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.02 ms /   219 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   280 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6938.45 ms /   365 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 479 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   479 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   261 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6737.40 ms /   740 tokens\n",
      " 19%|█▉        | 268/1400 [48:58<3:50:04, 12.20s/it]Llama.generate: 42 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   104 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     141.43 ms /   107 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 232 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   232 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.38 ms /   235 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   418 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10441.09 ms /   518 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 495 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   495 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   178 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4690.28 ms /   673 tokens\n",
      " 19%|█▉        | 269/1400 [49:14<4:08:33, 13.19s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.56 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.61 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   270 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6662.92 ms /   336 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   118 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3205.28 ms /   579 tokens\n",
      " 19%|█▉        | 270/1400 [49:24<3:51:40, 12.30s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     130.25 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.59 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   274 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6781.46 ms /   346 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   159 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4192.27 ms /   626 tokens\n",
      " 19%|█▉        | 271/1400 [49:35<3:45:56, 12.01s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.87 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.12 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   225 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5519.49 ms /   292 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2559.16 ms /   554 tokens\n",
      " 19%|█▉        | 272/1400 [49:44<3:25:50, 10.95s/it]Llama.generate: 42 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.92 ms /   102 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 226 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   226 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.45 ms /   229 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   337 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8402.99 ms /   431 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 489 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   489 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   179 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4703.76 ms /   668 tokens\n",
      " 20%|█▉        | 273/1400 [49:57<3:40:00, 11.71s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.46 ms /    85 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.25 ms /   213 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   322 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8000.12 ms /   400 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   121 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3279.18 ms /   593 tokens\n",
      " 20%|█▉        | 274/1400 [50:09<3:39:37, 11.70s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     126.89 ms /    61 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.61 ms /   189 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   224 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5480.93 ms /   279 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2703.19 ms /   548 tokens\n",
      " 20%|█▉        | 275/1400 [50:17<3:21:30, 10.75s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.77 ms /    85 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.81 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   317 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7836.18 ms /   394 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   198 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5199.56 ms /   670 tokens\n",
      " 20%|█▉        | 276/1400 [50:31<3:36:18, 11.55s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     136.85 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.43 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   257 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6327.61 ms /   323 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   162 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4299.99 ms /   621 tokens\n",
      " 20%|█▉        | 277/1400 [50:42<3:32:57, 11.38s/it]Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     139.38 ms /   103 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   228 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.27 ms /   231 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 97 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    97 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   322 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8024.42 ms /   419 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 491 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   491 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   151 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4021.09 ms /   642 tokens\n",
      " 20%|█▉        | 278/1400 [50:54<3:38:36, 11.69s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.09 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     222.88 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   364 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9071.83 ms /   440 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   189 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4994.55 ms /   660 tokens\n",
      " 20%|█▉        | 279/1400 [51:09<3:54:00, 12.52s/it]Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.03 ms /   100 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 224 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   224 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.95 ms /   227 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   298 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7420.07 ms /   390 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 487 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   487 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   173 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4584.10 ms /   660 tokens\n",
      " 20%|██        | 280/1400 [51:21<3:53:00, 12.48s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     138.53 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.46 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   279 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6926.23 ms /   343 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2101.88 ms /   532 tokens\n",
      " 20%|██        | 281/1400 [51:30<3:35:27, 11.55s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     142.43 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.09 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   314 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7783.12 ms /   388 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   168 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4443.89 ms /   636 tokens\n",
      " 20%|██        | 282/1400 [51:43<3:41:04, 11.86s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.72 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.35 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   289 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7122.10 ms /   345 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2795.70 ms /   553 tokens\n",
      " 20%|██        | 283/1400 [51:53<3:32:08, 11.39s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.87 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.67 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   314 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7804.38 ms /   384 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   196 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5140.90 ms /   661 tokens\n",
      " 20%|██        | 284/1400 [52:07<3:42:40, 11.97s/it]Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.81 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   211 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.61 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   258 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6411.63 ms /   337 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 474 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   474 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3014.25 ms /   584 tokens\n",
      " 20%|██        | 285/1400 [52:16<3:30:32, 11.33s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     129.73 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.75 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   227 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5573.79 ms /   285 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2207.39 ms /   530 tokens\n",
      " 20%|██        | 286/1400 [52:24<3:12:26, 10.36s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     128.08 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.63 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   345 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8587.77 ms /   422 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   179 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4681.81 ms /   650 tokens\n",
      " 20%|██        | 287/1400 [52:38<3:30:20, 11.34s/it]Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.63 ms /    96 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     226.58 ms /   224 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   237 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5886.36 ms /   327 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 483 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   483 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   142 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3794.52 ms /   625 tokens\n",
      " 21%|██        | 288/1400 [52:48<3:23:15, 10.97s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.98 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.31 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   221 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5444.13 ms /   291 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   176 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4652.60 ms /   641 tokens\n",
      " 21%|██        | 289/1400 [52:59<3:20:24, 10.82s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.75 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.70 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   241 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5947.17 ms /   307 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2790.23 ms /   561 tokens\n",
      " 21%|██        | 290/1400 [53:08<3:10:52, 10.32s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.66 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.36 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   267 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6581.11 ms /   329 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   114 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3108.41 ms /   571 tokens\n",
      " 21%|██        | 291/1400 [53:18<3:09:24, 10.25s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     147.44 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.00 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   190 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4691.44 ms /   245 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    78 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2166.51 ms /   528 tokens\n",
      " 21%|██        | 292/1400 [53:25<2:52:32,  9.34s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.68 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.39 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5789.35 ms /   294 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   179 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4669.27 ms /   631 tokens\n",
      " 21%|██        | 293/1400 [53:36<3:00:44,  9.80s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     135.06 ms /    85 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.74 ms /   213 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   302 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7486.57 ms /   380 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   242 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6310.22 ms /   715 tokens\n",
      " 21%|██        | 294/1400 [53:50<3:24:41, 11.10s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     123.60 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.60 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   262 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6463.33 ms /   318 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    65 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1907.68 ms /   515 tokens\n",
      " 21%|██        | 295/1400 [53:59<3:11:12, 10.38s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     138.74 ms /    85 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.06 ms /   213 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   314 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7830.52 ms /   393 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   151 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4041.35 ms /   624 tokens\n",
      " 21%|██        | 296/1400 [54:11<3:21:14, 10.94s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.44 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.00 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   221 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5477.42 ms /   283 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2801.37 ms /   558 tokens\n",
      " 21%|██        | 297/1400 [54:20<3:08:33, 10.26s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     125.65 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.14 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   222 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5468.64 ms /   279 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2550.18 ms /   545 tokens\n",
      " 21%|██▏       | 298/1400 [54:28<2:57:52,  9.68s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     156.52 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.08 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   230 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5726.13 ms /   306 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   180 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4759.52 ms /   651 tokens\n",
      " 21%|██▏       | 299/1400 [54:39<3:04:10, 10.04s/it]Llama.generate: 42 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     145.25 ms /   106 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 231 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   231 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.79 ms /   234 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   279 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6933.46 ms /   379 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 494 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   494 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   223 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5820.93 ms /   717 tokens\n",
      " 21%|██▏       | 300/1400 [54:52<3:21:02, 10.97s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.43 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.19 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   276 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6840.20 ms /   344 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   131 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3497.13 ms /   594 tokens\n",
      " 22%|██▏       | 301/1400 [55:03<3:19:28, 10.89s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.41 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.32 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   372 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9268.43 ms /   441 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   118 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3162.13 ms /   582 tokens\n",
      " 22%|██▏       | 302/1400 [55:16<3:29:52, 11.47s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.74 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.47 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   245 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6046.55 ms /   316 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   266 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6915.27 ms /   731 tokens\n",
      " 22%|██▏       | 303/1400 [55:29<3:40:05, 12.04s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     158.74 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.39 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   292 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7220.19 ms /   360 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   183 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4830.35 ms /   645 tokens\n",
      " 22%|██▏       | 304/1400 [55:41<3:41:58, 12.15s/it]Llama.generate: 42 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   111 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.69 ms /   115 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 239 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   239 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     234.04 ms /   243 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   107 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6355.04 ms /   362 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 502 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   502 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   183 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4843.53 ms /   685 tokens\n",
      " 22%|██▏       | 305/1400 [55:53<3:38:55, 12.00s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.53 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.43 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   277 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6835.61 ms /   337 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   152 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4036.83 ms /   607 tokens\n",
      " 22%|██▏       | 306/1400 [56:04<3:34:39, 11.77s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     149.01 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.32 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5803.65 ms /   292 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2153.54 ms /   527 tokens\n",
      " 22%|██▏       | 307/1400 [56:13<3:15:42, 10.74s/it]Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.67 ms /    93 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     216.18 ms /   221 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   302 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7486.75 ms /   388 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 480 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   480 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   195 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5118.08 ms /   675 tokens\n",
      " 22%|██▏       | 308/1400 [56:26<3:27:55, 11.42s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     128.47 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.38 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   242 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5965.89 ms /   309 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   117 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3171.38 ms /   579 tokens\n",
      " 22%|██▏       | 309/1400 [56:35<3:17:07, 10.84s/it]Llama.generate: 42 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     149.09 ms /    97 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 222 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   222 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.93 ms /   225 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   281 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6980.21 ms /   371 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 485 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   485 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   206 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5385.80 ms /   691 tokens\n",
      " 22%|██▏       | 310/1400 [56:48<3:27:18, 11.41s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.12 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.04 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   248 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6098.07 ms /   314 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   192 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5020.31 ms /   653 tokens\n",
      " 22%|██▏       | 311/1400 [56:59<3:27:32, 11.43s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     141.39 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.85 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   317 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7878.91 ms /   394 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   203 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5343.02 ms /   675 tokens\n",
      " 22%|██▏       | 312/1400 [57:13<3:39:01, 12.08s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     138.36 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.83 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   279 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6921.82 ms /   350 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   132 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3544.23 ms /   597 tokens\n",
      " 22%|██▏       | 313/1400 [57:24<3:31:59, 11.70s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     130.04 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.67 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5389.17 ms /   275 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    66 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1921.07 ms /   518 tokens\n",
      " 22%|██▏       | 314/1400 [57:31<3:09:57, 10.49s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.31 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.07 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   278 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6922.66 ms /   348 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   123 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3332.35 ms /   588 tokens\n",
      " 22%|██▎       | 315/1400 [57:42<3:10:31, 10.54s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     148.81 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.38 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   181 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4428.85 ms /   236 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1913.59 ms /   518 tokens\n",
      " 23%|██▎       | 316/1400 [57:49<2:49:39,  9.39s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.90 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.05 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   190 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4643.43 ms /   245 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2399.58 ms /   536 tokens\n",
      " 23%|██▎       | 317/1400 [57:56<2:39:07,  8.82s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.13 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.40 ms /   189 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   242 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5933.30 ms /   296 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2236.74 ms /   528 tokens\n",
      " 23%|██▎       | 318/1400 [58:05<2:37:39,  8.74s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     129.79 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.96 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   259 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6385.41 ms /   331 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   187 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4896.40 ms /   653 tokens\n",
      " 23%|██▎       | 319/1400 [58:16<2:53:04,  9.61s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     156.28 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.07 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   292 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7224.12 ms /   365 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   148 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3956.85 ms /   616 tokens\n",
      " 23%|██▎       | 320/1400 [58:28<3:03:26, 10.19s/it]Llama.generate: 42 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   119 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.86 ms /   123 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 247 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   247 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.16 ms /   250 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   115 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   362 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9052.24 ms /   477 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 510 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   510 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   271 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7020.10 ms /   781 tokens\n",
      " 23%|██▎       | 321/1400 [58:44<3:37:12, 12.08s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.24 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.08 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   209 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5127.02 ms /   272 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2989.01 ms /   567 tokens\n",
      " 23%|██▎       | 322/1400 [58:53<3:17:48, 11.01s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     156.72 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     215.29 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   231 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5682.46 ms /   294 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   127 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3442.87 ms /   584 tokens\n",
      " 23%|██▎       | 323/1400 [59:03<3:09:38, 10.57s/it]Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     135.12 ms /    95 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.34 ms /   223 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   279 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6950.24 ms /   366 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 483 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   483 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   215 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5592.60 ms /   698 tokens\n",
      " 23%|██▎       | 324/1400 [59:15<3:22:03, 11.27s/it]Llama.generate: 42 prefix-match hit, remaining 154 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   154 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.79 ms /   158 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 282 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   282 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     225.44 ms /   285 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 150 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   150 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   352 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8822.39 ms /   502 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 545 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   545 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   284 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7423.54 ms /   829 tokens\n",
      " 23%|██▎       | 325/1400 [59:32<3:51:02, 12.90s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.82 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.65 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   197 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4833.35 ms /   263 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2649.86 ms /   556 tokens\n",
      " 23%|██▎       | 326/1400 [59:40<3:23:55, 11.39s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     148.23 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.38 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   217 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5320.31 ms /   275 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   187 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4888.10 ms /   640 tokens\n",
      " 23%|██▎       | 327/1400 [59:51<3:19:16, 11.14s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     148.96 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.08 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   212 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5188.56 ms /   266 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2432.34 ms /   537 tokens\n",
      " 23%|██▎       | 328/1400 [59:59<3:02:15, 10.20s/it]Llama.generate: 42 prefix-match hit, remaining 113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   113 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     146.77 ms /   116 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 241 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   241 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.84 ms /   244 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 109 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   109 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   275 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6828.43 ms /   384 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 504 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   504 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   234 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6115.63 ms /   738 tokens\n",
      " 24%|██▎       | 329/1400 [1:00:12<3:18:47, 11.14s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     151.31 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.13 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   285 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7075.64 ms /   343 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2656.50 ms /   549 tokens\n",
      " 24%|██▎       | 330/1400 [1:00:22<3:13:10, 10.83s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.94 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.29 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   212 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5218.00 ms /   271 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2776.12 ms /   555 tokens\n",
      " 24%|██▎       | 331/1400 [1:00:30<2:59:55, 10.10s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.75 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     215.93 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   187 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4609.81 ms /   250 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   105 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2895.27 ms /   563 tokens\n",
      " 24%|██▎       | 332/1400 [1:00:38<2:48:01,  9.44s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     133.56 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     216.70 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   286 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7069.64 ms /   348 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2303.06 ms /   539 tokens\n",
      " 24%|██▍       | 333/1400 [1:00:48<2:49:32,  9.53s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.90 ms /    85 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.71 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   285 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7064.91 ms /   362 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   148 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3966.67 ms /   620 tokens\n",
      " 24%|██▍       | 334/1400 [1:00:59<2:59:25, 10.10s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.74 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     241.27 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   223 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5468.81 ms /   282 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2475.47 ms /   542 tokens\n",
      " 24%|██▍       | 335/1400 [1:01:08<2:50:02,  9.58s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     165.67 ms /    86 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     231.82 ms /   213 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   279 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6912.01 ms /   358 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   201 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5275.51 ms /   674 tokens\n",
      " 24%|██▍       | 336/1400 [1:01:20<3:06:01, 10.49s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     149.13 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.64 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   200 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4926.69 ms /   260 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2575.10 ms /   547 tokens\n",
      " 24%|██▍       | 337/1400 [1:01:28<2:51:57,  9.71s/it]Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     131.13 ms /    60 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.86 ms /   188 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    52 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   235 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5767.84 ms /   287 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 448 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   448 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2711.50 ms /   547 tokens\n",
      " 24%|██▍       | 338/1400 [1:01:37<2:47:08,  9.44s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.66 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     235.40 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   293 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7277.77 ms /   368 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   159 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4194.92 ms /   629 tokens\n",
      " 24%|██▍       | 339/1400 [1:01:49<2:59:58, 10.18s/it]Llama.generate: 42 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   110 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.83 ms /   114 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 238 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   238 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     230.02 ms /   242 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   106 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6698.41 ms /   375 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 501 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   501 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   197 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5157.41 ms /   698 tokens\n",
      " 24%|██▍       | 340/1400 [1:02:01<3:10:56, 10.81s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.62 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.78 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   162 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3955.19 ms /   218 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2391.83 ms /   535 tokens\n",
      " 24%|██▍       | 341/1400 [1:02:08<2:49:28,  9.60s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     156.20 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     244.03 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   207 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5119.27 ms /   267 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    95 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2633.16 ms /   550 tokens\n",
      " 24%|██▍       | 342/1400 [1:02:16<2:41:48,  9.18s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     156.87 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.45 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   309 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7733.16 ms /   384 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   183 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4806.05 ms /   654 tokens\n",
      " 24%|██▍       | 343/1400 [1:02:29<3:01:31, 10.30s/it]Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     137.00 ms /   102 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   227 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     238.66 ms /   230 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   329 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8216.23 ms /   424 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 490 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   490 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   259 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6726.34 ms /   749 tokens\n",
      " 25%|██▍       | 344/1400 [1:02:45<3:27:58, 11.82s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.17 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.59 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   209 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5115.49 ms /   265 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2774.93 ms /   553 tokens\n",
      " 25%|██▍       | 345/1400 [1:02:53<3:09:02, 10.75s/it]Llama.generate: 42 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   119 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.79 ms /   123 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 247 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   247 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     236.09 ms /   251 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   115 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   276 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6864.82 ms /   391 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 510 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   510 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   156 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4133.06 ms /   666 tokens\n",
      " 25%|██▍       | 346/1400 [1:03:04<3:12:30, 10.96s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     133.21 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     229.13 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   294 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7310.19 ms /   370 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   139 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3725.78 ms /   610 tokens\n",
      " 25%|██▍       | 347/1400 [1:03:16<3:14:46, 11.10s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     139.52 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.45 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   309 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7681.75 ms /   382 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   187 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4915.67 ms /   654 tokens\n",
      " 25%|██▍       | 348/1400 [1:03:29<3:24:19, 11.65s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.46 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.93 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   377 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9408.51 ms /   458 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   187 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4895.68 ms /   662 tokens\n",
      " 25%|██▍       | 349/1400 [1:03:43<3:40:05, 12.56s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.08 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.98 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   287 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7115.03 ms /   351 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    93 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2565.77 ms /   552 tokens\n",
      " 25%|██▌       | 350/1400 [1:03:53<3:26:43, 11.81s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     129.82 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.43 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   254 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6360.93 ms /   320 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   161 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4244.63 ms /   621 tokens\n",
      " 25%|██▌       | 351/1400 [1:04:04<3:22:00, 11.55s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.27 ms /    91 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.32 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   257 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6382.98 ms /   340 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   155 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4127.60 ms /   633 tokens\n",
      " 25%|██▌       | 352/1400 [1:04:15<3:18:29, 11.36s/it]Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.90 ms /    92 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   216 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.25 ms /   220 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   257 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6411.17 ms /   341 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 479 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   479 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   137 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3692.28 ms /   616 tokens\n",
      " 25%|██▌       | 353/1400 [1:04:26<3:13:48, 11.11s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.96 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     217.53 ms /   216 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   239 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5951.50 ms /   319 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   130 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3513.20 ms /   605 tokens\n",
      " 25%|██▌       | 354/1400 [1:04:36<3:07:09, 10.74s/it]Llama.generate: 42 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.10 ms /   107 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 231 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   231 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     246.18 ms /   235 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   160 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4000.51 ms /   259 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 494 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   494 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   149 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3955.08 ms /   643 tokens\n",
      " 25%|██▌       | 355/1400 [1:04:44<2:54:47, 10.04s/it]Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     133.32 ms /    86 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   211 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.43 ms /   214 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   345 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8580.67 ms /   424 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 474 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   474 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   189 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4937.36 ms /   663 tokens\n",
      " 25%|██▌       | 356/1400 [1:04:58<3:14:37, 11.18s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     135.17 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.48 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   251 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6204.27 ms /   310 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   133 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3558.48 ms /   587 tokens\n",
      " 26%|██▌       | 357/1400 [1:05:08<3:08:57, 10.87s/it]Llama.generate: 42 prefix-match hit, remaining 166 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   166 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.60 ms /   170 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 294 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   294 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     238.92 ms /   297 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 161 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   161 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   266 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6733.74 ms /   427 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 557 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   557 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   290 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7570.10 ms /   847 tokens\n",
      " 26%|██▌       | 358/1400 [1:05:23<3:29:07, 12.04s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     139.26 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.23 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   241 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5953.56 ms /   314 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   191 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5032.26 ms /   659 tokens\n",
      " 26%|██▌       | 359/1400 [1:05:34<3:25:16, 11.83s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     131.89 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     225.82 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   296 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7318.50 ms /   369 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   159 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4199.67 ms /   627 tokens\n",
      " 26%|██▌       | 360/1400 [1:05:46<3:25:26, 11.85s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.18 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.97 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   195 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4785.08 ms /   270 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   123 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3352.79 ms /   591 tokens\n",
      " 26%|██▌       | 361/1400 [1:05:55<3:07:59, 10.86s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.53 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     231.84 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   176 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4313.20 ms /   235 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2531.92 ms /   546 tokens\n",
      " 26%|██▌       | 362/1400 [1:06:02<2:49:12,  9.78s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.91 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.79 ms /   189 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   235 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5788.46 ms /   289 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2441.04 ms /   538 tokens\n",
      " 26%|██▌       | 363/1400 [1:06:10<2:42:56,  9.43s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.74 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     219.57 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   217 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5315.56 ms /   272 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   119 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3187.94 ms /   569 tokens\n",
      " 26%|██▌       | 364/1400 [1:06:19<2:40:03,  9.27s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.13 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     247.57 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   310 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7698.06 ms /   385 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   154 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4115.49 ms /   622 tokens\n",
      " 26%|██▌       | 365/1400 [1:06:32<2:55:18, 10.16s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.05 ms /    86 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.38 ms /   214 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   220 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5465.29 ms /   299 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   115 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3127.81 ms /   588 tokens\n",
      " 26%|██▌       | 366/1400 [1:06:41<2:49:05,  9.81s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.68 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     216.18 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   260 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6414.98 ms /   323 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   165 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4360.92 ms /   623 tokens\n",
      " 26%|██▌       | 367/1400 [1:06:52<2:55:57, 10.22s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.10 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.78 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   227 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5613.10 ms /   293 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3036.36 ms /   573 tokens\n",
      " 26%|██▋       | 368/1400 [1:07:01<2:49:45,  9.87s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     133.74 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.58 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   242 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5983.94 ms /   327 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   192 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5062.17 ms /   670 tokens\n",
      " 26%|██▋       | 369/1400 [1:07:12<2:57:33, 10.33s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     141.77 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.60 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   214 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5277.90 ms /   276 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2914.21 ms /   564 tokens\n",
      " 26%|██▋       | 370/1400 [1:07:21<2:48:13,  9.80s/it]Llama.generate: 42 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   118 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     148.34 ms /   121 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 246 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   246 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.91 ms /   249 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   114 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   301 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7487.60 ms /   415 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 509 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   509 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   201 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5297.34 ms /   710 tokens\n",
      " 26%|██▋       | 371/1400 [1:07:34<3:05:24, 10.81s/it]Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     146.71 ms /    98 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.29 ms /   226 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   288 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7151.12 ms /   379 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   486 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   196 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5126.11 ms /   682 tokens\n",
      " 27%|██▋       | 372/1400 [1:07:47<3:14:39, 11.36s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.36 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     234.93 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   238 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5848.84 ms /   302 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   111 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3030.58 ms /   570 tokens\n",
      " 27%|██▋       | 373/1400 [1:07:56<3:03:54, 10.74s/it]Llama.generate: 42 prefix-match hit, remaining 106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   106 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     170.00 ms /   110 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 234 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   234 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.75 ms /   237 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   102 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   298 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7424.44 ms /   400 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 497 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   497 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6973.52 ms /   766 tokens\n",
      " 27%|██▋       | 374/1400 [1:08:11<3:24:34, 11.96s/it]Llama.generate: 42 prefix-match hit, remaining 106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   106 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.26 ms /   110 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 234 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   234 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.58 ms /   237 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   102 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   284 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7057.83 ms /   386 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 497 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   497 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   177 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4680.04 ms /   674 tokens\n",
      " 27%|██▋       | 375/1400 [1:08:23<3:25:16, 12.02s/it]Llama.generate: 42 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.57 ms /   107 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 231 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   231 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     226.41 ms /   235 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   216 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5334.88 ms /   316 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 494 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   494 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   139 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3722.74 ms /   633 tokens\n",
      " 27%|██▋       | 376/1400 [1:08:32<3:12:05, 11.26s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     126.88 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.01 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   245 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6053.66 ms /   300 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   511 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   13164.17 ms /   961 tokens\n",
      " 27%|██▋       | 377/1400 [1:08:52<3:54:30, 13.75s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.73 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.47 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   246 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6069.06 ms /   308 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2791.99 ms /   560 tokens\n",
      " 27%|██▋       | 378/1400 [1:09:01<3:31:15, 12.40s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.64 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.87 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   133 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3252.46 ms /   200 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   143 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3783.33 ms /   605 tokens\n",
      " 27%|██▋       | 379/1400 [1:09:09<3:05:49, 10.92s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.98 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.89 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   277 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6848.57 ms /   338 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   105 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2881.76 ms /   561 tokens\n",
      " 27%|██▋       | 380/1400 [1:09:19<3:01:34, 10.68s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     145.43 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.28 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   238 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5899.75 ms /   318 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   128 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3427.03 ms /   603 tokens\n",
      " 27%|██▋       | 381/1400 [1:09:28<2:56:20, 10.38s/it]Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.85 ms /    99 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.06 ms /   226 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   262 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6526.94 ms /   353 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   486 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   137 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3651.86 ms /   623 tokens\n",
      " 27%|██▋       | 382/1400 [1:09:39<2:57:05, 10.44s/it]Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     136.56 ms /    98 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.27 ms /   226 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   282 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7005.42 ms /   373 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   486 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   197 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5184.89 ms /   683 tokens\n",
      " 27%|██▋       | 383/1400 [1:09:52<3:07:38, 11.07s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.38 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.54 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   219 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5404.32 ms /   279 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2668.15 ms /   551 tokens\n",
      " 27%|██▋       | 384/1400 [1:10:00<2:54:03, 10.28s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     125.27 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.06 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   226 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5564.82 ms /   284 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   125 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3331.28 ms /   578 tokens\n",
      " 28%|██▊       | 385/1400 [1:10:09<2:48:34,  9.96s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.70 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.41 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   271 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6691.46 ms /   351 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   147 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3924.57 ms /   622 tokens\n",
      " 28%|██▊       | 386/1400 [1:10:20<2:53:37, 10.27s/it]Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.82 ms /    92 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   216 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.69 ms /   220 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   230 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5698.19 ms /   313 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 479 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   479 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   123 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3321.53 ms /   602 tokens\n",
      " 28%|██▊       | 387/1400 [1:10:30<2:49:10, 10.02s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.95 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.59 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   239 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5926.30 ms /   305 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   129 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3466.60 ms /   590 tokens\n",
      " 28%|██▊       | 388/1400 [1:10:39<2:47:44,  9.95s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.03 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.88 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   180 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4459.48 ms /   241 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2488.47 ms /   544 tokens\n",
      " 28%|██▊       | 389/1400 [1:10:47<2:34:22,  9.16s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.06 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.66 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   223 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5480.39 ms /   283 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2714.65 ms /   554 tokens\n",
      " 28%|██▊       | 390/1400 [1:10:55<2:31:15,  8.99s/it]Llama.generate: 42 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   135 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     165.85 ms /   138 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 263 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   263 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     224.28 ms /   266 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   130 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   414 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10385.86 ms /   544 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 526 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   526 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   211 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5546.10 ms /   737 tokens\n",
      " 28%|██▊       | 391/1400 [1:11:12<3:08:16, 11.20s/it]Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     142.37 ms /   103 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   228 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.39 ms /   231 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   260 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6452.10 ms /   356 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 491 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   491 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   186 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4867.07 ms /   677 tokens\n",
      " 28%|██▊       | 392/1400 [1:11:23<3:10:34, 11.34s/it]Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.98 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   218 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     234.81 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   326 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8153.79 ms /   412 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 481 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   481 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   224 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5833.88 ms /   705 tokens\n",
      " 28%|██▊       | 393/1400 [1:11:38<3:25:52, 12.27s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.74 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.32 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   241 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5916.37 ms /   312 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   147 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3910.14 ms /   612 tokens\n",
      " 28%|██▊       | 394/1400 [1:11:48<3:15:16, 11.65s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     148.19 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.84 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   252 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6227.56 ms /   312 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2530.69 ms /   547 tokens\n",
      " 28%|██▊       | 395/1400 [1:11:57<3:02:29, 10.90s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.68 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.64 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5623.08 ms /   292 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   129 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3464.91 ms /   587 tokens\n",
      " 28%|██▊       | 396/1400 [1:12:07<2:55:12, 10.47s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.74 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.80 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   184 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4542.32 ms /   245 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2539.31 ms /   546 tokens\n",
      " 28%|██▊       | 397/1400 [1:12:14<2:39:52,  9.56s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.61 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     235.24 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   241 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5946.36 ms /   309 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   127 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3433.29 ms /   590 tokens\n",
      " 28%|██▊       | 398/1400 [1:12:24<2:40:53,  9.63s/it]Llama.generate: 42 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    93 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     170.50 ms /    97 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   221 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.90 ms /   224 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   266 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6595.35 ms /   356 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 484 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   484 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   149 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3948.64 ms /   633 tokens\n",
      " 28%|██▊       | 399/1400 [1:12:35<2:47:13, 10.02s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.01 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.22 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   242 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5976.39 ms /   297 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2853.26 ms /   554 tokens\n",
      " 29%|██▊       | 400/1400 [1:12:44<2:43:01,  9.78s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.09 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     215.38 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   207 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5123.22 ms /   275 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2833.99 ms /   567 tokens\n",
      " 29%|██▊       | 401/1400 [1:12:52<2:35:47,  9.36s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.40 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.25 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5833.17 ms /   306 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   123 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3317.90 ms /   588 tokens\n",
      " 29%|██▊       | 402/1400 [1:13:02<2:36:26,  9.41s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     126.96 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     220.89 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   300 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7431.94 ms /   355 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2380.68 ms /   537 tokens\n",
      " 29%|██▉       | 403/1400 [1:13:12<2:40:11,  9.64s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     151.82 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.00 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   376 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9382.47 ms /   436 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2683.79 ms /   551 tokens\n",
      " 29%|██▉       | 404/1400 [1:13:25<2:54:02, 10.48s/it]Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     137.67 ms /    99 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 224 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   224 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.25 ms /   227 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   275 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6849.63 ms /   366 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 487 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   487 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   158 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4180.78 ms /   645 tokens\n",
      " 29%|██▉       | 405/1400 [1:13:36<2:58:21, 10.76s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     139.04 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.98 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   285 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7026.63 ms /   360 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   174 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4599.11 ms /   643 tokens\n",
      " 29%|██▉       | 406/1400 [1:13:48<3:04:17, 11.12s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.33 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     222.95 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   249 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6168.39 ms /   326 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3052.34 ms /   583 tokens\n",
      " 29%|██▉       | 407/1400 [1:13:58<2:56:40, 10.68s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.79 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.39 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   245 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5995.54 ms /   308 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   205 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5362.94 ms /   663 tokens\n",
      " 29%|██▉       | 408/1400 [1:14:09<3:01:52, 11.00s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     146.96 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.62 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   283 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6987.88 ms /   358 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   193 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5084.53 ms /   663 tokens\n",
      " 29%|██▉       | 409/1400 [1:14:22<3:08:48, 11.43s/it]Llama.generate: 42 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   119 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.82 ms /   123 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 247 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   247 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.14 ms /   250 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   116 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   339 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8518.99 ms /   455 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 510 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   510 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   223 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5849.55 ms /   733 tokens\n",
      " 29%|██▉       | 410/1400 [1:14:37<3:25:11, 12.44s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     141.84 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.72 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   304 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7596.30 ms /   388 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   143 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3803.09 ms /   621 tokens\n",
      " 29%|██▉       | 411/1400 [1:14:48<3:21:38, 12.23s/it]Llama.generate: 42 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   111 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.03 ms /   115 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 239 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   239 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.25 ms /   242 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   107 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   283 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7081.17 ms /   390 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 502 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   502 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   205 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5343.18 ms /   707 tokens\n",
      " 29%|██▉       | 412/1400 [1:15:01<3:24:32, 12.42s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.19 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.33 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   298 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7358.74 ms /   365 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   153 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4112.23 ms /   615 tokens\n",
      " 30%|██▉       | 413/1400 [1:15:13<3:21:31, 12.25s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     134.95 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.87 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   266 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6554.19 ms /   330 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   118 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3164.57 ms /   576 tokens\n",
      " 30%|██▉       | 414/1400 [1:15:23<3:10:33, 11.60s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     139.62 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     240.35 ms /   216 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   267 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6641.46 ms /   349 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   135 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3634.90 ms /   611 tokens\n",
      " 30%|██▉       | 415/1400 [1:15:34<3:05:52, 11.32s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.44 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     217.04 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   239 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5919.15 ms /   306 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3084.96 ms /   575 tokens\n",
      " 30%|██▉       | 416/1400 [1:15:43<2:56:15, 10.75s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.52 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.07 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   148 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3613.85 ms /   203 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1746.56 ms /   511 tokens\n",
      " 30%|██▉       | 417/1400 [1:15:49<2:31:28,  9.25s/it]Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     126.71 ms /    60 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.91 ms /   188 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   285 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7035.99 ms /   339 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 448 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   448 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    60 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1771.84 ms /   508 tokens\n",
      " 30%|██▉       | 418/1400 [1:15:58<2:30:49,  9.22s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     127.71 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.16 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   263 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6499.84 ms /   321 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2415.17 ms /   539 tokens\n",
      " 30%|██▉       | 419/1400 [1:16:07<2:30:52,  9.23s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     158.03 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.96 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   292 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7239.48 ms /   364 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   162 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4256.80 ms /   629 tokens\n",
      " 30%|███       | 420/1400 [1:16:19<2:43:39, 10.02s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.55 ms /    91 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.33 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   312 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7784.93 ms /   396 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   277 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7186.23 ms /   755 tokens\n",
      " 30%|███       | 421/1400 [1:16:35<3:09:39, 11.62s/it]Llama.generate: 42 prefix-match hit, remaining 133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   133 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.28 ms /   136 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 261 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   261 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     220.18 ms /   264 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   129 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   312 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7849.43 ms /   441 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 524 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   524 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   267 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6963.51 ms /   791 tokens\n",
      " 30%|███       | 422/1400 [1:16:50<3:27:05, 12.71s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.38 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.90 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   196 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4883.17 ms /   265 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   134 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3565.38 ms /   600 tokens\n",
      " 30%|███       | 423/1400 [1:16:59<3:07:53, 11.54s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.45 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     239.28 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   210 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5186.06 ms /   278 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   114 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3095.13 ms /   575 tokens\n",
      " 30%|███       | 424/1400 [1:17:07<2:53:52, 10.69s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     131.61 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.35 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   288 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7152.22 ms /   344 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    48 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1475.54 ms /   499 tokens\n",
      " 30%|███       | 425/1400 [1:17:16<2:45:17, 10.17s/it]Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     142.62 ms /   102 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   227 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     227.25 ms /   230 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7549.10 ms /   399 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 490 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   490 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   161 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4268.15 ms /   651 tokens\n",
      " 30%|███       | 426/1400 [1:17:28<2:55:03, 10.78s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.06 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.90 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   313 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7751.87 ms /   386 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   203 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5314.42 ms /   671 tokens\n",
      " 30%|███       | 427/1400 [1:17:42<3:07:52, 11.59s/it]Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.39 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     248.79 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   226 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5603.80 ms /   309 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 477 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   477 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   150 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3997.16 ms /   627 tokens\n",
      " 31%|███       | 428/1400 [1:17:52<3:00:10, 11.12s/it]Llama.generate: 42 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   126 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     170.07 ms /   130 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 254 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   254 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     223.40 ms /   257 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   122 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   282 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7038.16 ms /   404 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 517 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   517 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   282 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7349.63 ms /   799 tokens\n",
      " 31%|███       | 429/1400 [1:18:07<3:17:54, 12.23s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     142.37 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.44 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   272 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6730.44 ms /   344 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   178 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4688.68 ms /   645 tokens\n",
      " 31%|███       | 430/1400 [1:18:19<3:15:29, 12.09s/it]Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     139.48 ms /    91 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   216 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.93 ms /   219 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   270 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6720.90 ms /   354 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 479 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   479 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   208 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5430.46 ms /   687 tokens\n",
      " 31%|███       | 431/1400 [1:18:31<3:17:18, 12.22s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     138.27 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.25 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   511 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12769.85 ms /   572 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   180 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4728.19 ms /   636 tokens\n",
      " 31%|███       | 432/1400 [1:18:49<3:44:21, 13.91s/it]Llama.generate: 42 prefix-match hit, remaining 181 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   181 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.75 ms /   184 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 309 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   309 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     242.76 ms /   312 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 178 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   178 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   355 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8952.80 ms /   533 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 572 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   572 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7031.19 ms /   841 tokens\n",
      " 31%|███       | 433/1400 [1:19:05<3:56:23, 14.67s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     156.30 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     241.62 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   222 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5509.12 ms /   292 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2764.75 ms /   566 tokens\n",
      " 31%|███       | 434/1400 [1:19:14<3:27:18, 12.88s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.78 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.17 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   183 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4540.16 ms /   245 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2736.87 ms /   557 tokens\n",
      " 31%|███       | 435/1400 [1:19:22<3:01:57, 11.31s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     134.36 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.22 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   256 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6330.45 ms /   334 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   141 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3753.19 ms /   613 tokens\n",
      " 31%|███       | 436/1400 [1:19:32<2:57:34, 11.05s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     124.49 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.86 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   346 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8624.55 ms /   404 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2655.50 ms /   549 tokens\n",
      " 31%|███       | 437/1400 [1:19:44<3:00:07, 11.22s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.19 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     253.45 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   163 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3980.65 ms /   222 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2535.17 ms /   545 tokens\n",
      " 31%|███▏      | 438/1400 [1:19:51<2:39:22,  9.94s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     138.74 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.73 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   341 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8475.92 ms /   404 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1874.62 ms /   520 tokens\n",
      " 31%|███▏      | 439/1400 [1:20:01<2:42:57, 10.17s/it]Llama.generate: 42 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   102 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     142.56 ms /   105 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 230 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   230 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     236.09 ms /   233 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   355 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8875.96 ms /   454 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 493 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   493 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   240 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6262.25 ms /   733 tokens\n",
      " 31%|███▏      | 440/1400 [1:20:17<3:08:33, 11.78s/it]Llama.generate: 42 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   108 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     147.62 ms /   111 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 236 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   236 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.19 ms /   239 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   104 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   295 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7334.36 ms /   399 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 499 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   499 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   185 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4850.57 ms /   684 tokens\n",
      " 32%|███▏      | 441/1400 [1:20:30<3:12:04, 12.02s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.49 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.01 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   262 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6500.24 ms /   327 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   147 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3899.64 ms /   606 tokens\n",
      " 32%|███▏      | 442/1400 [1:20:40<3:05:56, 11.65s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     156.40 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.59 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   315 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7812.76 ms /   391 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   207 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5392.10 ms /   677 tokens\n",
      " 32%|███▏      | 443/1400 [1:20:54<3:15:06, 12.23s/it]Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     144.63 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.09 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   330 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8236.53 ms /   416 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 482 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   482 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   153 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4075.09 ms /   635 tokens\n",
      " 32%|███▏      | 444/1400 [1:21:07<3:17:01, 12.37s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.93 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.16 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   270 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6673.00 ms /   339 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   127 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3418.40 ms /   592 tokens\n",
      " 32%|███▏      | 445/1400 [1:21:17<3:07:44, 11.80s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     158.26 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.31 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6333.25 ms /   315 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2611.16 ms /   549 tokens\n",
      " 32%|███▏      | 446/1400 [1:21:26<2:55:47, 11.06s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     134.02 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.74 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   200 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4949.24 ms /   266 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2276.55 ms /   542 tokens\n",
      " 32%|███▏      | 447/1400 [1:21:34<2:39:00, 10.01s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     126.94 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.89 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   289 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7142.90 ms /   348 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2611.63 ms /   548 tokens\n",
      " 32%|███▏      | 448/1400 [1:21:44<2:39:12, 10.03s/it]Llama.generate: 42 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   138 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.40 ms /   142 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 266 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   266 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     222.15 ms /   269 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   134 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   301 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7581.80 ms /   435 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 529 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   529 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   176 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4673.98 ms /   705 tokens\n",
      " 32%|███▏      | 449/1400 [1:21:57<2:51:44, 10.84s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     135.02 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.48 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   249 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6149.62 ms /   316 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   179 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4698.80 ms /   639 tokens\n",
      " 32%|███▏      | 450/1400 [1:22:08<2:53:15, 10.94s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     149.00 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.66 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   225 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5576.19 ms /   279 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2460.37 ms /   537 tokens\n",
      " 32%|███▏      | 451/1400 [1:22:16<2:41:03, 10.18s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.97 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.72 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   196 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4862.80 ms /   258 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    93 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2607.34 ms /   551 tokens\n",
      " 32%|███▏      | 452/1400 [1:22:24<2:29:54,  9.49s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     149.40 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.50 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   190 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4688.36 ms /   246 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2151.68 ms /   525 tokens\n",
      " 32%|███▏      | 453/1400 [1:22:31<2:18:58,  8.80s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     127.28 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     217.16 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   252 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6190.70 ms /   308 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   118 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3185.81 ms /   569 tokens\n",
      " 32%|███▏      | 454/1400 [1:22:41<2:23:16,  9.09s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.60 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     229.46 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   180 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4426.87 ms /   240 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2100.88 ms /   527 tokens\n",
      " 32%|███▎      | 455/1400 [1:22:48<2:12:58,  8.44s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.26 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.79 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   321 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7945.69 ms /   390 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   130 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3502.27 ms /   594 tokens\n",
      " 33%|███▎      | 456/1400 [1:23:00<2:28:49,  9.46s/it]Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.66 ms /    92 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   216 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     227.58 ms /   220 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   287 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7150.64 ms /   371 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 479 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   479 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   230 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5989.31 ms /   709 tokens\n",
      " 33%|███▎      | 457/1400 [1:23:14<2:47:57, 10.69s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     129.88 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.21 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   249 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6160.88 ms /   313 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1838.03 ms /   522 tokens\n",
      " 33%|███▎      | 458/1400 [1:23:22<2:36:44,  9.98s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     131.93 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.53 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   209 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5157.88 ms /   267 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2070.71 ms /   526 tokens\n",
      " 33%|███▎      | 459/1400 [1:23:29<2:25:12,  9.26s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     158.18 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     248.55 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   284 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7023.27 ms /   357 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2824.10 ms /   571 tokens\n",
      " 33%|███▎      | 460/1400 [1:23:40<2:29:52,  9.57s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     165.69 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.88 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   249 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6146.50 ms /   313 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   210 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5459.32 ms /   669 tokens\n",
      " 33%|███▎      | 461/1400 [1:23:52<2:41:10, 10.30s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.65 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.78 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   196 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4822.49 ms /   255 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    87 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2438.15 ms /   541 tokens\n",
      " 33%|███▎      | 462/1400 [1:23:59<2:28:32,  9.50s/it]Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.24 ms /   104 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   228 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     229.45 ms /   232 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   238 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5931.16 ms /   334 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 491 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   491 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   217 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5634.43 ms /   708 tokens\n",
      " 33%|███▎      | 463/1400 [1:24:11<2:40:03, 10.25s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.10 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.19 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   298 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7383.00 ms /   359 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2785.94 ms /   557 tokens\n",
      " 33%|███▎      | 464/1400 [1:24:22<2:41:15, 10.34s/it]Llama.generate: 42 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   118 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.23 ms /   121 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 246 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   246 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     239.49 ms /   249 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   115 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   395 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9884.24 ms /   510 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 509 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   509 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   279 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7245.01 ms /   788 tokens\n",
      " 33%|███▎      | 465/1400 [1:24:39<3:14:46, 12.50s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.20 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.21 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   194 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4805.60 ms /   260 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2615.98 ms /   555 tokens\n",
      " 33%|███▎      | 466/1400 [1:24:47<2:52:41, 11.09s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.07 ms /    89 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.63 ms /   217 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7520.89 ms /   385 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   158 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4161.09 ms /   634 tokens\n",
      " 33%|███▎      | 467/1400 [1:24:59<2:57:08, 11.39s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     148.66 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     224.21 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   162 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3951.11 ms /   218 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2620.88 ms /   545 tokens\n",
      " 33%|███▎      | 468/1400 [1:25:06<2:36:21, 10.07s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.83 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.90 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   268 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6643.42 ms /   342 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   223 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5820.35 ms /   692 tokens\n",
      " 34%|███▎      | 469/1400 [1:25:19<2:49:12, 10.91s/it]Llama.generate: 42 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   117 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     149.99 ms /   120 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 245 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   245 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.29 ms /   248 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   113 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7547.82 ms /   416 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 508 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   508 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   251 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6558.52 ms /   759 tokens\n",
      " 34%|███▎      | 470/1400 [1:25:34<3:05:40, 11.98s/it]Llama.generate: 42 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     148.16 ms /   101 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 226 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   226 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     249.39 ms /   229 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   404 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10090.46 ms /   499 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 489 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   489 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   165 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4361.93 ms /   654 tokens\n",
      " 34%|███▎      | 471/1400 [1:25:49<3:18:56, 12.85s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.86 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.71 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   277 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6852.75 ms /   345 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   263 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6824.09 ms /   726 tokens\n",
      " 34%|███▎      | 472/1400 [1:26:03<3:24:24, 13.22s/it]Llama.generate: 42 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   102 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.52 ms /   106 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 230 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   230 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     240.76 ms /   233 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6337.18 ms /   354 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 493 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   493 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5688.57 ms /   711 tokens\n",
      " 34%|███▍      | 473/1400 [1:26:15<3:20:42, 12.99s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     133.39 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.88 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   211 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5279.98 ms /   266 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1562.91 ms /   503 tokens\n",
      " 34%|███▍      | 474/1400 [1:26:22<2:53:44, 11.26s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.70 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.23 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   266 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6572.24 ms /   332 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   120 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3254.87 ms /   580 tokens\n",
      " 34%|███▍      | 475/1400 [1:26:32<2:48:36, 10.94s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.43 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.17 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   223 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5532.16 ms /   292 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   117 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3170.11 ms /   581 tokens\n",
      " 34%|███▍      | 476/1400 [1:26:42<2:39:54, 10.38s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.38 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     254.98 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   227 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5602.72 ms /   303 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3070.32 ms /   582 tokens\n",
      " 34%|███▍      | 477/1400 [1:26:51<2:33:51, 10.00s/it]Llama.generate: 42 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   110 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     165.38 ms /   114 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 238 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   238 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     226.46 ms /   242 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   106 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   191 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4733.92 ms /   297 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 501 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   501 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   160 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4258.98 ms /   661 tokens\n",
      " 34%|███▍      | 478/1400 [1:27:00<2:30:58,  9.82s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     140.07 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.40 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   237 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5848.37 ms /   297 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    93 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2550.37 ms /   548 tokens\n",
      " 34%|███▍      | 479/1400 [1:27:09<2:25:51,  9.50s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     129.79 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     217.57 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   321 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7955.64 ms /   394 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   153 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4056.77 ms /   620 tokens\n",
      " 34%|███▍      | 480/1400 [1:27:21<2:38:56, 10.37s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.06 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     218.52 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   250 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6145.38 ms /   305 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2384.83 ms /   536 tokens\n",
      " 34%|███▍      | 481/1400 [1:27:30<2:32:12,  9.94s/it]Llama.generate: 42 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   108 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     143.00 ms /   111 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 236 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   236 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     216.45 ms /   239 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   104 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   307 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7637.88 ms /   411 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 499 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   499 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   248 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6413.91 ms /   747 tokens\n",
      " 34%|███▍      | 482/1400 [1:27:45<2:52:41, 11.29s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.60 ms /    85 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.61 ms /   213 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   332 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8194.91 ms /   410 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   206 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5359.83 ms /   679 tokens\n",
      " 34%|███▍      | 483/1400 [1:27:59<3:04:46, 12.09s/it]Llama.generate: 42 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   111 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.00 ms /   115 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 239 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   239 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.36 ms /   242 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   107 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   317 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7864.15 ms /   424 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 502 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   502 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   200 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5225.11 ms /   702 tokens\n",
      " 35%|███▍      | 484/1400 [1:28:12<3:11:04, 12.52s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.45 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.13 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   204 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5032.74 ms /   264 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2857.33 ms /   559 tokens\n",
      " 35%|███▍      | 485/1400 [1:28:20<2:51:28, 11.24s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.00 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.23 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   297 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7352.83 ms /   369 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   171 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4495.98 ms /   638 tokens\n",
      " 35%|███▍      | 486/1400 [1:28:33<2:55:48, 11.54s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.84 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.28 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   400 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9951.43 ms /   475 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   140 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3753.33 ms /   608 tokens\n",
      " 35%|███▍      | 487/1400 [1:28:47<3:07:12, 12.30s/it]Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.64 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     232.76 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   230 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5709.83 ms /   313 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 477 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   477 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   109 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2959.79 ms /   586 tokens\n",
      " 35%|███▍      | 488/1400 [1:28:56<2:52:23, 11.34s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.03 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     218.52 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   273 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6734.90 ms /   339 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   185 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4838.89 ms /   645 tokens\n",
      " 35%|███▍      | 489/1400 [1:29:08<2:55:05, 11.53s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     165.45 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.67 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   193 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4767.74 ms /   256 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   159 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4179.97 ms /   616 tokens\n",
      " 35%|███▌      | 490/1400 [1:29:17<2:44:52, 10.87s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.47 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     250.23 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   223 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5499.85 ms /   284 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   144 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3829.07 ms /   600 tokens\n",
      " 35%|███▌      | 491/1400 [1:29:27<2:39:38, 10.54s/it]Llama.generate: 42 prefix-match hit, remaining 233 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   233 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     228.26 ms /   237 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 361 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   361 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     257.29 ms /   364 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 229 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   229 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   193 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4919.04 ms /   422 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 624 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   624 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   306 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8018.21 ms /   930 tokens\n",
      " 35%|███▌      | 492/1400 [1:29:40<2:52:41, 11.41s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     156.54 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.33 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   296 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7331.80 ms /   369 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   154 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4058.76 ms /   623 tokens\n",
      " 35%|███▌      | 493/1400 [1:29:52<2:54:13, 11.53s/it]Llama.generate: 42 prefix-match hit, remaining 154 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   154 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.16 ms /   157 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 282 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   282 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     223.56 ms /   285 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 151 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   151 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   358 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8964.91 ms /   509 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 545 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   545 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   281 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7290.72 ms /   826 tokens\n",
      " 35%|███▌      | 494/1400 [1:30:09<3:17:22, 13.07s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.62 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     249.44 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   296 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7344.30 ms /   367 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   129 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3444.30 ms /   595 tokens\n",
      " 35%|███▌      | 495/1400 [1:30:20<3:08:47, 12.52s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     136.12 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.36 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   263 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6478.30 ms /   325 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2493.56 ms /   547 tokens\n",
      " 35%|███▌      | 496/1400 [1:30:29<2:54:08, 11.56s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     135.76 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.78 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   317 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7842.30 ms /   385 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   142 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3746.95 ms /   605 tokens\n",
      " 36%|███▌      | 497/1400 [1:30:41<2:55:39, 11.67s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     133.13 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.86 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   249 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6117.22 ms /   324 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2986.04 ms /   580 tokens\n",
      " 36%|███▌      | 498/1400 [1:30:51<2:45:26, 11.00s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     124.85 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.25 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   186 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4552.26 ms /   243 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   134 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3561.88 ms /   585 tokens\n",
      " 36%|███▌      | 499/1400 [1:30:59<2:33:51, 10.25s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.42 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.51 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   265 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6538.44 ms /   323 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    78 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2175.76 ms /   530 tokens\n",
      " 36%|███▌      | 500/1400 [1:31:08<2:28:29,  9.90s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.38 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     266.94 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   217 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5363.73 ms /   292 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2745.81 ms /   570 tokens\n",
      " 36%|███▌      | 501/1400 [1:31:17<2:22:19,  9.50s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.99 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.36 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5791.79 ms /   301 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   217 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5636.89 ms /   676 tokens\n",
      " 36%|███▌      | 502/1400 [1:31:29<2:32:31, 10.19s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     137.98 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.74 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   217 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5323.92 ms /   279 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2069.75 ms /   529 tokens\n",
      " 36%|███▌      | 503/1400 [1:31:36<2:21:21,  9.46s/it]Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.63 ms /   104 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   228 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.43 ms /   231 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 97 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    97 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   369 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9203.72 ms /   466 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 491 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   491 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   250 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6491.34 ms /   741 tokens\n",
      " 36%|███▌      | 504/1400 [1:31:52<2:50:54, 11.44s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.69 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     259.69 ms /   216 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   219 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5415.92 ms /   299 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   106 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2900.28 ms /   581 tokens\n",
      " 36%|███▌      | 505/1400 [1:32:01<2:38:43, 10.64s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.83 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.74 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   234 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5791.40 ms /   296 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   115 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3116.57 ms /   572 tokens\n",
      " 36%|███▌      | 506/1400 [1:32:11<2:32:30, 10.24s/it]Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     145.86 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.40 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   370 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9219.95 ms /   457 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 482 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   482 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   188 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4947.25 ms /   670 tokens\n",
      " 36%|███▌      | 507/1400 [1:32:25<2:51:37, 11.53s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.98 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.03 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   280 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6897.55 ms /   344 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   228 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5889.86 ms /   687 tokens\n",
      " 36%|███▋      | 508/1400 [1:32:38<2:58:40, 12.02s/it]Llama.generate: 42 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   104 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     137.91 ms /   107 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 232 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   232 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.53 ms /   235 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6649.73 ms /   370 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 495 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   495 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   308 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7950.49 ms /   803 tokens\n",
      " 36%|███▋      | 509/1400 [1:32:53<3:11:36, 12.90s/it]Llama.generate: 42 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     165.93 ms /   107 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 231 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   231 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     225.64 ms /   235 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   293 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7303.23 ms /   392 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 494 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   494 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   226 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5896.00 ms /   720 tokens\n",
      " 36%|███▋      | 510/1400 [1:33:07<3:14:35, 13.12s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.94 ms /    85 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     219.28 ms /   213 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   224 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5518.87 ms /   300 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   120 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3235.57 ms /   592 tokens\n",
      " 36%|███▋      | 511/1400 [1:33:16<2:56:48, 11.93s/it]Llama.generate: 42 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     139.24 ms /    97 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 222 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   222 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.89 ms /   225 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   388 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9683.77 ms /   478 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 485 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   485 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   253 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6534.57 ms /   738 tokens\n",
      " 37%|███▋      | 512/1400 [1:33:33<3:17:13, 13.33s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     156.91 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.54 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   282 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6963.80 ms /   344 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2847.42 ms /   561 tokens\n",
      " 37%|███▋      | 513/1400 [1:33:43<3:03:03, 12.38s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     137.37 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.17 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   262 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6459.50 ms /   320 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   133 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3538.34 ms /   586 tokens\n",
      " 37%|███▋      | 514/1400 [1:33:53<2:53:56, 11.78s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     130.29 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.75 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   261 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6404.49 ms /   326 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2099.23 ms /   532 tokens\n",
      " 37%|███▋      | 515/1400 [1:34:02<2:40:53, 10.91s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.40 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.84 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   315 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7797.02 ms /   386 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   124 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3323.02 ms /   591 tokens\n",
      " 37%|███▋      | 516/1400 [1:34:13<2:43:22, 11.09s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     170.60 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.79 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   197 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4860.06 ms /   264 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2496.75 ms /   551 tokens\n",
      " 37%|███▋      | 517/1400 [1:34:21<2:28:31, 10.09s/it]Llama.generate: 42 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   128 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     158.82 ms /   131 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 256 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   256 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.50 ms /   259 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   124 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   342 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8508.16 ms /   466 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 519 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   519 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   197 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5170.30 ms /   716 tokens\n",
      " 37%|███▋      | 518/1400 [1:34:35<2:45:53, 11.29s/it]Llama.generate: 42 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    93 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     136.61 ms /    96 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   221 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.32 ms /   224 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7540.76 ms /   393 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 484 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   484 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   145 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3865.00 ms /   629 tokens\n",
      " 37%|███▋      | 519/1400 [1:34:47<2:47:51, 11.43s/it]Llama.generate: 42 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   138 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.29 ms /   142 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 266 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   266 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     246.04 ms /   270 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 134 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   134 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   259 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6477.59 ms /   393 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 529 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   529 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   182 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4842.20 ms /   711 tokens\n",
      " 37%|███▋      | 520/1400 [1:34:59<2:49:11, 11.54s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.82 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.23 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   271 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6642.46 ms /   337 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   130 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3464.71 ms /   590 tokens\n",
      " 37%|███▋      | 521/1400 [1:35:09<2:44:25, 11.22s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.08 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.40 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   475 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11819.73 ms /   535 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   115 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3121.11 ms /   570 tokens\n",
      " 37%|███▋      | 522/1400 [1:35:25<3:02:16, 12.46s/it]Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     141.48 ms /    93 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   218 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.49 ms /   221 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   285 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7029.95 ms /   372 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 481 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   481 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   146 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3886.65 ms /   627 tokens\n",
      " 37%|███▋      | 523/1400 [1:35:36<2:56:52, 12.10s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.68 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.60 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   277 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6794.80 ms /   344 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   131 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3497.48 ms /   593 tokens\n",
      " 37%|███▋      | 524/1400 [1:35:47<2:50:31, 11.68s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.25 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.04 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   312 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7687.06 ms /   377 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   114 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3084.23 ms /   574 tokens\n",
      " 38%|███▊      | 525/1400 [1:35:58<2:48:15, 11.54s/it]Llama.generate: 42 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   108 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.94 ms /   111 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 236 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   236 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.27 ms /   239 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   105 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   339 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8411.86 ms /   444 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 499 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   499 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   187 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4899.42 ms /   686 tokens\n",
      " 38%|███▊      | 526/1400 [1:36:12<2:57:37, 12.19s/it]Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.22 ms /    96 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.00 ms /   223 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   345 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8549.87 ms /   433 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 483 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   483 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   185 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4862.95 ms /   668 tokens\n",
      " 38%|███▊      | 527/1400 [1:36:25<3:04:29, 12.68s/it]Llama.generate: 42 prefix-match hit, remaining 184 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   184 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.61 ms /   188 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 312 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   312 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     241.87 ms /   315 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 180 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   180 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   359 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8967.49 ms /   539 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 575 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   575 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   374 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9731.65 ms /   949 tokens\n",
      " 38%|███▊      | 528/1400 [1:36:45<3:32:36, 14.63s/it]Llama.generate: 42 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   138 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     223.21 ms /   142 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 266 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   266 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     220.79 ms /   269 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   135 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   291 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7258.10 ms /   426 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 529 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   529 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   200 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5274.94 ms /   729 tokens\n",
      " 38%|███▊      | 529/1400 [1:36:58<3:25:17, 14.14s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.03 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.73 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   294 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7217.49 ms /   359 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   156 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4180.32 ms /   615 tokens\n",
      " 38%|███▊      | 530/1400 [1:37:09<3:14:52, 13.44s/it]Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.19 ms /   103 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   228 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.01 ms /   231 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   285 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7043.19 ms /   381 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 491 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   491 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   201 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5277.75 ms /   692 tokens\n",
      " 38%|███▊      | 531/1400 [1:37:22<3:11:34, 13.23s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.93 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.09 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   170 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4135.80 ms /   224 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2515.28 ms /   539 tokens\n",
      " 38%|███▊      | 532/1400 [1:37:29<2:44:35, 11.38s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.54 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.34 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   220 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5367.04 ms /   274 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2333.24 ms /   534 tokens\n",
      " 38%|███▊      | 533/1400 [1:37:37<2:30:09, 10.39s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.63 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.48 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   237 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5799.82 ms /   306 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   143 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3775.96 ms /   607 tokens\n",
      " 38%|███▊      | 534/1400 [1:37:47<2:28:07, 10.26s/it]Llama.generate: 42 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    93 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.25 ms /    97 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   221 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.59 ms /   225 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   223 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5505.17 ms /   312 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 484 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   484 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   142 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3794.92 ms /   626 tokens\n",
      " 38%|███▊      | 535/1400 [1:37:57<2:25:41, 10.11s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.15 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.20 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   193 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4705.31 ms /   251 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    87 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2395.73 ms /   539 tokens\n",
      " 38%|███▊      | 536/1400 [1:38:05<2:14:23,  9.33s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.51 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.15 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   331 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8177.65 ms /   408 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   138 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3676.78 ms /   610 tokens\n",
      " 38%|███▊      | 537/1400 [1:38:17<2:26:58, 10.22s/it]Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.76 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   218 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.82 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   315 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7795.80 ms /   401 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 481 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   481 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   124 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3336.65 ms /   605 tokens\n",
      " 38%|███▊      | 538/1400 [1:38:28<2:32:40, 10.63s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.86 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.40 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   205 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4995.95 ms /   259 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    87 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2411.79 ms /   536 tokens\n",
      " 38%|███▊      | 539/1400 [1:38:36<2:20:28,  9.79s/it]Llama.generate: 42 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   104 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.09 ms /   107 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 232 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   232 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.24 ms /   235 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   286 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7073.88 ms /   387 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 495 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   495 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   217 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5639.65 ms /   712 tokens\n",
      " 39%|███▊      | 540/1400 [1:38:49<2:34:38, 10.79s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.66 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.58 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   241 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5878.09 ms /   303 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   144 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3853.28 ms /   601 tokens\n",
      " 39%|███▊      | 541/1400 [1:38:59<2:31:39, 10.59s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.03 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.88 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   253 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6202.19 ms /   323 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   192 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5012.28 ms /   656 tokens\n",
      " 39%|███▊      | 542/1400 [1:39:11<2:35:50, 10.90s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     128.51 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.19 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   263 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6432.34 ms /   321 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2727.54 ms /   553 tokens\n",
      " 39%|███▉      | 543/1400 [1:39:21<2:29:39, 10.48s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.35 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.54 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   261 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6414.21 ms /   330 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   128 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3424.99 ms /   592 tokens\n",
      " 39%|███▉      | 544/1400 [1:39:31<2:28:33, 10.41s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.45 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.77 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   165 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4015.36 ms /   221 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    87 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2427.00 ms /   538 tokens\n",
      " 39%|███▉      | 545/1400 [1:39:38<2:13:13,  9.35s/it]Llama.generate: 42 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   123 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.10 ms /   127 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 251 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   251 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.86 ms /   254 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   120 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7510.75 ms /   423 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 514 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   514 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7877.97 ms /   817 tokens\n",
      " 39%|███▉      | 546/1400 [1:39:54<2:40:43, 11.29s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.62 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.20 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   321 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7965.79 ms /   396 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   142 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3784.24 ms /   612 tokens\n",
      " 39%|███▉      | 547/1400 [1:40:06<2:44:04, 11.54s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.39 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     169.91 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   267 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6536.19 ms /   327 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   165 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4345.46 ms /   620 tokens\n",
      " 39%|███▉      | 548/1400 [1:40:17<2:42:41, 11.46s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     219.02 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.04 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   201 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4908.15 ms /   272 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2533.37 ms /   558 tokens\n",
      " 39%|███▉      | 549/1400 [1:40:25<2:27:20, 10.39s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.18 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.38 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   285 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7017.23 ms /   359 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   197 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5113.74 ms /   666 tokens\n",
      " 39%|███▉      | 550/1400 [1:40:37<2:36:25, 11.04s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.18 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.37 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   304 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7466.06 ms /   363 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2787.54 ms /   555 tokens\n",
      " 39%|███▉      | 551/1400 [1:40:48<2:34:34, 10.92s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.94 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.00 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   273 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6702.35 ms /   345 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2922.77 ms /   574 tokens\n",
      " 39%|███▉      | 552/1400 [1:40:58<2:30:45, 10.67s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.85 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.26 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   278 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6815.81 ms /   336 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2146.77 ms /   529 tokens\n",
      " 40%|███▉      | 553/1400 [1:41:07<2:25:02, 10.27s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.22 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.45 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   235 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5744.78 ms /   293 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   106 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2880.90 ms /   558 tokens\n",
      " 40%|███▉      | 554/1400 [1:41:16<2:19:40,  9.91s/it]Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.91 ms /    98 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.45 ms /   226 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   295 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7273.37 ms /   386 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   486 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   153 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4058.71 ms /   639 tokens\n",
      " 40%|███▉      | 555/1400 [1:41:28<2:27:19, 10.46s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.03 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.70 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5790.58 ms /   311 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   118 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3176.58 ms /   588 tokens\n",
      " 40%|███▉      | 556/1400 [1:41:38<2:22:29, 10.13s/it]Llama.generate: 42 prefix-match hit, remaining 139 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   139 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.63 ms /   143 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 267 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   267 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     245.33 ms /   271 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   135 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   301 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7505.50 ms /   436 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 530 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   530 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   219 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5743.42 ms /   749 tokens\n",
      " 40%|███▉      | 557/1400 [1:41:51<2:37:30, 11.21s/it]Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     170.29 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.12 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   277 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6812.52 ms /   359 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 477 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   477 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   165 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4347.19 ms /   642 tokens\n",
      " 40%|███▉      | 558/1400 [1:42:03<2:38:49, 11.32s/it]Llama.generate: 42 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   112 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.56 ms /   116 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 240 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   240 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     232.14 ms /   244 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   108 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   260 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6440.08 ms /   368 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 503 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   503 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   145 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3878.17 ms /   648 tokens\n",
      " 40%|███▉      | 559/1400 [1:42:14<2:36:22, 11.16s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.30 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.59 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   208 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5084.77 ms /   278 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3059.28 ms /   577 tokens\n",
      " 40%|████      | 560/1400 [1:42:22<2:25:19, 10.38s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     147.04 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.74 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   275 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6748.46 ms /   339 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   106 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2886.67 ms /   564 tokens\n",
      " 40%|████      | 561/1400 [1:42:32<2:23:31, 10.26s/it]Llama.generate: 42 prefix-match hit, remaining 164 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   164 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.29 ms /   167 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 292 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   292 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     238.95 ms /   295 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 161 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   161 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   335 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8388.82 ms /   496 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 555 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   555 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   222 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5830.68 ms /   777 tokens\n",
      " 40%|████      | 562/1400 [1:42:47<2:41:49, 11.59s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.71 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.63 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   293 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7216.17 ms /   368 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   217 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5644.06 ms /   686 tokens\n",
      " 40%|████      | 563/1400 [1:43:00<2:48:33, 12.08s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.35 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.07 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   287 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7047.93 ms /   348 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2845.24 ms /   560 tokens\n",
      " 40%|████      | 564/1400 [1:43:10<2:41:00, 11.56s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.39 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.38 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   244 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5958.32 ms /   302 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2330.53 ms /   537 tokens\n",
      " 40%|████      | 565/1400 [1:43:19<2:28:45, 10.69s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.04 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.65 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   264 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6467.70 ms /   326 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2600.06 ms /   550 tokens\n",
      " 40%|████      | 566/1400 [1:43:29<2:23:37, 10.33s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.23 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.15 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   304 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7468.46 ms /   363 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2669.36 ms /   551 tokens\n",
      " 40%|████      | 567/1400 [1:43:39<2:24:21, 10.40s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.38 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.54 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   414 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10283.56 ms /   473 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   174 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4548.44 ms /   628 tokens\n",
      " 41%|████      | 568/1400 [1:43:54<2:44:11, 11.84s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.52 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.47 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   213 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5187.17 ms /   274 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    73 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2099.09 ms /   528 tokens\n",
      " 41%|████      | 569/1400 [1:44:02<2:26:52, 10.60s/it]Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.67 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.56 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5632.97 ms /   315 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 482 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   482 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   184 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4826.58 ms /   666 tokens\n",
      " 41%|████      | 570/1400 [1:44:13<2:27:44, 10.68s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.17 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.66 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   275 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6751.70 ms /   343 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   174 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4543.98 ms /   636 tokens\n",
      " 41%|████      | 571/1400 [1:44:25<2:31:48, 10.99s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     147.40 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.17 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   339 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8445.37 ms /   410 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   132 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3509.43 ms /   599 tokens\n",
      " 41%|████      | 572/1400 [1:44:37<2:37:08, 11.39s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.75 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.93 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7492.58 ms /   375 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   150 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3959.31 ms /   618 tokens\n",
      " 41%|████      | 573/1400 [1:44:49<2:38:52, 11.53s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.76 ms /    86 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     215.34 ms /   214 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   274 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6784.11 ms /   351 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   186 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4884.04 ms /   659 tokens\n",
      " 41%|████      | 574/1400 [1:45:01<2:40:58, 11.69s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.64 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     169.64 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   225 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5504.42 ms /   283 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   146 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3854.26 ms /   599 tokens\n",
      " 41%|████      | 575/1400 [1:45:11<2:32:43, 11.11s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.73 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.30 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   290 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7118.08 ms /   352 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2384.73 ms /   541 tokens\n",
      " 41%|████      | 576/1400 [1:45:21<2:27:25, 10.74s/it]Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.04 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.23 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   284 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7017.03 ms /   372 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 482 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   482 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   146 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3859.88 ms /   628 tokens\n",
      " 41%|████      | 577/1400 [1:45:32<2:29:27, 10.90s/it]Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.29 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.91 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   336 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8334.52 ms /   420 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 477 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   477 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   203 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5332.54 ms /   680 tokens\n",
      " 41%|████▏     | 578/1400 [1:45:46<2:42:27, 11.86s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.71 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.91 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   389 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9692.69 ms /   452 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   195 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5107.97 ms /   652 tokens\n",
      " 41%|████▏     | 579/1400 [1:46:01<2:55:56, 12.86s/it]Llama.generate: 42 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   108 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.84 ms /   112 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 236 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   236 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     218.13 ms /   240 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   105 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   231 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5679.43 ms /   336 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 499 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   499 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   136 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3617.63 ms /   635 tokens\n",
      " 41%|████▏     | 580/1400 [1:46:11<2:42:58, 11.92s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.30 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.43 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   228 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5597.76 ms /   296 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   144 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3793.34 ms /   607 tokens\n",
      " 42%|████▏     | 581/1400 [1:46:21<2:34:01, 11.28s/it]Llama.generate: 42 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   125 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     240.84 ms /   129 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 253 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   253 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     227.99 ms /   257 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   121 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   352 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8752.51 ms /   473 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 516 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   516 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   254 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6614.36 ms /   770 tokens\n",
      " 42%|████▏     | 582/1400 [1:46:37<2:52:34, 12.66s/it]Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.81 ms /   103 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   227 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.97 ms /   230 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   345 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8567.98 ms /   441 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 490 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   490 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   244 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6325.00 ms /   734 tokens\n",
      " 42%|████▏     | 583/1400 [1:46:52<3:03:16, 13.46s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.88 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.32 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   289 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7118.30 ms /   355 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3031.93 ms /   573 tokens\n",
      " 42%|████▏     | 584/1400 [1:47:02<2:51:07, 12.58s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     222.46 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.36 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   238 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5846.34 ms /   299 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   120 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3223.73 ms /   575 tokens\n",
      " 42%|████▏     | 585/1400 [1:47:12<2:38:19, 11.66s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.92 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.71 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   200 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4903.77 ms /   256 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2309.48 ms /   533 tokens\n",
      " 42%|████▏     | 586/1400 [1:47:20<2:21:43, 10.45s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.15 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.00 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   233 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5711.25 ms /   297 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2066.78 ms /   530 tokens\n",
      " 42%|████▏     | 587/1400 [1:47:28<2:12:24,  9.77s/it]Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.01 ms /   100 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 224 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   224 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.90 ms /   227 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    93 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   329 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8149.98 ms /   422 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 487 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   487 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   166 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4384.28 ms /   653 tokens\n",
      " 42%|████▏     | 588/1400 [1:47:41<2:25:12, 10.73s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.79 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.70 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   295 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7265.37 ms /   359 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1982.18 ms /   528 tokens\n",
      " 42%|████▏     | 589/1400 [1:47:50<2:20:27, 10.39s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.74 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.21 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   231 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5737.77 ms /   295 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3053.91 ms /   571 tokens\n",
      " 42%|████▏     | 590/1400 [1:48:00<2:15:35, 10.04s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.83 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.47 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   264 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6486.79 ms /   334 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   115 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3134.14 ms /   580 tokens\n",
      " 42%|████▏     | 591/1400 [1:48:10<2:15:29, 10.05s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.16 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.09 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   289 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7113.32 ms /   356 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   182 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4763.02 ms /   643 tokens\n",
      " 42%|████▏     | 592/1400 [1:48:22<2:24:18, 10.72s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.15 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.36 ms /   189 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   195 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4744.95 ms /   249 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   233 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6043.81 ms /   682 tokens\n",
      " 42%|████▏     | 593/1400 [1:48:33<2:26:00, 10.86s/it]Llama.generate: 42 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   102 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     235.75 ms /   106 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 230 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   230 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     223.44 ms /   234 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   290 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7227.18 ms /   389 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 493 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   493 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   148 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3922.88 ms /   641 tokens\n",
      " 42%|████▏     | 594/1400 [1:48:45<2:28:58, 11.09s/it]Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     233.96 ms /   100 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 224 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   224 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     215.53 ms /   228 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   342 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8512.44 ms /   434 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 487 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   487 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   140 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3725.35 ms /   627 tokens\n",
      " 42%|████▎     | 595/1400 [1:48:57<2:35:19, 11.58s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.86 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.55 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6262.58 ms /   329 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   153 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4045.34 ms /   622 tokens\n",
      " 43%|████▎     | 596/1400 [1:49:08<2:31:41, 11.32s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     170.16 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.60 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   357 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8822.87 ms /   412 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   232 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5994.96 ms /   682 tokens\n",
      " 43%|████▎     | 597/1400 [1:49:23<2:47:02, 12.48s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.71 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.19 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   241 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5894.36 ms /   302 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3243.09 ms /   578 tokens\n",
      " 43%|████▎     | 598/1400 [1:49:33<2:35:09, 11.61s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.23 ms /    85 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.68 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   320 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7901.75 ms /   397 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   156 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4148.15 ms /   628 tokens\n",
      " 43%|████▎     | 599/1400 [1:49:45<2:38:24, 11.87s/it]Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.85 ms /   102 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   227 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.32 ms /   230 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   281 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6943.57 ms /   377 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 490 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   490 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   190 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4981.21 ms /   680 tokens\n",
      " 43%|████▎     | 600/1400 [1:49:58<2:40:02, 12.00s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.08 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.79 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   223 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5453.61 ms /   284 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2863.23 ms /   560 tokens\n",
      " 43%|████▎     | 601/1400 [1:50:06<2:26:44, 11.02s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.57 ms /    85 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.46 ms /   213 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   240 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5905.09 ms /   319 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   211 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5471.86 ms /   684 tokens\n",
      " 43%|████▎     | 602/1400 [1:50:18<2:29:32, 11.24s/it]Llama.generate: 42 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   135 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.92 ms /   139 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 263 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   263 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     242.11 ms /   267 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   132 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   234 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5796.93 ms /   366 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 526 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   526 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   226 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5929.25 ms /   752 tokens\n",
      " 43%|████▎     | 603/1400 [1:50:30<2:33:12, 11.53s/it]Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.78 ms /   100 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 224 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   224 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.68 ms /   228 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   257 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6337.64 ms /   349 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 487 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   487 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   182 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4785.75 ms /   669 tokens\n",
      " 43%|████▎     | 604/1400 [1:50:42<2:33:03, 11.54s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.81 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.16 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   341 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8414.28 ms /   402 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1962.17 ms /   524 tokens\n",
      " 43%|████▎     | 605/1400 [1:50:53<2:29:49, 11.31s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.20 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.48 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   259 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6359.59 ms /   333 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   145 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3885.53 ms /   614 tokens\n",
      " 43%|████▎     | 606/1400 [1:51:03<2:27:00, 11.11s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.67 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.24 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   254 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6226.90 ms /   315 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   128 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3422.52 ms /   584 tokens\n",
      " 43%|████▎     | 607/1400 [1:51:13<2:22:39, 10.79s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.19 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.41 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   243 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5946.25 ms /   305 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   166 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4340.75 ms /   622 tokens\n",
      " 43%|████▎     | 608/1400 [1:51:24<2:21:54, 10.75s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.77 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.26 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   211 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5169.19 ms /   273 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   119 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3239.36 ms /   576 tokens\n",
      " 44%|████▎     | 609/1400 [1:51:33<2:13:59, 10.16s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     217.89 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.97 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   199 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4911.01 ms /   275 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   126 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3406.86 ms /   597 tokens\n",
      " 44%|████▎     | 610/1400 [1:51:42<2:08:19,  9.75s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.77 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.17 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   272 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6679.66 ms /   348 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   242 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6260.32 ms /   714 tokens\n",
      " 44%|████▎     | 611/1400 [1:51:55<2:22:17, 10.82s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.27 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.44 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   295 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7236.88 ms /   365 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2764.68 ms /   565 tokens\n",
      " 44%|████▎     | 612/1400 [1:52:05<2:20:34, 10.70s/it]Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     233.63 ms /    95 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.24 ms /   223 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   259 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6387.21 ms /   346 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 482 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   482 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   121 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3253.78 ms /   603 tokens\n",
      " 44%|████▍     | 613/1400 [1:52:15<2:18:04, 10.53s/it]Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.06 ms /    96 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     216.22 ms /   224 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5615.43 ms /   317 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 483 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   483 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   121 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3273.36 ms /   604 tokens\n",
      " 44%|████▍     | 614/1400 [1:52:25<2:13:04, 10.16s/it]Llama.generate: 42 prefix-match hit, remaining 147 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   147 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.10 ms /   151 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 275 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   275 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     249.59 ms /   279 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 144 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   144 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   401 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10071.34 ms /   545 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 538 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   538 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   237 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6203.28 ms /   775 tokens\n",
      " 44%|████▍     | 615/1400 [1:52:42<2:38:50, 12.14s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.01 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.94 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   286 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7022.22 ms /   351 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   194 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5078.02 ms /   654 tokens\n",
      " 44%|████▍     | 616/1400 [1:52:54<2:40:05, 12.25s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.64 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.14 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   221 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5389.89 ms /   278 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2323.50 ms /   535 tokens\n",
      " 44%|████▍     | 617/1400 [1:53:02<2:23:46, 11.02s/it]Llama.generate: 42 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   121 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.13 ms /   125 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 249 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   249 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     228.33 ms /   253 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   117 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   260 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6430.68 ms /   377 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 512 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   512 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   162 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4284.16 ms /   674 tokens\n",
      " 44%|████▍     | 618/1400 [1:53:13<2:24:11, 11.06s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.27 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.18 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   275 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6804.75 ms /   346 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   129 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3471.52 ms /   595 tokens\n",
      " 44%|████▍     | 619/1400 [1:53:24<2:22:36, 10.96s/it]Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.43 ms /    96 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.81 ms /   224 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   267 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6560.46 ms /   356 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 483 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   483 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5677.21 ms /   701 tokens\n",
      " 44%|████▍     | 620/1400 [1:53:37<2:29:10, 11.47s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.39 ms /    89 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.39 ms /   217 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   255 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6301.99 ms /   338 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   117 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3171.48 ms /   593 tokens\n",
      " 44%|████▍     | 621/1400 [1:53:47<2:22:52, 11.00s/it]Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.83 ms /    91 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   216 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.26 ms /   219 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   306 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7552.55 ms /   390 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 479 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   479 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   253 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6549.60 ms /   732 tokens\n",
      " 44%|████▍     | 622/1400 [1:54:01<2:36:16, 12.05s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.81 ms /    85 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.98 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   259 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6392.58 ms /   337 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   111 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3013.31 ms /   583 tokens\n",
      " 44%|████▍     | 623/1400 [1:54:11<2:27:28, 11.39s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.51 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.29 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   361 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8890.70 ms /   417 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2614.39 ms /   547 tokens\n",
      " 45%|████▍     | 624/1400 [1:54:23<2:29:11, 11.54s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.66 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.49 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   287 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7028.10 ms /   344 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2336.64 ms /   537 tokens\n",
      " 45%|████▍     | 625/1400 [1:54:33<2:22:02, 11.00s/it]Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.37 ms /   100 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 224 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   224 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.26 ms /   228 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   243 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6050.29 ms /   335 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 487 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   487 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   142 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3764.23 ms /   629 tokens\n",
      " 45%|████▍     | 626/1400 [1:54:43<2:18:57, 10.77s/it]Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.45 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.41 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   261 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6418.43 ms /   348 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 482 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   482 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   153 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4039.62 ms /   635 tokens\n",
      " 45%|████▍     | 627/1400 [1:54:54<2:19:07, 10.80s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     140.65 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.62 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   283 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6986.83 ms /   360 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   141 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3766.37 ms /   612 tokens\n",
      " 45%|████▍     | 628/1400 [1:55:05<2:20:08, 10.89s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.10 ms /    91 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.86 ms /   219 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   230 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5696.28 ms /   314 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2877.70 ms /   582 tokens\n",
      " 45%|████▍     | 629/1400 [1:55:14<2:12:44, 10.33s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.52 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.07 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   267 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6573.92 ms /   326 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2882.20 ms /   558 tokens\n",
      " 45%|████▌     | 630/1400 [1:55:24<2:10:37, 10.18s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.74 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.18 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   242 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5983.25 ms /   304 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2593.27 ms /   551 tokens\n",
      " 45%|████▌     | 631/1400 [1:55:33<2:05:59,  9.83s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.39 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.51 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   286 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7013.04 ms /   351 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2883.35 ms /   566 tokens\n",
      " 45%|████▌     | 632/1400 [1:55:43<2:07:29,  9.96s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.06 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.10 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   217 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5292.60 ms /   277 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   105 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2841.68 ms /   560 tokens\n",
      " 45%|████▌     | 633/1400 [1:55:51<2:01:44,  9.52s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.59 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.85 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   293 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7220.57 ms /   374 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   148 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3933.58 ms /   623 tokens\n",
      " 45%|████▌     | 634/1400 [1:56:03<2:09:21, 10.13s/it]Llama.generate: 42 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.50 ms /   106 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 231 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   231 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.16 ms /   234 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   317 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7917.53 ms /   416 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 494 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   494 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   214 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5565.88 ms /   708 tokens\n",
      " 45%|████▌     | 635/1400 [1:56:17<2:23:37, 11.26s/it]Llama.generate: 42 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    93 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.10 ms /    96 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   221 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.70 ms /   224 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   279 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6868.26 ms /   369 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 484 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   484 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   159 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4199.34 ms /   643 tokens\n",
      " 45%|████▌     | 636/1400 [1:56:28<2:24:16, 11.33s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.77 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.75 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   263 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6449.34 ms /   335 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   185 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4824.69 ms /   652 tokens\n",
      " 46%|████▌     | 637/1400 [1:56:40<2:25:22, 11.43s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.89 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.05 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   245 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6028.60 ms /   317 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2700.40 ms /   564 tokens\n",
      " 46%|████▌     | 638/1400 [1:56:49<2:16:34, 10.75s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     233.81 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.53 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   284 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7019.23 ms /   359 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   106 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2871.12 ms /   575 tokens\n",
      " 46%|████▌     | 639/1400 [1:57:00<2:14:53, 10.64s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.40 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.87 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   305 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7501.31 ms /   372 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   149 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3940.56 ms /   610 tokens\n",
      " 46%|████▌     | 640/1400 [1:57:11<2:19:15, 10.99s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.22 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.63 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   222 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5437.67 ms /   277 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    93 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2609.66 ms /   543 tokens\n",
      " 46%|████▌     | 641/1400 [1:57:20<2:09:25, 10.23s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.12 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.38 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   308 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7632.60 ms /   380 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   157 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4166.47 ms /   624 tokens\n",
      " 46%|████▌     | 642/1400 [1:57:32<2:16:53, 10.84s/it]Llama.generate: 42 prefix-match hit, remaining 174 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   174 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.48 ms /   178 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 302 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   302 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     239.19 ms /   305 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 170 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   170 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   367 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9185.33 ms /   537 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 565 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   565 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   280 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7287.08 ms /   845 tokens\n",
      " 46%|████▌     | 643/1400 [1:57:49<2:39:52, 12.67s/it]Llama.generate: 42 prefix-match hit, remaining 144 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   144 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.50 ms /   148 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 272 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   272 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     246.42 ms /   276 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 141 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   141 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   357 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8986.03 ms /   498 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 535 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   535 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   220 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5740.75 ms /   755 tokens\n",
      " 46%|████▌     | 644/1400 [1:58:04<2:49:15, 13.43s/it]Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.72 ms /    99 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 224 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   224 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.31 ms /   227 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   240 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5909.31 ms /   332 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 487 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   487 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   216 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5580.93 ms /   703 tokens\n",
      " 46%|████▌     | 645/1400 [1:58:16<2:43:11, 12.97s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     132.54 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.49 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   207 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5079.29 ms /   263 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   183 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4778.69 ms /   633 tokens\n",
      " 46%|████▌     | 646/1400 [1:58:26<2:32:30, 12.14s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     151.91 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.86 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   267 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6589.99 ms /   345 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   137 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3659.28 ms /   609 tokens\n",
      " 46%|████▌     | 647/1400 [1:58:37<2:26:34, 11.68s/it]Llama.generate: 42 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   119 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.12 ms /   123 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 247 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   247 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     222.25 ms /   251 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   115 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   247 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6134.12 ms /   362 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 510 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   510 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   173 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4561.12 ms /   683 tokens\n",
      " 46%|████▋     | 648/1400 [1:58:48<2:24:22, 11.52s/it]Llama.generate: 42 prefix-match hit, remaining 157 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   157 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     217.67 ms /   161 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 285 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   285 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     226.65 ms /   288 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 153 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   153 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   354 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8875.72 ms /   507 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 548 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   548 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   253 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6591.62 ms /   801 tokens\n",
      " 46%|████▋     | 649/1400 [1:59:04<2:40:47, 12.85s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.15 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.33 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   216 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5303.32 ms /   272 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1790.94 ms /   514 tokens\n",
      " 46%|████▋     | 650/1400 [1:59:12<2:20:22, 11.23s/it]Llama.generate: 42 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    93 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.86 ms /    96 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   221 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.41 ms /   224 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   395 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9912.21 ms /   485 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 484 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   484 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   316 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8122.92 ms /   800 tokens\n",
      " 46%|████▋     | 651/1400 [1:59:30<2:47:10, 13.39s/it]Llama.generate: 42 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   122 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.71 ms /   125 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 250 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   250 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.21 ms /   253 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   118 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   295 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7376.38 ms /   413 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 513 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   513 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   347 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9004.32 ms /   860 tokens\n",
      " 47%|████▋     | 652/1400 [1:59:47<2:59:35, 14.41s/it]Llama.generate: 42 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   111 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.66 ms /   115 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 239 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   239 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.19 ms /   242 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   107 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   300 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7463.50 ms /   407 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 502 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   502 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   179 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4709.55 ms /   681 tokens\n",
      " 47%|████▋     | 653/1400 [1:59:59<2:52:38, 13.87s/it]Llama.generate: 42 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   108 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     217.76 ms /   112 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 236 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   236 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     223.70 ms /   240 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   104 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   226 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5570.39 ms /   330 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 499 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   499 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   140 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3773.06 ms /   639 tokens\n",
      " 47%|████▋     | 654/1400 [2:00:09<2:37:17, 12.65s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     222.17 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.01 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   320 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7920.92 ms /   378 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2668.84 ms /   550 tokens\n",
      " 47%|████▋     | 655/1400 [2:00:20<2:30:59, 12.16s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.51 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.07 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   251 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6134.35 ms /   309 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1660.26 ms /   508 tokens\n",
      " 47%|████▋     | 656/1400 [2:00:28<2:15:58, 10.97s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.04 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.25 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   235 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5763.65 ms /   294 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   215 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5594.01 ms /   669 tokens\n",
      " 47%|████▋     | 657/1400 [2:00:40<2:18:51, 11.21s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.75 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.80 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   244 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5983.01 ms /   308 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   119 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3186.98 ms /   578 tokens\n",
      " 47%|████▋     | 658/1400 [2:00:50<2:12:31, 10.72s/it]Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.44 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.28 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5385.23 ms /   301 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 477 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   477 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   148 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3909.20 ms /   625 tokens\n",
      " 47%|████▋     | 659/1400 [2:00:59<2:08:39, 10.42s/it]Llama.generate: 42 prefix-match hit, remaining 106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   106 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.54 ms /   110 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 234 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   234 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     224.24 ms /   238 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   288 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7153.48 ms /   391 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 497 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   497 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   142 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3788.04 ms /   639 tokens\n",
      " 47%|████▋     | 660/1400 [2:01:11<2:12:08, 10.71s/it]Llama.generate: 42 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   116 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.58 ms /   120 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 244 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   244 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     226.42 ms /   248 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   111 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   260 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6458.54 ms /   371 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 507 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   507 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   153 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4069.60 ms /   660 tokens\n",
      " 47%|████▋     | 661/1400 [2:01:22<2:12:59, 10.80s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.97 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.70 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   208 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5104.49 ms /   264 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    71 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1995.95 ms /   521 tokens\n",
      " 47%|████▋     | 662/1400 [2:01:29<2:00:40,  9.81s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.45 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.78 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6604.64 ms /   330 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   124 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3335.48 ms /   580 tokens\n",
      " 47%|████▋     | 663/1400 [2:01:40<2:02:23,  9.96s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.16 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.91 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5640.83 ms /   297 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2650.78 ms /   559 tokens\n",
      " 47%|████▋     | 664/1400 [2:01:48<1:57:42,  9.60s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.40 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.52 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   313 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7762.93 ms /   381 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   160 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4235.39 ms /   623 tokens\n",
      " 48%|████▊     | 665/1400 [2:02:01<2:07:48, 10.43s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.35 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.52 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   282 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6972.95 ms /   357 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5931.23 ms /   699 tokens\n",
      " 48%|████▊     | 666/1400 [2:02:14<2:18:08, 11.29s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.79 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.87 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   263 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6502.95 ms /   334 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   140 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3713.20 ms /   606 tokens\n",
      " 48%|████▊     | 667/1400 [2:02:25<2:15:34, 11.10s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.48 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.31 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   298 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7350.08 ms /   370 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   173 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4522.72 ms /   639 tokens\n",
      " 48%|████▊     | 668/1400 [2:02:37<2:19:48, 11.46s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.69 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.49 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   190 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4710.73 ms /   264 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   108 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2926.93 ms /   577 tokens\n",
      " 48%|████▊     | 669/1400 [2:02:45<2:07:14, 10.44s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.48 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.21 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   190 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4674.34 ms /   249 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2516.36 ms /   545 tokens\n",
      " 48%|████▊     | 670/1400 [2:02:53<1:56:33,  9.58s/it]Llama.generate: 42 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   102 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.71 ms /   105 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 230 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   230 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.92 ms /   233 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7544.70 ms /   402 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 493 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   493 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   246 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6380.98 ms /   739 tokens\n",
      " 48%|████▊     | 671/1400 [2:03:07<2:13:46, 11.01s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.38 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.22 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   266 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6573.86 ms /   330 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2761.36 ms /   560 tokens\n",
      " 48%|████▊     | 672/1400 [2:03:17<2:09:04, 10.64s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.60 ms /    86 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     216.94 ms /   214 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   353 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8777.92 ms /   432 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   182 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4776.72 ms /   655 tokens\n",
      " 48%|████▊     | 673/1400 [2:03:31<2:20:58, 11.64s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.43 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.76 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   178 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4334.48 ms /   234 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2130.74 ms /   526 tokens\n",
      " 48%|████▊     | 674/1400 [2:03:38<2:03:34, 10.21s/it]Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.80 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.09 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   309 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7680.73 ms /   397 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 482 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   482 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   157 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4137.07 ms /   639 tokens\n",
      " 48%|████▊     | 675/1400 [2:03:50<2:10:40, 10.81s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.22 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.56 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   202 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4981.20 ms /   259 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2376.91 ms /   537 tokens\n",
      " 48%|████▊     | 676/1400 [2:03:58<1:59:29,  9.90s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.03 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.06 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   267 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6609.52 ms /   337 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   146 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3886.71 ms /   611 tokens\n",
      " 48%|████▊     | 677/1400 [2:04:08<2:02:51, 10.20s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.09 ms /    91 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.96 ms /   219 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   259 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6380.43 ms /   342 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5673.95 ms /   696 tokens\n",
      " 48%|████▊     | 678/1400 [2:04:21<2:10:57, 10.88s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.30 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.05 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   183 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4529.02 ms /   257 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3001.93 ms /   578 tokens\n",
      " 48%|████▊     | 679/1400 [2:04:29<2:00:15, 10.01s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.60 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.32 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   232 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5703.46 ms /   287 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2920.58 ms /   557 tokens\n",
      " 49%|████▊     | 680/1400 [2:04:38<1:56:39,  9.72s/it]Llama.generate: 42 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   115 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     224.01 ms /   119 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 243 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   243 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.67 ms /   246 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   111 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   264 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6559.68 ms /   375 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 506 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   506 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   187 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4900.78 ms /   693 tokens\n",
      " 49%|████▊     | 681/1400 [2:04:50<2:04:23, 10.38s/it]Llama.generate: 42 prefix-match hit, remaining 113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   113 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.45 ms /   116 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 241 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   241 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.00 ms /   244 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   108 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   288 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7171.43 ms /   396 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 504 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   504 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   279 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7216.67 ms /   783 tokens\n",
      " 49%|████▊     | 682/1400 [2:05:05<2:20:11, 11.71s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.30 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.60 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   190 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4637.29 ms /   247 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2390.96 ms /   538 tokens\n",
      " 49%|████▉     | 683/1400 [2:05:12<2:04:40, 10.43s/it]Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.75 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.14 ms /   217 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   240 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5962.77 ms /   321 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 477 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   477 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   178 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4658.73 ms /   655 tokens\n",
      " 49%|████▉     | 684/1400 [2:05:23<2:06:41, 10.62s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.95 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.12 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   240 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5938.89 ms /   307 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2915.28 ms /   570 tokens\n",
      " 49%|████▉     | 685/1400 [2:05:32<2:01:42, 10.21s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.26 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.55 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   242 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5978.18 ms /   308 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2731.35 ms /   559 tokens\n",
      " 49%|████▉     | 686/1400 [2:05:42<1:57:39,  9.89s/it]Llama.generate: 42 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   128 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.87 ms /   131 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 256 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   256 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.98 ms /   259 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   124 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   411 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10260.01 ms /   535 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 519 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   519 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5730.64 ms /   737 tokens\n",
      " 49%|████▉     | 687/1400 [2:05:58<2:20:40, 11.84s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.07 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.80 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   277 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6805.61 ms /   340 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   185 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4811.52 ms /   643 tokens\n",
      " 49%|████▉     | 688/1400 [2:06:10<2:21:02, 11.89s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     165.72 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.30 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   203 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4976.50 ms /   261 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2894.79 ms /   560 tokens\n",
      " 49%|████▉     | 689/1400 [2:06:18<2:07:58, 10.80s/it]Llama.generate: 42 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   125 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     223.61 ms /   128 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 253 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   253 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.18 ms /   256 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   122 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   260 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6465.26 ms /   382 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 516 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   516 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   266 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6900.17 ms /   782 tokens\n",
      " 49%|████▉     | 690/1400 [2:06:32<2:18:33, 11.71s/it]Llama.generate: 42 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   115 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.23 ms /   118 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 243 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   243 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.00 ms /   246 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   112 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   334 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8350.89 ms /   446 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 506 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   506 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   194 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5109.44 ms /   700 tokens\n",
      " 49%|████▉     | 691/1400 [2:06:46<2:26:02, 12.36s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.23 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.35 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   242 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5967.55 ms /   308 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   109 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2939.65 ms /   570 tokens\n",
      " 49%|████▉     | 692/1400 [2:06:55<2:14:56, 11.44s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.54 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.32 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   246 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6036.02 ms /   307 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2122.92 ms /   531 tokens\n",
      " 50%|████▉     | 693/1400 [2:07:04<2:04:31, 10.57s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.84 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.46 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   271 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6793.60 ms /   345 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   249 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6429.59 ms /   718 tokens\n",
      " 50%|████▉     | 694/1400 [2:07:17<2:15:12, 11.49s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.39 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.28 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   240 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5907.88 ms /   302 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   134 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3569.25 ms /   592 tokens\n",
      " 50%|████▉     | 695/1400 [2:07:27<2:09:22, 11.01s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.41 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.98 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   277 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6810.70 ms /   332 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    87 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2370.25 ms /   537 tokens\n",
      " 50%|████▉     | 696/1400 [2:07:37<2:04:02, 10.57s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.88 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.82 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   380 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9479.38 ms /   462 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   196 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5100.62 ms /   674 tokens\n",
      " 50%|████▉     | 697/1400 [2:07:52<2:19:27, 11.90s/it]Llama.generate: 42 prefix-match hit, remaining 133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   133 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     237.23 ms /   137 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 261 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   261 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     220.61 ms /   264 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   130 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   391 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9774.93 ms /   521 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 524 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   524 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   208 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5486.92 ms /   732 tokens\n",
      " 50%|████▉     | 698/1400 [2:08:08<2:32:45, 13.06s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.24 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.03 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   192 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4708.96 ms /   260 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2523.84 ms /   553 tokens\n",
      " 50%|████▉     | 699/1400 [2:08:15<2:13:37, 11.44s/it]Llama.generate: 42 prefix-match hit, remaining 113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   113 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.54 ms /   116 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 241 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   241 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.02 ms /   244 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 109 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   109 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7527.72 ms /   412 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 504 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   504 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   231 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5994.56 ms /   735 tokens\n",
      " 50%|█████     | 700/1400 [2:08:29<2:22:05, 12.18s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.00 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.05 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7458.05 ms /   367 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   126 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3383.11 ms /   585 tokens\n",
      " 50%|█████     | 701/1400 [2:08:40<2:18:30, 11.89s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.37 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.54 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   254 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6250.38 ms /   328 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   143 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3809.08 ms /   613 tokens\n",
      " 50%|█████     | 702/1400 [2:08:51<2:13:20, 11.46s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.58 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.17 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5637.16 ms /   285 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2417.86 ms /   539 tokens\n",
      " 50%|█████     | 703/1400 [2:08:59<2:02:37, 10.56s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.57 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.96 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   336 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8327.99 ms /   402 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   146 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3862.86 ms /   607 tokens\n",
      " 50%|█████     | 704/1400 [2:09:12<2:09:39, 11.18s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.05 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.77 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   261 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6464.98 ms /   333 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   141 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3731.02 ms /   607 tokens\n",
      " 50%|█████     | 705/1400 [2:09:23<2:07:22, 11.00s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.89 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.68 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   206 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5033.79 ms /   269 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3089.58 ms /   570 tokens\n",
      " 50%|█████     | 706/1400 [2:09:31<1:58:41, 10.26s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.44 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.24 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   248 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6062.63 ms /   304 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   136 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3590.10 ms /   588 tokens\n",
      " 50%|█████     | 707/1400 [2:09:41<1:57:42, 10.19s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.02 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.04 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   220 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5430.63 ms /   282 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   111 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3017.24 ms /   568 tokens\n",
      " 51%|█████     | 708/1400 [2:09:50<1:53:02,  9.80s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.20 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.87 ms /   216 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   338 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8388.22 ms /   420 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   213 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5561.59 ms /   689 tokens\n",
      " 51%|█████     | 709/1400 [2:10:04<2:08:38, 11.17s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.03 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.03 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   262 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6479.69 ms /   328 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   167 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4373.81 ms /   627 tokens\n",
      " 51%|█████     | 710/1400 [2:10:16<2:08:38, 11.19s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.76 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.08 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   266 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6532.76 ms /   326 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2480.44 ms /   544 tokens\n",
      " 51%|█████     | 711/1400 [2:10:25<2:02:21, 10.66s/it]Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.34 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   211 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.74 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   232 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5725.34 ms /   312 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 474 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   474 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   126 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3388.76 ms /   600 tokens\n",
      " 51%|█████     | 712/1400 [2:10:35<1:58:22, 10.32s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.95 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.41 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   311 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7721.98 ms /   395 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   125 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3335.54 ms /   603 tokens\n",
      " 51%|█████     | 713/1400 [2:10:46<2:02:05, 10.66s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     237.76 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.64 ms /   216 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   244 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6017.68 ms /   324 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   133 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3569.71 ms /   608 tokens\n",
      " 51%|█████     | 714/1400 [2:10:56<1:59:50, 10.48s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.77 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.04 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   275 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6810.00 ms /   337 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   161 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4220.46 ms /   619 tokens\n",
      " 51%|█████     | 715/1400 [2:11:08<2:03:01, 10.78s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.54 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.78 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   206 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5078.88 ms /   277 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2999.31 ms /   577 tokens\n",
      " 51%|█████     | 716/1400 [2:11:16<1:55:05, 10.10s/it]Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.83 ms /    89 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.31 ms /   217 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   256 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6337.10 ms /   339 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 477 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   477 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   134 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3580.01 ms /   611 tokens\n",
      " 51%|█████     | 717/1400 [2:11:26<1:55:42, 10.16s/it]Llama.generate: 42 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   110 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     219.72 ms /   114 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 238 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   238 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     231.02 ms /   242 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   107 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   311 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7715.67 ms /   418 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 501 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   501 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   169 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4440.43 ms /   670 tokens\n",
      " 51%|█████▏    | 718/1400 [2:11:39<2:03:57, 10.91s/it]Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     151.79 ms /   103 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   228 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.76 ms /   231 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 97 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    97 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   271 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6720.00 ms /   368 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 491 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   491 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   217 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5649.38 ms /   708 tokens\n",
      " 51%|█████▏    | 719/1400 [2:11:52<2:10:03, 11.46s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.37 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.30 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   185 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4571.85 ms /   252 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   152 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4003.14 ms /   615 tokens\n",
      " 51%|█████▏    | 720/1400 [2:12:01<2:01:32, 10.72s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     151.04 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.23 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   309 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7649.86 ms /   382 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   146 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3887.41 ms /   614 tokens\n",
      " 52%|█████▏    | 721/1400 [2:12:13<2:05:21, 11.08s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.51 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.18 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   251 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6211.84 ms /   319 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   111 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2987.42 ms /   574 tokens\n",
      " 52%|█████▏    | 722/1400 [2:12:22<2:00:14, 10.64s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.71 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.18 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   213 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5216.84 ms /   284 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2725.20 ms /   566 tokens\n",
      " 52%|█████▏    | 723/1400 [2:12:31<1:52:26,  9.96s/it]Llama.generate: 42 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   123 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.30 ms /   126 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 251 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   251 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.98 ms /   254 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   119 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   279 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6931.14 ms /   398 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 514 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   514 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   319 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8241.66 ms /   833 tokens\n",
      " 52%|█████▏    | 724/1400 [2:12:46<2:11:19, 11.66s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.69 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.59 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   280 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6921.86 ms /   343 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2513.33 ms /   549 tokens\n",
      " 52%|█████▏    | 725/1400 [2:12:56<2:05:03, 11.12s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.30 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.66 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5805.33 ms /   310 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2763.71 ms /   569 tokens\n",
      " 52%|█████▏    | 726/1400 [2:13:05<1:57:44, 10.48s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.31 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.35 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6627.13 ms /   342 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5642.60 ms /   685 tokens\n",
      " 52%|█████▏    | 727/1400 [2:13:18<2:04:49, 11.13s/it]Llama.generate: 42 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   102 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     228.85 ms /   106 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 230 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   230 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     230.93 ms /   234 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   265 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6556.22 ms /   363 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 493 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   493 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   133 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3556.51 ms /   626 tokens\n",
      " 52%|█████▏    | 728/1400 [2:13:28<2:02:51, 10.97s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.83 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.11 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   238 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5876.30 ms /   293 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2137.98 ms /   526 tokens\n",
      " 52%|█████▏    | 729/1400 [2:13:37<1:54:03, 10.20s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.39 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.74 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   268 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6626.54 ms /   327 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2505.58 ms /   543 tokens\n",
      " 52%|█████▏    | 730/1400 [2:13:46<1:51:39, 10.00s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.62 ms /    86 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.11 ms /   214 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   209 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5191.56 ms /   287 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   127 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3408.61 ms /   600 tokens\n",
      " 52%|█████▏    | 731/1400 [2:13:55<1:48:19,  9.72s/it]Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     216.56 ms /    96 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.45 ms /   224 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   263 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6498.23 ms /   352 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 483 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   483 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   193 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5048.87 ms /   676 tokens\n",
      " 52%|█████▏    | 732/1400 [2:14:07<1:55:49, 10.40s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     165.74 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.81 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   238 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5853.18 ms /   303 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   166 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4380.85 ms /   626 tokens\n",
      " 52%|█████▏    | 733/1400 [2:14:18<1:56:20, 10.47s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     169.74 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.72 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   208 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5157.60 ms /   280 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2531.85 ms /   559 tokens\n",
      " 52%|█████▏    | 734/1400 [2:14:26<1:48:16,  9.75s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     131.50 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.10 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   328 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8117.07 ms /   394 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3255.94 ms /   582 tokens\n",
      " 52%|█████▎    | 735/1400 [2:14:38<1:54:38, 10.34s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.40 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.56 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   331 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8227.80 ms /   412 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   144 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3852.56 ms /   619 tokens\n",
      " 53%|█████▎    | 736/1400 [2:14:50<2:01:35, 10.99s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.13 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.95 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   192 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4763.68 ms /   257 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2489.86 ms /   548 tokens\n",
      " 53%|█████▎    | 737/1400 [2:14:58<1:50:26,  9.99s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.79 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.22 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   351 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8665.73 ms /   406 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2746.92 ms /   552 tokens\n",
      " 53%|█████▎    | 738/1400 [2:15:10<1:56:17, 10.54s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.34 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.24 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   304 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7483.01 ms /   372 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6917.46 ms /   732 tokens\n",
      " 53%|█████▎    | 739/1400 [2:15:25<2:10:12, 11.82s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.39 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.82 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   222 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5475.88 ms /   286 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   127 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3420.99 ms /   587 tokens\n",
      " 53%|█████▎    | 740/1400 [2:15:34<2:01:37, 11.06s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.61 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.05 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   295 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7294.85 ms /   351 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2990.37 ms /   562 tokens\n",
      " 53%|█████▎    | 741/1400 [2:15:45<2:00:08, 10.94s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.87 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.01 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   268 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6629.06 ms /   337 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   226 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5859.95 ms /   690 tokens\n",
      " 53%|█████▎    | 742/1400 [2:15:57<2:06:24, 11.53s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.78 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.30 ms /   216 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   224 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5532.21 ms /   304 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3303.71 ms /   597 tokens\n",
      " 53%|█████▎    | 743/1400 [2:16:07<1:58:49, 10.85s/it]Llama.generate: 42 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   111 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.28 ms /   115 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 239 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   239 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     227.43 ms /   243 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   108 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   285 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7118.24 ms /   393 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 502 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   502 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   189 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4938.83 ms /   691 tokens\n",
      " 53%|█████▎    | 744/1400 [2:16:19<2:04:04, 11.35s/it]Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.37 ms /    98 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.32 ms /   226 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   332 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8223.84 ms /   423 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   486 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   246 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6345.28 ms /   732 tokens\n",
      " 53%|█████▎    | 745/1400 [2:16:34<2:15:44, 12.43s/it]Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.68 ms /   104 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   228 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     226.76 ms /   232 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   262 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6479.20 ms /   357 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 491 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   491 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   214 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5562.31 ms /   705 tokens\n",
      " 53%|█████▎    | 746/1400 [2:16:47<2:15:45, 12.45s/it]Llama.generate: 42 prefix-match hit, remaining 146 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   146 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.21 ms /   149 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 274 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   274 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     222.74 ms /   277 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 142 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   142 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   330 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8278.79 ms /   472 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 537 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   537 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   190 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5024.41 ms /   727 tokens\n",
      " 53%|█████▎    | 747/1400 [2:17:00<2:19:45, 12.84s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.39 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.03 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5842.39 ms /   302 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2701.41 ms /   559 tokens\n",
      " 53%|█████▎    | 748/1400 [2:17:09<2:06:55, 11.68s/it]Llama.generate: 42 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   112 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.34 ms /   116 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 240 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   240 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     226.21 ms /   244 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   107 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   257 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6366.24 ms /   364 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 503 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   503 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   148 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3946.19 ms /   651 tokens\n",
      " 54%|█████▎    | 749/1400 [2:17:20<2:03:47, 11.41s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.65 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.73 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   262 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6451.94 ms /   327 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2858.77 ms /   564 tokens\n",
      " 54%|█████▎    | 750/1400 [2:17:30<1:58:08, 10.90s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.46 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.96 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   213 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5224.58 ms /   267 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    79 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2203.14 ms /   528 tokens\n",
      " 54%|█████▎    | 751/1400 [2:17:38<1:47:56,  9.98s/it]Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.18 ms /   103 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   227 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     224.07 ms /   231 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   242 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5973.54 ms /   338 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 490 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   490 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   139 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3715.14 ms /   629 tokens\n",
      " 54%|█████▎    | 752/1400 [2:17:48<1:48:18, 10.03s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.68 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.21 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   264 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6554.08 ms /   344 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   239 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6190.79 ms /   714 tokens\n",
      " 54%|█████▍    | 753/1400 [2:18:01<1:58:15, 10.97s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.34 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.36 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   199 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4863.23 ms /   259 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    87 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2408.99 ms /   542 tokens\n",
      " 54%|█████▍    | 754/1400 [2:18:09<1:47:28,  9.98s/it]Llama.generate: 42 prefix-match hit, remaining 122 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   122 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     219.71 ms /   126 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 250 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   250 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.37 ms /   253 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   119 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   298 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7426.74 ms /   417 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 513 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   513 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   230 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6014.68 ms /   743 tokens\n",
      " 54%|█████▍    | 755/1400 [2:18:23<1:59:56, 11.16s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.32 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.99 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   189 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4640.94 ms /   248 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2228.13 ms /   534 tokens\n",
      " 54%|█████▍    | 756/1400 [2:18:30<1:47:16,  9.99s/it]Llama.generate: 42 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   107 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     146.88 ms /   110 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 235 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   235 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.76 ms /   238 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   286 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7065.62 ms /   389 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 498 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   498 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   208 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5413.45 ms /   706 tokens\n",
      " 54%|█████▍    | 757/1400 [2:18:43<1:56:17, 10.85s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.79 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.48 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   197 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4825.72 ms /   257 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2920.78 ms /   562 tokens\n",
      " 54%|█████▍    | 758/1400 [2:18:51<1:47:33, 10.05s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.19 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.82 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   209 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5142.34 ms /   282 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   124 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3348.95 ms /   592 tokens\n",
      " 54%|█████▍    | 759/1400 [2:19:00<1:43:45,  9.71s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.49 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.97 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   230 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5689.66 ms /   290 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   121 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3238.36 ms /   575 tokens\n",
      " 54%|█████▍    | 760/1400 [2:19:09<1:42:18,  9.59s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.04 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.43 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5788.07 ms /   294 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   115 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3129.52 ms /   569 tokens\n",
      " 54%|█████▍    | 761/1400 [2:19:18<1:41:10,  9.50s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.78 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.28 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   240 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5909.82 ms /   305 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   152 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4005.14 ms /   612 tokens\n",
      " 54%|█████▍    | 762/1400 [2:19:29<1:43:42,  9.75s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.05 ms /    89 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.51 ms /   217 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   235 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5824.50 ms /   317 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   138 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3698.51 ms /   614 tokens\n",
      " 55%|█████▍    | 763/1400 [2:19:39<1:44:13,  9.82s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     158.94 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.68 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   386 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9617.90 ms /   453 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   145 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3826.10 ms /   607 tokens\n",
      " 55%|█████▍    | 764/1400 [2:19:53<1:56:46, 11.02s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.30 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.46 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   178 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4372.55 ms /   243 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   106 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2905.53 ms /   566 tokens\n",
      " 55%|█████▍    | 765/1400 [2:20:00<1:46:04, 10.02s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.41 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.85 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   312 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7702.10 ms /   372 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   115 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3118.17 ms /   569 tokens\n",
      " 55%|█████▍    | 766/1400 [2:20:11<1:49:35, 10.37s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     134.74 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.09 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   244 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6018.78 ms /   312 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   171 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4472.55 ms /   633 tokens\n",
      " 55%|█████▍    | 767/1400 [2:20:22<1:50:53, 10.51s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.98 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.50 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   238 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5893.45 ms /   304 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    93 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2570.94 ms /   554 tokens\n",
      " 55%|█████▍    | 768/1400 [2:20:31<1:45:38, 10.03s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.93 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.08 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   268 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6578.38 ms /   337 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   117 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3169.35 ms /   580 tokens\n",
      " 55%|█████▍    | 769/1400 [2:20:41<1:45:51, 10.07s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     227.80 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.65 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   341 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8453.32 ms /   422 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   177 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4653.41 ms /   652 tokens\n",
      " 55%|█████▌    | 770/1400 [2:20:55<1:56:42, 11.12s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.32 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.68 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   244 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5998.16 ms /   305 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   128 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3454.51 ms /   585 tokens\n",
      " 55%|█████▌    | 771/1400 [2:21:05<1:52:27, 10.73s/it]Llama.generate: 42 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   119 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.99 ms /   122 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 247 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   247 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.17 ms /   250 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   114 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   319 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7959.20 ms /   433 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 510 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   510 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   272 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7055.61 ms /   782 tokens\n",
      " 55%|█████▌    | 772/1400 [2:21:20<2:07:00, 12.13s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.13 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.84 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   214 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5261.16 ms /   289 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   183 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4782.52 ms /   652 tokens\n",
      " 55%|█████▌    | 773/1400 [2:21:31<2:01:32, 11.63s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.67 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.87 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   214 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5286.17 ms /   283 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2793.87 ms /   565 tokens\n",
      " 55%|█████▌    | 774/1400 [2:21:39<1:51:31, 10.69s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.53 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.16 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   271 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6743.33 ms /   352 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   144 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3838.74 ms /   619 tokens\n",
      " 55%|█████▌    | 775/1400 [2:21:50<1:52:08, 10.77s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.55 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.25 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   247 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6075.81 ms /   313 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2616.51 ms /   554 tokens\n",
      " 55%|█████▌    | 776/1400 [2:21:59<1:46:48, 10.27s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.44 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.91 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   188 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4624.54 ms /   245 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2764.43 ms /   554 tokens\n",
      " 56%|█████▌    | 777/1400 [2:22:07<1:39:01,  9.54s/it]Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.90 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.85 ms /   217 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   309 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7641.09 ms /   391 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 477 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   477 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   177 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4619.70 ms /   654 tokens\n",
      " 56%|█████▌    | 778/1400 [2:22:20<1:48:38, 10.48s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.75 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.64 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   261 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6429.60 ms /   321 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   117 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3149.40 ms /   572 tokens\n",
      " 56%|█████▌    | 779/1400 [2:22:30<1:46:52, 10.33s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.43 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.98 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   344 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8526.12 ms /   406 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2682.01 ms /   554 tokens\n",
      " 56%|█████▌    | 780/1400 [2:22:41<1:50:36, 10.70s/it]Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     224.06 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 315 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   315 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     241.47 ms /   318 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 184 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   184 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   318 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8001.38 ms /   502 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 578 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   578 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   295 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7726.33 ms /   873 tokens\n",
      " 56%|█████▌    | 781/1400 [2:22:57<2:07:29, 12.36s/it]Llama.generate: 42 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   105 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.61 ms /   109 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 233 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   233 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     223.87 ms /   237 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   102 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   233 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5777.01 ms /   335 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 496 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   496 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   146 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3881.73 ms /   642 tokens\n",
      " 56%|█████▌    | 782/1400 [2:23:08<2:00:16, 11.68s/it]Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.48 ms /    93 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.15 ms /   221 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   309 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7664.02 ms /   394 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 480 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   480 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   167 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4402.11 ms /   647 tokens\n",
      " 56%|█████▌    | 783/1400 [2:23:20<2:02:36, 11.92s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.94 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.79 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   226 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5569.80 ms /   291 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2824.35 ms /   563 tokens\n",
      " 56%|█████▌    | 784/1400 [2:23:29<1:52:51, 10.99s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     141.26 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.68 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   239 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5903.24 ms /   309 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   119 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3190.86 ms /   585 tokens\n",
      " 56%|█████▌    | 785/1400 [2:23:38<1:47:53, 10.53s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.70 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.77 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   196 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4836.70 ms /   261 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3278.15 ms /   583 tokens\n",
      " 56%|█████▌    | 786/1400 [2:23:47<1:41:37,  9.93s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.16 ms /    89 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.09 ms /   217 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5633.04 ms /   311 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   135 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3593.99 ms /   611 tokens\n",
      " 56%|█████▌    | 787/1400 [2:23:56<1:40:37,  9.85s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     170.29 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.75 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   260 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6398.54 ms /   324 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   168 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4408.49 ms /   627 tokens\n",
      " 56%|█████▋    | 788/1400 [2:24:08<1:44:34, 10.25s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.96 ms /    91 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.13 ms /   219 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   233 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5734.79 ms /   316 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   127 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3399.63 ms /   605 tokens\n",
      " 56%|█████▋    | 789/1400 [2:24:17<1:42:21, 10.05s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.24 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.51 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   282 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6948.46 ms /   336 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2879.31 ms /   557 tokens\n",
      " 56%|█████▋    | 790/1400 [2:24:27<1:42:39, 10.10s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.37 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.68 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   224 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5485.31 ms /   285 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2445.68 ms /   544 tokens\n",
      " 56%|█████▋    | 791/1400 [2:24:36<1:37:14,  9.58s/it]Llama.generate: 42 prefix-match hit, remaining 109 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   109 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.78 ms /   113 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 237 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   237 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     222.96 ms /   241 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   106 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   324 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8070.14 ms /   430 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 500 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   500 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   176 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4655.02 ms /   676 tokens\n",
      " 57%|█████▋    | 792/1400 [2:24:49<1:48:00, 10.66s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.06 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.13 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   211 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5163.44 ms /   271 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2527.05 ms /   547 tokens\n",
      " 57%|█████▋    | 793/1400 [2:24:57<1:40:07,  9.90s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     138.16 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.41 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   211 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5177.25 ms /   270 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   198 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5182.52 ms /   652 tokens\n",
      " 57%|█████▋    | 794/1400 [2:25:08<1:42:23, 10.14s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.59 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.43 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   228 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5623.74 ms /   304 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   203 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5326.34 ms /   673 tokens\n",
      " 57%|█████▋    | 795/1400 [2:25:19<1:46:01, 10.51s/it]Llama.generate: 42 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     236.27 ms /   102 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 226 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   226 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     226.45 ms /   230 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   245 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6073.08 ms /   339 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 489 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   489 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   145 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3882.06 ms /   634 tokens\n",
      " 57%|█████▋    | 796/1400 [2:25:30<1:45:37, 10.49s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.89 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.65 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   210 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5151.42 ms /   274 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3256.34 ms /   580 tokens\n",
      " 57%|█████▋    | 797/1400 [2:25:38<1:40:22,  9.99s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.75 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.88 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   237 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5802.82 ms /   292 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2422.41 ms /   537 tokens\n",
      " 57%|█████▋    | 798/1400 [2:25:47<1:36:05,  9.58s/it]Llama.generate: 42 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.90 ms /    98 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 222 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   222 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.36 ms /   225 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   316 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7876.78 ms /   407 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 485 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   485 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   166 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4387.89 ms /   651 tokens\n",
      " 57%|█████▋    | 799/1400 [2:26:00<1:45:15, 10.51s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     133.50 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.50 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   264 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6506.06 ms /   323 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   152 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4013.50 ms /   606 tokens\n",
      " 57%|█████▋    | 800/1400 [2:26:11<1:46:08, 10.61s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.46 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.08 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   201 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4956.26 ms /   259 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   170 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4444.15 ms /   622 tokens\n",
      " 57%|█████▋    | 801/1400 [2:26:20<1:43:27, 10.36s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.42 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.04 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   227 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5558.26 ms /   291 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2554.18 ms /   550 tokens\n",
      " 57%|█████▋    | 802/1400 [2:26:29<1:37:46,  9.81s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     221.14 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.26 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   456 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   11337.64 ms /   522 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   145 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3832.18 ms /   605 tokens\n",
      " 57%|█████▋    | 803/1400 [2:26:45<1:54:58, 11.55s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     218.73 ms /    89 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.47 ms /   217 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   304 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7523.46 ms /   385 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   175 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4590.47 ms /   651 tokens\n",
      " 57%|█████▋    | 804/1400 [2:26:57<1:57:48, 11.86s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.05 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.43 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5649.14 ms /   296 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   189 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4978.02 ms /   651 tokens\n",
      " 57%|█████▊    | 805/1400 [2:27:08<1:55:06, 11.61s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     148.35 ms /    61 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.81 ms /   189 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   310 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7629.49 ms /   365 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    83 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2324.44 ms /   532 tokens\n",
      " 58%|█████▊    | 806/1400 [2:27:18<1:51:02, 11.22s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     137.57 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.63 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   235 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5790.73 ms /   293 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   109 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2949.93 ms /   562 tokens\n",
      " 58%|█████▊    | 807/1400 [2:27:28<1:44:29, 10.57s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.15 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.34 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   313 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7744.49 ms /   393 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   134 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3585.30 ms /   609 tokens\n",
      " 58%|█████▊    | 808/1400 [2:27:39<1:47:43, 10.92s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.29 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.91 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   193 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4774.74 ms /   255 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2391.52 ms /   544 tokens\n",
      " 58%|█████▊    | 809/1400 [2:27:47<1:37:46,  9.93s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.00 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.96 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   239 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5868.49 ms /   300 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   134 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3587.93 ms /   590 tokens\n",
      " 58%|█████▊    | 810/1400 [2:27:57<1:37:20,  9.90s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.52 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.92 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   250 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6168.41 ms /   317 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2602.98 ms /   552 tokens\n",
      " 58%|█████▊    | 811/1400 [2:28:06<1:35:08,  9.69s/it]Llama.generate: 42 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   120 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     222.68 ms /   124 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 248 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   248 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     228.29 ms /   252 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   116 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   258 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6453.73 ms /   374 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 511 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   511 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   151 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3994.33 ms /   662 tokens\n",
      " 58%|█████▊    | 812/1400 [2:28:17<1:38:36, 10.06s/it]Llama.generate: 42 prefix-match hit, remaining 146 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   146 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.60 ms /   149 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 274 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   274 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     221.70 ms /   277 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 142 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   142 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   284 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7151.47 ms /   426 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 537 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   537 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   332 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8601.62 ms /   869 tokens\n",
      " 58%|█████▊    | 813/1400 [2:28:33<1:56:28, 11.91s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     156.36 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.50 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6621.68 ms /   331 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1962.66 ms /   525 tokens\n",
      " 58%|█████▊    | 814/1400 [2:28:42<1:47:38, 11.02s/it]Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.85 ms /    98 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.56 ms /   226 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   254 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6265.15 ms /   346 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   486 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   176 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4616.13 ms /   662 tokens\n",
      " 58%|█████▊    | 815/1400 [2:28:53<1:48:14, 11.10s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.33 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.52 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   204 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5005.56 ms /   266 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2378.85 ms /   541 tokens\n",
      " 58%|█████▊    | 816/1400 [2:29:01<1:38:27, 10.12s/it]Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     230.04 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   211 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.25 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   220 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5435.54 ms /   299 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 474 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   474 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   119 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3205.00 ms /   593 tokens\n",
      " 58%|█████▊    | 817/1400 [2:29:10<1:35:20,  9.81s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.82 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.83 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   259 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6385.41 ms /   328 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   202 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5268.01 ms /   665 tokens\n",
      " 58%|█████▊    | 818/1400 [2:29:22<1:41:41, 10.48s/it]Llama.generate: 42 prefix-match hit, remaining 97 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    97 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.75 ms /   101 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 225 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   225 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     219.98 ms /   229 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    93 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   233 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5758.04 ms /   326 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 488 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   488 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   180 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4707.36 ms /   668 tokens\n",
      " 58%|█████▊    | 819/1400 [2:29:33<1:42:45, 10.61s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.28 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.66 ms /   189 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   194 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4748.14 ms /   248 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2410.84 ms /   537 tokens\n",
      " 59%|█████▊    | 820/1400 [2:29:41<1:33:40,  9.69s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.12 ms /    86 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.34 ms /   214 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   285 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7080.43 ms /   363 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   183 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4783.03 ms /   656 tokens\n",
      " 59%|█████▊    | 821/1400 [2:29:53<1:41:00, 10.47s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.43 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.40 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   301 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7432.56 ms /   377 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   186 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4862.36 ms /   657 tokens\n",
      " 59%|█████▊    | 822/1400 [2:30:06<1:47:13, 11.13s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.37 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.10 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   252 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6229.62 ms /   329 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   151 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4012.42 ms /   622 tokens\n",
      " 59%|█████▉    | 823/1400 [2:30:16<1:45:42, 10.99s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.80 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.32 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   259 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6376.30 ms /   328 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3079.12 ms /   577 tokens\n",
      " 59%|█████▉    | 824/1400 [2:30:26<1:42:14, 10.65s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.18 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.04 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   289 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7174.83 ms /   363 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   126 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3370.19 ms /   594 tokens\n",
      " 59%|█████▉    | 825/1400 [2:30:37<1:43:01, 10.75s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.79 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.13 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   207 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5105.04 ms /   269 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2750.51 ms /   558 tokens\n",
      " 59%|█████▉    | 826/1400 [2:30:45<1:35:40, 10.00s/it]Llama.generate: 42 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   115 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.79 ms /   118 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 243 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   243 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.24 ms /   246 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   113 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   295 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7348.10 ms /   408 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 506 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   506 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   246 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6358.31 ms /   752 tokens\n",
      " 59%|█████▉    | 827/1400 [2:31:00<1:47:20, 11.24s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     147.91 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.36 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   307 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7599.36 ms /   384 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   194 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5088.96 ms /   665 tokens\n",
      " 59%|█████▉    | 828/1400 [2:31:13<1:52:23, 11.79s/it]Llama.generate: 42 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   129 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.16 ms /   132 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 257 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   257 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     220.24 ms /   260 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   126 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   247 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6124.65 ms /   373 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 520 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   520 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   202 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5330.84 ms /   722 tokens\n",
      " 59%|█████▉    | 829/1400 [2:31:24<1:52:28, 11.82s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.03 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.86 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   325 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8073.12 ms /   400 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   117 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3141.28 ms /   587 tokens\n",
      " 59%|█████▉    | 830/1400 [2:31:36<1:51:41, 11.76s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.45 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.98 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   222 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5450.97 ms /   282 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2941.24 ms /   561 tokens\n",
      " 59%|█████▉    | 831/1400 [2:31:45<1:43:01, 10.86s/it]Llama.generate: 42 prefix-match hit, remaining 139 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   139 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.81 ms /   142 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 267 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   267 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     222.98 ms /   270 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 137 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   137 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   334 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8418.90 ms /   471 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 530 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   530 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   259 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6750.54 ms /   789 tokens\n",
      " 59%|█████▉    | 832/1400 [2:32:01<1:56:22, 12.29s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.44 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.59 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   298 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7405.40 ms /   371 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   160 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4253.58 ms /   628 tokens\n",
      " 60%|█████▉    | 833/1400 [2:32:13<1:55:32, 12.23s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.00 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.89 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   288 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7116.47 ms /   356 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   156 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4113.74 ms /   619 tokens\n",
      " 60%|█████▉    | 834/1400 [2:32:24<1:53:35, 12.04s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.68 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     217.18 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   223 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5490.30 ms /   288 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   106 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2891.84 ms /   565 tokens\n",
      " 60%|█████▉    | 835/1400 [2:32:33<1:44:13, 11.07s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.62 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.66 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   240 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5914.43 ms /   296 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   148 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3904.06 ms /   598 tokens\n",
      " 60%|█████▉    | 836/1400 [2:32:43<1:41:34, 10.81s/it]Llama.generate: 42 prefix-match hit, remaining 154 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   154 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.17 ms /   157 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 282 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   282 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     226.60 ms /   285 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 150 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   150 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   304 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7631.38 ms /   454 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 545 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   545 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   292 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7614.75 ms /   837 tokens\n",
      " 60%|█████▉    | 837/1400 [2:32:59<1:55:12, 12.28s/it]Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.63 ms /    92 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.50 ms /   220 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   280 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6937.28 ms /   366 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 480 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   480 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   142 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3791.02 ms /   622 tokens\n",
      " 60%|█████▉    | 838/1400 [2:33:10<1:51:40, 11.92s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.15 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.53 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   264 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6530.65 ms /   326 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   114 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3079.36 ms /   570 tokens\n",
      " 60%|█████▉    | 839/1400 [2:33:20<1:46:01, 11.34s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.45 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.51 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   233 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5722.55 ms /   300 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    70 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1998.13 ms /   532 tokens\n",
      " 60%|██████    | 840/1400 [2:33:28<1:36:46, 10.37s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.80 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.64 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   295 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7291.10 ms /   367 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   119 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3227.64 ms /   586 tokens\n",
      " 60%|██████    | 841/1400 [2:33:39<1:38:07, 10.53s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.17 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.96 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   262 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6453.52 ms /   321 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   123 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3308.68 ms /   576 tokens\n",
      " 60%|██████    | 842/1400 [2:33:49<1:36:54, 10.42s/it]Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.47 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 317 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   317 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     242.97 ms /   320 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   328 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8252.27 ms /   513 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 580 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   580 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   324 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8455.95 ms /   904 tokens\n",
      " 60%|██████    | 843/1400 [2:34:06<1:55:32, 12.45s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.88 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.20 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   200 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4909.88 ms /   267 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2671.58 ms /   559 tokens\n",
      " 60%|██████    | 844/1400 [2:34:14<1:42:55, 11.11s/it]Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     218.61 ms /    96 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.39 ms /   223 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   306 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7574.88 ms /   394 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 483 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   483 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   131 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3511.35 ms /   614 tokens\n",
      " 60%|██████    | 845/1400 [2:34:26<1:43:53, 11.23s/it]Llama.generate: 42 prefix-match hit, remaining 133 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   133 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.23 ms /   136 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 261 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   261 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     220.44 ms /   264 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   130 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   282 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7036.41 ms /   412 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 524 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   524 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   253 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6632.23 ms /   777 tokens\n",
      " 60%|██████    | 846/1400 [2:34:40<1:51:41, 12.10s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     132.97 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.10 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   186 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4576.96 ms /   243 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    83 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2292.82 ms /   534 tokens\n",
      " 60%|██████    | 847/1400 [2:34:47<1:37:57, 10.63s/it]Llama.generate: 42 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   116 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.45 ms /   119 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 244 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   244 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.35 ms /   247 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   113 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   342 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8543.23 ms /   455 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 507 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   507 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   305 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7891.18 ms /   812 tokens\n",
      " 61%|██████    | 848/1400 [2:35:04<1:54:57, 12.50s/it]Llama.generate: 42 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   138 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.75 ms /   141 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 266 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   266 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     221.70 ms /   269 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   135 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   286 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7146.79 ms /   421 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 529 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   529 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   206 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5430.15 ms /   735 tokens\n",
      " 61%|██████    | 849/1400 [2:35:17<1:56:12, 12.65s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.06 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.81 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   241 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5934.41 ms /   302 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2381.40 ms /   540 tokens\n",
      " 61%|██████    | 850/1400 [2:35:26<1:45:10, 11.47s/it]Llama.generate: 42 prefix-match hit, remaining 130 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   130 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.73 ms /   133 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 258 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   258 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     221.08 ms /   261 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   127 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   286 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7117.39 ms /   413 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 521 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   521 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   341 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8823.64 ms /   862 tokens\n",
      " 61%|██████    | 851/1400 [2:35:42<1:58:26, 12.94s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.96 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.29 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   265 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6536.24 ms /   330 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   178 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4672.29 ms /   638 tokens\n",
      " 61%|██████    | 852/1400 [2:35:54<1:54:31, 12.54s/it]Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     227.39 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   211 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.08 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   247 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6074.61 ms /   326 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 474 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   474 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   121 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3244.57 ms /   595 tokens\n",
      " 61%|██████    | 853/1400 [2:36:03<1:46:46, 11.71s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.94 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.80 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   245 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6059.99 ms /   305 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3288.82 ms /   577 tokens\n",
      " 61%|██████    | 854/1400 [2:36:13<1:41:08, 11.11s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.11 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.58 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   198 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4877.98 ms /   253 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2137.50 ms /   526 tokens\n",
      " 61%|██████    | 855/1400 [2:36:21<1:30:44,  9.99s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     148.67 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.97 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   245 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6023.32 ms /   299 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   190 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4924.35 ms /   640 tokens\n",
      " 61%|██████    | 856/1400 [2:36:32<1:34:09, 10.38s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.10 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.30 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   293 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7227.26 ms /   359 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   125 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3370.21 ms /   585 tokens\n",
      " 61%|██████    | 857/1400 [2:36:43<1:35:38, 10.57s/it]Llama.generate: 42 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   116 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.94 ms /   119 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 244 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   244 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.70 ms /   247 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   112 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   390 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9721.13 ms /   502 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 507 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   507 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   213 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5563.26 ms /   720 tokens\n",
      " 61%|██████▏   | 858/1400 [2:36:59<1:49:25, 12.11s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     169.48 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.66 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   252 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6187.70 ms /   316 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   121 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3275.07 ms /   580 tokens\n",
      " 61%|██████▏   | 859/1400 [2:37:08<1:43:08, 11.44s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.15 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.41 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   328 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8139.19 ms /   394 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   196 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5109.73 ms /   656 tokens\n",
      " 61%|██████▏   | 860/1400 [2:37:22<1:48:51, 12.09s/it]Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.96 ms /    98 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.94 ms /   226 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   276 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6836.90 ms /   368 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   486 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   251 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6486.14 ms /   737 tokens\n",
      " 62%|██████▏   | 861/1400 [2:37:36<1:53:01, 12.58s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.10 ms /    85 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.36 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   330 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8187.22 ms /   409 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   192 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5004.49 ms /   664 tokens\n",
      " 62%|██████▏   | 862/1400 [2:37:49<1:55:28, 12.88s/it]Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.23 ms /    60 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.69 ms /   188 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 53 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    53 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   212 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5202.33 ms /   265 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 448 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   448 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    53 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1541.81 ms /   501 tokens\n",
      " 62%|██████▏   | 863/1400 [2:37:56<1:39:48, 11.15s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.74 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.63 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   205 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5078.01 ms /   279 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2730.62 ms /   567 tokens\n",
      " 62%|██████▏   | 864/1400 [2:38:05<1:31:49, 10.28s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.18 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.50 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   195 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4800.09 ms /   251 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1771.95 ms /   513 tokens\n",
      " 62%|██████▏   | 865/1400 [2:38:12<1:22:45,  9.28s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     138.11 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.12 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   216 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5298.59 ms /   273 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2350.60 ms /   537 tokens\n",
      " 62%|██████▏   | 866/1400 [2:38:20<1:19:09,  8.89s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.79 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.80 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   222 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5486.96 ms /   297 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2918.56 ms /   577 tokens\n",
      " 62%|██████▏   | 867/1400 [2:38:29<1:18:55,  8.88s/it]Llama.generate: 42 prefix-match hit, remaining 131 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   131 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.77 ms /   134 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 259 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   259 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     220.15 ms /   262 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   127 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   338 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8428.41 ms /   465 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 522 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   522 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   176 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4692.64 ms /   698 tokens\n",
      " 62%|██████▏   | 868/1400 [2:38:42<1:31:13, 10.29s/it]Llama.generate: 42 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.07 ms /   104 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 229 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   229 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.69 ms /   232 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 97 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    97 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   259 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6423.21 ms /   356 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 492 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   492 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   261 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6730.00 ms /   753 tokens\n",
      " 62%|██████▏   | 869/1400 [2:38:56<1:39:42, 11.27s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     165.83 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.32 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   357 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8839.70 ms /   421 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   144 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3811.46 ms /   603 tokens\n",
      " 62%|██████▏   | 870/1400 [2:39:09<1:44:11, 11.80s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.12 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.58 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   264 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6534.93 ms /   339 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   120 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3228.09 ms /   590 tokens\n",
      " 62%|██████▏   | 871/1400 [2:39:19<1:39:48, 11.32s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.48 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.01 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   220 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5450.91 ms /   289 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2744.52 ms /   564 tokens\n",
      " 62%|██████▏   | 872/1400 [2:39:28<1:32:33, 10.52s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.22 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.70 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   370 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9174.15 ms /   438 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   115 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3087.87 ms /   578 tokens\n",
      " 62%|██████▏   | 873/1400 [2:39:40<1:38:00, 11.16s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.55 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.28 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   227 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5592.69 ms /   299 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   159 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4223.43 ms /   626 tokens\n",
      " 62%|██████▏   | 874/1400 [2:39:50<1:35:27, 10.89s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.52 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.93 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   318 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7863.65 ms /   394 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   160 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4207.45 ms /   631 tokens\n",
      " 62%|██████▎   | 875/1400 [2:40:03<1:39:25, 11.36s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.12 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.45 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   253 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6237.09 ms /   317 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2786.24 ms /   560 tokens\n",
      " 63%|██████▎   | 876/1400 [2:40:12<1:34:14, 10.79s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     158.82 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.50 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   284 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6977.90 ms /   340 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2975.81 ms /   561 tokens\n",
      " 63%|██████▎   | 877/1400 [2:40:23<1:32:50, 10.65s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.69 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.89 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   210 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5153.46 ms /   265 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2457.93 ms /   540 tokens\n",
      " 63%|██████▎   | 878/1400 [2:40:31<1:25:47,  9.86s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.13 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.32 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   404 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10066.16 ms /   479 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   165 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4325.46 ms /   635 tokens\n",
      " 63%|██████▎   | 879/1400 [2:40:46<1:38:27, 11.34s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.13 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.30 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   353 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8758.08 ms /   423 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   227 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5885.93 ms /   692 tokens\n",
      " 63%|██████▎   | 880/1400 [2:41:01<1:47:52, 12.45s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.68 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.92 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   284 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7032.04 ms /   356 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   187 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4884.55 ms /   654 tokens\n",
      " 63%|██████▎   | 881/1400 [2:41:13<1:47:19, 12.41s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.51 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.15 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   264 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6494.49 ms /   339 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   160 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4198.42 ms /   629 tokens\n",
      " 63%|██████▎   | 882/1400 [2:41:24<1:43:40, 12.01s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.31 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.67 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   226 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5545.21 ms /   291 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   238 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6150.96 ms /   697 tokens\n",
      " 63%|██████▎   | 883/1400 [2:41:36<1:43:39, 12.03s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.03 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.00 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   267 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6611.24 ms /   326 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   134 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3574.64 ms /   588 tokens\n",
      " 63%|██████▎   | 884/1400 [2:41:47<1:39:45, 11.60s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.67 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.44 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   210 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5145.85 ms /   268 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    72 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2051.71 ms /   525 tokens\n",
      " 63%|██████▎   | 885/1400 [2:41:54<1:29:15, 10.40s/it]Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.73 ms /    89 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.92 ms /   217 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   222 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5482.93 ms /   304 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 477 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   477 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   191 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4970.63 ms /   668 tokens\n",
      " 63%|██████▎   | 886/1400 [2:42:05<1:30:11, 10.53s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.45 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.17 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   185 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4577.88 ms /   240 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1821.52 ms /   514 tokens\n",
      " 63%|██████▎   | 887/1400 [2:42:12<1:20:28,  9.41s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.05 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.77 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   249 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6141.43 ms /   313 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   131 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3504.38 ms /   590 tokens\n",
      " 63%|██████▎   | 888/1400 [2:42:22<1:21:55,  9.60s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.94 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.85 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   316 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7788.63 ms /   388 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   135 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3577.26 ms /   601 tokens\n",
      " 64%|██████▎   | 889/1400 [2:42:34<1:27:17, 10.25s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.50 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.95 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   225 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5578.07 ms /   300 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   143 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3802.73 ms /   613 tokens\n",
      " 64%|██████▎   | 890/1400 [2:42:43<1:25:54, 10.11s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     147.73 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.53 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   169 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4138.90 ms /   227 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    64 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1866.94 ms /   517 tokens\n",
      " 64%|██████▎   | 891/1400 [2:42:50<1:16:12,  8.98s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.88 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.41 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   305 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7563.01 ms /   368 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   119 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3199.73 ms /   577 tokens\n",
      " 64%|██████▎   | 892/1400 [2:43:01<1:21:29,  9.63s/it]Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.41 ms /    92 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.19 ms /   220 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   302 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7529.58 ms /   387 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 480 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   480 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   185 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4891.01 ms /   665 tokens\n",
      " 64%|██████▍   | 893/1400 [2:43:14<1:29:23, 10.58s/it]Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.09 ms /    96 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.15 ms /   224 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   220 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5429.28 ms /   308 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 483 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   483 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   174 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4564.87 ms /   657 tokens\n",
      " 64%|██████▍   | 894/1400 [2:43:24<1:28:48, 10.53s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.95 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.56 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   334 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8270.44 ms /   404 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   141 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3735.76 ms /   606 tokens\n",
      " 64%|██████▍   | 895/1400 [2:43:37<1:33:23, 11.10s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.00 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.47 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   324 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8034.08 ms /   400 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   148 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3926.63 ms /   618 tokens\n",
      " 64%|██████▍   | 896/1400 [2:43:49<1:36:25, 11.48s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.14 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.22 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   213 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5261.05 ms /   279 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   149 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3927.69 ms /   610 tokens\n",
      " 64%|██████▍   | 897/1400 [2:43:59<1:31:33, 10.92s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     146.69 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.18 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   231 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5653.30 ms /   294 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2831.26 ms /   560 tokens\n",
      " 64%|██████▍   | 898/1400 [2:44:07<1:26:08, 10.30s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.09 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.03 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   198 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4866.91 ms /   267 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2674.64 ms /   561 tokens\n",
      " 64%|██████▍   | 899/1400 [2:44:15<1:20:08,  9.60s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.65 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.27 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   186 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4557.62 ms /   241 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2826.47 ms /   554 tokens\n",
      " 64%|██████▍   | 900/1400 [2:44:23<1:15:28,  9.06s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.53 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.78 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   221 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5468.35 ms /   297 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   106 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2898.69 ms /   577 tokens\n",
      " 64%|██████▍   | 901/1400 [2:44:32<1:14:38,  8.98s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.79 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.27 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   178 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4415.43 ms /   242 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   126 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3384.73 ms /   584 tokens\n",
      " 64%|██████▍   | 902/1400 [2:44:40<1:12:37,  8.75s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.13 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.04 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   286 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7080.82 ms /   360 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   124 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3379.26 ms /   593 tokens\n",
      " 64%|██████▍   | 903/1400 [2:44:51<1:17:41,  9.38s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.89 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.86 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   258 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6351.95 ms /   335 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   149 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3966.41 ms /   620 tokens\n",
      " 65%|██████▍   | 904/1400 [2:45:02<1:20:52,  9.78s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.05 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.07 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   231 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5677.01 ms /   302 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   108 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2944.42 ms /   574 tokens\n",
      " 65%|██████▍   | 905/1400 [2:45:11<1:18:55,  9.57s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     136.54 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.33 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   257 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6349.15 ms /   321 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   197 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5151.87 ms /   656 tokens\n",
      " 65%|██████▍   | 906/1400 [2:45:23<1:24:26, 10.26s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.88 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.47 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   304 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7532.29 ms /   377 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   182 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4777.78 ms /   650 tokens\n",
      " 65%|██████▍   | 907/1400 [2:45:35<1:30:15, 10.98s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     229.52 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.93 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   221 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5447.21 ms /   292 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2745.84 ms /   566 tokens\n",
      " 65%|██████▍   | 908/1400 [2:45:44<1:24:21, 10.29s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     143.19 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.54 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   208 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5094.69 ms /   264 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   117 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3169.83 ms /   567 tokens\n",
      " 65%|██████▍   | 909/1400 [2:45:53<1:20:03,  9.78s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     148.05 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.44 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   400 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9983.79 ms /   476 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   150 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4005.52 ms /   621 tokens\n",
      " 65%|██████▌   | 910/1400 [2:46:07<1:31:04, 11.15s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.37 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.59 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   190 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4680.47 ms /   246 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2709.92 ms /   549 tokens\n",
      " 65%|██████▌   | 911/1400 [2:46:15<1:22:37, 10.14s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.09 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.52 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5401.17 ms /   293 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   130 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3485.47 ms /   599 tokens\n",
      " 65%|██████▌   | 912/1400 [2:46:24<1:20:26,  9.89s/it]Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.20 ms /    95 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.45 ms /   223 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   287 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7134.32 ms /   375 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 483 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   483 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   172 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4529.54 ms /   655 tokens\n",
      " 65%|██████▌   | 913/1400 [2:46:36<1:25:32, 10.54s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.75 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.15 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   208 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5108.13 ms /   265 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1776.34 ms /   513 tokens\n",
      " 65%|██████▌   | 914/1400 [2:46:43<1:17:20,  9.55s/it]Llama.generate: 42 prefix-match hit, remaining 160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   160 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     219.26 ms /   164 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 288 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   288 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     251.47 ms /   292 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 157 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   157 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   238 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6000.59 ms /   395 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 551 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   551 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   205 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5414.69 ms /   756 tokens\n",
      " 65%|██████▌   | 915/1400 [2:46:55<1:22:54, 10.26s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     134.63 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.48 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   233 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5728.48 ms /   309 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   167 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4378.16 ms /   638 tokens\n",
      " 65%|██████▌   | 916/1400 [2:47:06<1:23:14, 10.32s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.22 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.48 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   227 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5566.93 ms /   286 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1821.91 ms /   517 tokens\n",
      " 66%|██████▌   | 917/1400 [2:47:13<1:16:55,  9.56s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.08 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.99 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   172 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4231.24 ms /   232 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2525.14 ms /   547 tokens\n",
      " 66%|██████▌   | 918/1400 [2:47:21<1:11:05,  8.85s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.16 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.97 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   244 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6019.96 ms /   305 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   133 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3585.43 ms /   589 tokens\n",
      " 66%|██████▌   | 919/1400 [2:47:31<1:13:42,  9.19s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.00 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.94 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   357 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8897.28 ms /   434 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   178 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4670.82 ms /   648 tokens\n",
      " 66%|██████▌   | 920/1400 [2:47:45<1:24:59, 10.62s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     170.06 ms /    91 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.63 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   239 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5911.85 ms /   323 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7748.96 ms /   777 tokens\n",
      " 66%|██████▌   | 921/1400 [2:47:59<1:33:01, 11.65s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     219.79 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.24 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   191 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4737.87 ms /   250 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2444.11 ms /   542 tokens\n",
      " 66%|██████▌   | 922/1400 [2:48:06<1:23:12, 10.44s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     216.23 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.96 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   211 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5193.82 ms /   274 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   109 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2959.13 ms /   567 tokens\n",
      " 66%|██████▌   | 923/1400 [2:48:15<1:18:39,  9.89s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.91 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.35 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   298 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7393.11 ms /   376 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   247 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6394.93 ms /   719 tokens\n",
      " 66%|██████▌   | 924/1400 [2:48:29<1:28:41, 11.18s/it]Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.63 ms /    95 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.33 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7528.37 ms /   391 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 482 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   482 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   276 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7135.24 ms /   758 tokens\n",
      " 66%|██████▌   | 925/1400 [2:48:44<1:37:44, 12.35s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.76 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.45 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6668.84 ms /   347 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   135 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3601.47 ms /   607 tokens\n",
      " 66%|██████▌   | 926/1400 [2:48:55<1:33:31, 11.84s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.55 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.02 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   288 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7089.30 ms /   353 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   136 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3640.16 ms /   596 tokens\n",
      " 66%|██████▌   | 927/1400 [2:49:06<1:31:39, 11.63s/it]Llama.generate: 42 prefix-match hit, remaining 146 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   146 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.33 ms /   150 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 274 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   274 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     248.64 ms /   278 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   140 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   247 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6193.79 ms /   387 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 537 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   537 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   196 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5197.76 ms /   733 tokens\n",
      " 66%|██████▋   | 928/1400 [2:49:18<1:32:04, 11.70s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.14 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.72 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   278 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6838.43 ms /   338 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2851.26 ms /   559 tokens\n",
      " 66%|██████▋   | 929/1400 [2:49:28<1:28:06, 11.22s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     156.41 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.16 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   313 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7741.24 ms /   371 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   197 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5157.31 ms /   650 tokens\n",
      " 66%|██████▋   | 930/1400 [2:49:41<1:32:45, 11.84s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.18 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.51 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   372 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9274.07 ms /   444 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   121 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3237.09 ms /   588 tokens\n",
      " 66%|██████▋   | 931/1400 [2:49:54<1:35:01, 12.16s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.33 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.14 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   216 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5338.41 ms /   271 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    78 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2161.94 ms /   530 tokens\n",
      " 67%|██████▋   | 932/1400 [2:50:02<1:24:55, 10.89s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.92 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.56 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   267 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6628.79 ms /   336 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   152 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4016.49 ms /   616 tokens\n",
      " 67%|██████▋   | 933/1400 [2:50:13<1:25:01, 10.93s/it]Llama.generate: 42 prefix-match hit, remaining 128 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   128 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.82 ms /   132 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 256 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   256 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     228.58 ms /   260 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   126 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   265 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6599.79 ms /   391 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 519 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   519 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   198 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5255.56 ms /   717 tokens\n",
      " 67%|██████▋   | 934/1400 [2:50:25<1:28:04, 11.34s/it]Llama.generate: 42 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   125 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.51 ms /   128 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 253 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   253 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.07 ms /   256 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   121 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   322 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8042.35 ms /   443 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 516 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   516 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   238 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6216.57 ms /   754 tokens\n",
      " 67%|██████▋   | 935/1400 [2:50:40<1:35:39, 12.34s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.78 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.13 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   332 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8254.56 ms /   406 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   180 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4733.13 ms /   648 tokens\n",
      " 67%|██████▋   | 936/1400 [2:50:53<1:37:49, 12.65s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.48 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.71 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   247 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6093.16 ms /   314 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    95 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2602.55 ms /   558 tokens\n",
      " 67%|██████▋   | 937/1400 [2:51:03<1:29:28, 11.60s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.10 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.73 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5629.01 ms /   286 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2669.11 ms /   548 tokens\n",
      " 67%|██████▋   | 938/1400 [2:51:11<1:22:29, 10.71s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.60 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.77 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   250 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6123.61 ms /   306 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2436.76 ms /   539 tokens\n",
      " 67%|██████▋   | 939/1400 [2:51:20<1:18:13, 10.18s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.39 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.30 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   268 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6622.47 ms /   328 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   169 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4459.28 ms /   624 tokens\n",
      " 67%|██████▋   | 940/1400 [2:51:32<1:21:05, 10.58s/it]Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     156.01 ms /    91 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   216 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.02 ms /   219 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   377 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9397.68 ms /   462 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 479 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   479 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   150 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3981.36 ms /   629 tokens\n",
      " 67%|██████▋   | 941/1400 [2:51:45<1:28:13, 11.53s/it]Llama.generate: 42 prefix-match hit, remaining 262 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   262 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     226.36 ms /   265 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 390 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   390 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     298.63 ms /   393 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 258 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   258 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   431 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10925.92 ms /   689 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 653 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   653 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   363 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9534.09 ms /  1016 tokens\n",
      " 67%|██████▋   | 942/1400 [2:52:06<1:49:45, 14.38s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     146.70 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.60 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   256 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6324.42 ms /   318 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6100.36 ms /   693 tokens\n",
      " 67%|██████▋   | 943/1400 [2:52:19<1:45:52, 13.90s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.13 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.10 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   338 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8379.27 ms /   402 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   120 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3253.22 ms /   579 tokens\n",
      " 67%|██████▋   | 944/1400 [2:52:31<1:41:21, 13.34s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.07 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.17 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   281 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6945.74 ms /   347 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   106 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2894.84 ms /   567 tokens\n",
      " 68%|██████▊   | 945/1400 [2:52:41<1:34:03, 12.40s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.55 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.40 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   240 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5906.20 ms /   315 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   141 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3750.81 ms /   611 tokens\n",
      " 68%|██████▊   | 946/1400 [2:52:52<1:28:35, 11.71s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     222.55 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.29 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   199 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4894.45 ms /   275 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   127 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3422.10 ms /   598 tokens\n",
      " 68%|██████▊   | 947/1400 [2:53:00<1:21:43, 10.83s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     215.79 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.10 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7495.98 ms /   375 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   162 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4268.68 ms /   628 tokens\n",
      " 68%|██████▊   | 948/1400 [2:53:13<1:24:41, 11.24s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.56 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.40 ms /   216 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   287 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7124.58 ms /   368 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   149 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3990.00 ms /   625 tokens\n",
      " 68%|██████▊   | 949/1400 [2:53:24<1:25:06, 11.32s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.18 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.24 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   251 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6185.87 ms /   317 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   191 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5005.61 ms /   651 tokens\n",
      " 68%|██████▊   | 950/1400 [2:53:36<1:25:29, 11.40s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.29 ms /    86 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.07 ms /   214 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   206 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5080.98 ms /   284 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   148 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3900.11 ms /   621 tokens\n",
      " 68%|██████▊   | 951/1400 [2:53:45<1:20:48, 10.80s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     156.86 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.15 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   293 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7224.33 ms /   364 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   157 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4142.39 ms /   623 tokens\n",
      " 68%|██████▊   | 952/1400 [2:53:57<1:22:44, 11.08s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.74 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.36 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   237 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5855.65 ms /   302 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   135 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3594.61 ms /   595 tokens\n",
      " 68%|██████▊   | 953/1400 [2:54:07<1:19:46, 10.71s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.35 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.75 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   231 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5676.13 ms /   296 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   115 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3102.21 ms /   575 tokens\n",
      " 68%|██████▊   | 954/1400 [2:54:16<1:16:08, 10.24s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.27 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.69 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   282 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6941.16 ms /   343 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    95 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2598.45 ms /   550 tokens\n",
      " 68%|██████▊   | 955/1400 [2:54:26<1:15:16, 10.15s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.49 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.39 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   254 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6256.82 ms /   317 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   172 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4492.74 ms /   629 tokens\n",
      " 68%|██████▊   | 956/1400 [2:54:37<1:17:23, 10.46s/it]Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     148.20 ms /    60 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.31 ms /   188 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 53 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    53 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   263 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6444.57 ms /   316 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 448 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   448 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2501.63 ms /   540 tokens\n",
      " 68%|██████▊   | 957/1400 [2:54:46<1:14:40, 10.11s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.29 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.43 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   178 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4351.54 ms /   233 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    69 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1964.19 ms /   518 tokens\n",
      " 68%|██████▊   | 958/1400 [2:54:53<1:07:02,  9.10s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.56 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.40 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   201 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4950.86 ms /   266 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2840.39 ms /   564 tokens\n",
      " 68%|██████▊   | 959/1400 [2:55:01<1:04:56,  8.84s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.88 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.51 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   246 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6068.53 ms /   314 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2781.48 ms /   565 tokens\n",
      " 69%|██████▊   | 960/1400 [2:55:10<1:05:43,  8.96s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.57 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.39 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   250 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6162.29 ms /   320 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   136 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3622.57 ms /   600 tokens\n",
      " 69%|██████▊   | 961/1400 [2:55:21<1:08:16,  9.33s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.68 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.29 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   284 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7004.48 ms /   348 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2661.23 ms /   555 tokens\n",
      " 69%|██████▊   | 962/1400 [2:55:31<1:09:45,  9.56s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.75 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.61 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   244 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6019.70 ms /   309 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   183 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4770.54 ms /   643 tokens\n",
      " 69%|██████▉   | 963/1400 [2:55:42<1:13:12, 10.05s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.20 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.16 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   511 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   12759.38 ms /   591 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   202 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5264.11 ms /   677 tokens\n",
      " 69%|██████▉   | 964/1400 [2:56:00<1:31:18, 12.57s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.29 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     169.28 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   234 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5727.77 ms /   293 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   191 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4986.44 ms /   645 tokens\n",
      " 69%|██████▉   | 965/1400 [2:56:11<1:27:51, 12.12s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     165.53 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.00 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   251 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6179.05 ms /   309 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   104 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2827.63 ms /   558 tokens\n",
      " 69%|██████▉   | 966/1400 [2:56:21<1:21:45, 11.30s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     158.45 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     219.55 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   271 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6668.47 ms /   326 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2347.73 ms /   534 tokens\n",
      " 69%|██████▉   | 967/1400 [2:56:30<1:17:29, 10.74s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.88 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.94 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   216 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5295.76 ms /   289 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   214 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5562.88 ms /   682 tokens\n",
      " 69%|██████▉   | 968/1400 [2:56:41<1:18:31, 10.91s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.45 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.52 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   373 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9301.66 ms /   446 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   197 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5144.31 ms /   664 tokens\n",
      " 69%|██████▉   | 969/1400 [2:56:56<1:26:45, 12.08s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     217.64 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     220.71 ms /   216 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   340 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8440.65 ms /   420 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   117 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3155.73 ms /   592 tokens\n",
      " 69%|██████▉   | 970/1400 [2:57:08<1:26:31, 12.07s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.48 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     169.17 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   272 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6721.86 ms /   332 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1657.99 ms /   509 tokens\n",
      " 69%|██████▉   | 971/1400 [2:57:17<1:19:12, 11.08s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.91 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.87 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   205 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5042.98 ms /   266 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   109 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2960.83 ms /   565 tokens\n",
      " 69%|██████▉   | 972/1400 [2:57:26<1:13:20, 10.28s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.10 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.11 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   222 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5449.64 ms /   280 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    81 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2222.95 ms /   533 tokens\n",
      " 70%|██████▉   | 973/1400 [2:57:34<1:08:25,  9.61s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.13 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.01 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   239 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5861.34 ms /   301 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3003.40 ms /   566 tokens\n",
      " 70%|██████▉   | 974/1400 [2:57:43<1:07:35,  9.52s/it]Llama.generate: 42 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   116 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.20 ms /   119 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 244 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   244 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.98 ms /   247 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   112 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   410 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10262.42 ms /   522 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 507 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   507 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   244 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6346.10 ms /   751 tokens\n",
      " 70%|██████▉   | 975/1400 [2:58:00<1:23:23, 11.77s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.10 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.65 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   256 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6283.57 ms /   312 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2890.71 ms /   557 tokens\n",
      " 70%|██████▉   | 976/1400 [2:58:09<1:18:29, 11.11s/it]Llama.generate: 42 prefix-match hit, remaining 165 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   165 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.24 ms /   168 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 293 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   293 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     238.53 ms /   296 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 161 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   161 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   402 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10106.90 ms /   563 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 556 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   556 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   317 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8263.89 ms /   873 tokens\n",
      " 70%|██████▉   | 977/1400 [2:58:28<1:34:39, 13.43s/it]Llama.generate: 42 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   102 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     218.38 ms /   106 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 230 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   230 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.99 ms /   233 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   262 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6484.29 ms /   360 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 493 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   493 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   140 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3769.38 ms /   633 tokens\n",
      " 70%|██████▉   | 978/1400 [2:58:39<1:28:39, 12.61s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.23 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.74 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   231 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5669.61 ms /   299 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   130 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3463.61 ms /   593 tokens\n",
      " 70%|██████▉   | 979/1400 [2:58:49<1:21:55, 11.68s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.96 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.53 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   329 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8127.44 ms /   389 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   118 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3177.81 ms /   573 tokens\n",
      " 70%|███████   | 980/1400 [2:59:00<1:21:44, 11.68s/it]Llama.generate: 42 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     218.59 ms /   102 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 226 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   226 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     223.37 ms /   230 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   272 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6753.94 ms /   367 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 489 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   489 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   150 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3978.96 ms /   639 tokens\n",
      " 70%|███████   | 981/1400 [2:59:11<1:20:33, 11.54s/it]Llama.generate: 42 prefix-match hit, remaining 181 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   181 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     223.39 ms /   185 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 309 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   309 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     260.32 ms /   313 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 177 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   177 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   360 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9045.61 ms /   537 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 572 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   572 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   250 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6575.81 ms /   822 tokens\n",
      " 70%|███████   | 982/1400 [2:59:28<1:29:58, 12.92s/it]Llama.generate: 42 prefix-match hit, remaining 106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   106 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.46 ms /   110 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 234 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   234 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     225.82 ms /   238 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   102 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   241 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5992.40 ms /   343 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 497 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   497 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   168 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4426.20 ms /   665 tokens\n",
      " 70%|███████   | 983/1400 [2:59:38<1:25:30, 12.30s/it]Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.17 ms /   102 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   227 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.86 ms /   230 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5856.46 ms /   331 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 490 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   490 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   183 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4770.76 ms /   673 tokens\n",
      " 70%|███████   | 984/1400 [2:59:49<1:22:41, 11.93s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     169.38 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.68 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   208 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5157.17 ms /   282 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2766.49 ms /   569 tokens\n",
      " 70%|███████   | 985/1400 [2:59:58<1:15:01, 10.85s/it]Llama.generate: 42 prefix-match hit, remaining 97 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    97 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.35 ms /   100 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 225 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   225 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.51 ms /   228 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   259 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6400.67 ms /   351 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 488 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   488 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   276 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7145.02 ms /   764 tokens\n",
      " 70%|███████   | 986/1400 [3:00:12<1:21:14, 11.77s/it]Llama.generate: 42 prefix-match hit, remaining 110 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   110 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.24 ms /   113 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 238 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   238 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.58 ms /   241 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   106 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   380 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9463.86 ms /   486 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 501 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   501 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   184 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4824.35 ms /   685 tokens\n",
      " 70%|███████   | 987/1400 [3:00:26<1:27:04, 12.65s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.21 ms /    89 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.25 ms /   216 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   353 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8764.38 ms /   434 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   161 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4234.77 ms /   637 tokens\n",
      " 71%|███████   | 988/1400 [3:00:40<1:28:26, 12.88s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.53 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.71 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   358 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8894.60 ms /   439 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   143 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3793.64 ms /   618 tokens\n",
      " 71%|███████   | 989/1400 [3:00:53<1:28:41, 12.95s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.16 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.42 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   266 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6579.15 ms /   331 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2621.53 ms /   554 tokens\n",
      " 71%|███████   | 990/1400 [3:01:03<1:21:34, 11.94s/it]Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     221.74 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   218 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.94 ms /   221 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7539.11 ms /   389 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 481 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   481 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   216 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5603.73 ms /   697 tokens\n",
      " 71%|███████   | 991/1400 [3:01:16<1:24:44, 12.43s/it]Llama.generate: 42 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.42 ms /    98 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 222 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   222 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.85 ms /   226 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   221 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5453.91 ms /   311 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 485 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   485 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   182 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4812.04 ms /   667 tokens\n",
      " 71%|███████   | 992/1400 [3:01:27<1:20:59, 11.91s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.87 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.86 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   235 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5815.49 ms /   310 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3066.36 ms /   581 tokens\n",
      " 71%|███████   | 993/1400 [3:01:36<1:15:29, 11.13s/it]Llama.generate: 42 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   114 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.44 ms /   117 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 242 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   242 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.03 ms /   245 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   111 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   273 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6785.41 ms /   384 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 505 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   505 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   301 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7753.63 ms /   806 tokens\n",
      " 71%|███████   | 994/1400 [3:01:51<1:23:02, 12.27s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.43 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.87 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   240 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5928.93 ms /   308 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2781.46 ms /   565 tokens\n",
      " 71%|███████   | 995/1400 [3:02:00<1:16:29, 11.33s/it]Llama.generate: 42 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   126 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.33 ms /   129 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 254 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   254 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.36 ms /   257 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   123 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   292 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7275.10 ms /   415 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 517 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   517 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   247 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6435.70 ms /   764 tokens\n",
      " 71%|███████   | 996/1400 [3:02:14<1:21:56, 12.17s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     165.07 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.84 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   296 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7316.45 ms /   361 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   268 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6917.84 ms /   728 tokens\n",
      " 71%|███████   | 997/1400 [3:02:29<1:26:40, 12.91s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.45 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.81 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   191 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4663.79 ms /   249 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2242.33 ms /   533 tokens\n",
      " 71%|███████▏  | 998/1400 [3:02:36<1:15:11, 11.22s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.24 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.29 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   221 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5471.45 ms /   280 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   106 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2872.18 ms /   559 tokens\n",
      " 71%|███████▏  | 999/1400 [3:02:45<1:10:01, 10.48s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.84 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.78 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   273 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6726.05 ms /   344 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   127 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3389.98 ms /   593 tokens\n",
      " 71%|███████▏  | 1000/1400 [3:02:55<1:09:55, 10.49s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     158.74 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.82 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   289 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7148.23 ms /   359 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   167 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4397.62 ms /   632 tokens\n",
      " 72%|███████▏  | 1001/1400 [3:03:07<1:12:38, 10.92s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.02 ms /    91 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.66 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   301 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7482.42 ms /   384 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   232 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6045.67 ms /   710 tokens\n",
      " 72%|███████▏  | 1002/1400 [3:03:21<1:18:28, 11.83s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.74 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.15 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   238 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5855.32 ms /   298 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   129 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3425.91 ms /   583 tokens\n",
      " 72%|███████▏  | 1003/1400 [3:03:31<1:13:56, 11.17s/it]Llama.generate: 42 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.08 ms /   101 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 226 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   226 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.84 ms /   229 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   354 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8842.46 ms /   448 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 489 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   489 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   179 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4699.31 ms /   668 tokens\n",
      " 72%|███████▏  | 1004/1400 [3:03:45<1:19:14, 12.01s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.77 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.40 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   320 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7969.37 ms /   403 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   148 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3918.14 ms /   626 tokens\n",
      " 72%|███████▏  | 1005/1400 [3:03:57<1:19:36, 12.09s/it]Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.91 ms /    96 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.72 ms /   224 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   315 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7819.90 ms /   404 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 483 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   483 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   123 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3304.12 ms /   606 tokens\n",
      " 72%|███████▏  | 1006/1400 [3:04:09<1:18:18, 11.93s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.71 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.35 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   183 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4511.15 ms /   248 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2719.49 ms /   557 tokens\n",
      " 72%|███████▏  | 1007/1400 [3:04:16<1:09:41, 10.64s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.79 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.26 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   306 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7536.90 ms /   371 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   181 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4728.58 ms /   640 tokens\n",
      " 72%|███████▏  | 1008/1400 [3:04:29<1:13:31, 11.25s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.72 ms /    85 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.91 ms /   213 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   278 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6871.62 ms /   357 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   136 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3649.04 ms /   609 tokens\n",
      " 72%|███████▏  | 1009/1400 [3:04:40<1:12:38, 11.15s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     149.51 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.71 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   271 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6660.83 ms /   330 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3298.89 ms /   576 tokens\n",
      " 72%|███████▏  | 1010/1400 [3:04:50<1:10:49, 10.90s/it]Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.36 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.25 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   348 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8628.49 ms /   435 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 482 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   482 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   194 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5087.18 ms /   676 tokens\n",
      " 72%|███████▏  | 1011/1400 [3:05:04<1:16:53, 11.86s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.84 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.26 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   265 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6499.06 ms /   322 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   144 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3783.70 ms /   596 tokens\n",
      " 72%|███████▏  | 1012/1400 [3:05:15<1:14:23, 11.50s/it]Llama.generate: 42 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   119 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.41 ms /   123 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 247 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   247 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     224.58 ms /   251 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   115 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   261 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6486.63 ms /   376 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 510 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   510 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   187 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4920.51 ms /   697 tokens\n",
      " 72%|███████▏  | 1013/1400 [3:05:27<1:14:55, 11.62s/it]Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.68 ms /   102 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   227 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.74 ms /   230 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   329 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8157.13 ms /   424 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 490 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   490 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   284 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7318.49 ms /   774 tokens\n",
      " 72%|███████▏  | 1014/1400 [3:05:43<1:23:00, 12.90s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.82 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.29 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   296 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7301.14 ms /   367 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   179 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4668.86 ms /   645 tokens\n",
      " 72%|███████▎  | 1015/1400 [3:05:55<1:21:44, 12.74s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.79 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.52 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   139 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3446.15 ms /   194 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    74 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2090.36 ms /   524 tokens\n",
      " 73%|███████▎  | 1016/1400 [3:06:01<1:08:30, 10.71s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.97 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.15 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   286 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7057.03 ms /   360 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   120 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3212.27 ms /   589 tokens\n",
      " 73%|███████▎  | 1017/1400 [3:06:12<1:08:20, 10.71s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.28 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.07 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   273 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6707.99 ms /   329 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   119 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3180.60 ms /   570 tokens\n",
      " 73%|███████▎  | 1018/1400 [3:06:22<1:07:18, 10.57s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.49 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.01 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5809.22 ms /   306 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   134 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3594.74 ms /   600 tokens\n",
      " 73%|███████▎  | 1019/1400 [3:06:32<1:05:44, 10.35s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.26 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.87 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   206 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5059.41 ms /   266 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2305.32 ms /   537 tokens\n",
      " 73%|███████▎  | 1020/1400 [3:06:40<1:00:42,  9.59s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     147.14 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.96 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   385 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9611.43 ms /   449 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   202 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5256.70 ms /   661 tokens\n",
      " 73%|███████▎  | 1021/1400 [3:06:55<1:11:15, 11.28s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.35 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.20 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   311 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7660.30 ms /   378 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   116 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3141.27 ms /   578 tokens\n",
      " 73%|███████▎  | 1022/1400 [3:07:06<1:10:53, 11.25s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     137.92 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.29 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   300 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7403.80 ms /   367 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   186 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4878.46 ms /   648 tokens\n",
      " 73%|███████▎  | 1023/1400 [3:07:19<1:13:18, 11.67s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.28 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.44 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   268 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6598.02 ms /   342 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3091.93 ms /   582 tokens\n",
      " 73%|███████▎  | 1024/1400 [3:07:29<1:10:12, 11.20s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.73 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.50 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   211 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5175.04 ms /   276 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2525.10 ms /   552 tokens\n",
      " 73%|███████▎  | 1025/1400 [3:07:37<1:04:15, 10.28s/it]Llama.generate: 42 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   104 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.08 ms /   107 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 232 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   232 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.55 ms /   235 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   405 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10117.07 ms /   505 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 495 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   495 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5698.47 ms /   713 tokens\n",
      " 73%|███████▎  | 1026/1400 [3:07:53<1:15:10, 12.06s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.15 ms /    86 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.14 ms /   213 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   300 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7407.56 ms /   379 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   116 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3170.04 ms /   589 tokens\n",
      " 73%|███████▎  | 1027/1400 [3:08:04<1:12:56, 11.73s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.97 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.79 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   235 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5775.96 ms /   305 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   175 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4582.05 ms /   640 tokens\n",
      " 73%|███████▎  | 1028/1400 [3:08:15<1:10:55, 11.44s/it]Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.74 ms /    92 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   216 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     227.42 ms /   220 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   304 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7544.21 ms /   388 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 479 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   479 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   232 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5996.46 ms /   711 tokens\n",
      " 74%|███████▎  | 1029/1400 [3:08:29<1:15:24, 12.20s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.76 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.71 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   214 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5260.11 ms /   291 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3010.97 ms /   580 tokens\n",
      " 74%|███████▎  | 1030/1400 [3:08:38<1:08:44, 11.15s/it]Llama.generate: 42 prefix-match hit, remaining 132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   132 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     223.76 ms /   136 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 260 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   260 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     254.61 ms /   264 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   127 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   273 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6791.10 ms /   400 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 523 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   523 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   196 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5184.59 ms /   719 tokens\n",
      " 74%|███████▎  | 1031/1400 [3:08:50<1:11:00, 11.55s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.83 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.86 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   193 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4767.63 ms /   250 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    76 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2151.10 ms /   528 tokens\n",
      " 74%|███████▎  | 1032/1400 [3:08:58<1:03:02, 10.28s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.54 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.79 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   333 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8208.49 ms /   393 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   106 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2894.67 ms /   561 tokens\n",
      " 74%|███████▍  | 1033/1400 [3:09:09<1:05:06, 10.64s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.00 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.00 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   279 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6897.78 ms /   347 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   106 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2901.18 ms /   569 tokens\n",
      " 74%|███████▍  | 1034/1400 [3:09:19<1:04:07, 10.51s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.16 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.43 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   220 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5393.39 ms /   274 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2624.40 ms /   545 tokens\n",
      " 74%|███████▍  | 1035/1400 [3:09:28<1:00:06,  9.88s/it]Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.56 ms /   103 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   228 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.58 ms /   231 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   301 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7470.52 ms /   397 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 491 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   491 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   170 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4468.82 ms /   661 tokens\n",
      " 74%|███████▍  | 1036/1400 [3:09:40<1:04:27, 10.63s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.07 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.33 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   264 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6532.63 ms /   338 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2704.73 ms /   568 tokens\n",
      " 74%|███████▍  | 1037/1400 [3:09:50<1:02:29, 10.33s/it]Llama.generate: 42 prefix-match hit, remaining 142 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   142 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.82 ms /   145 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 270 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   270 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     225.21 ms /   273 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   138 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   284 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7088.67 ms /   422 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 533 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   533 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   315 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8181.50 ms /   848 tokens\n",
      " 74%|███████▍  | 1038/1400 [3:10:05<1:12:04, 11.95s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     146.18 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.31 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6628.47 ms /   329 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2503.85 ms /   545 tokens\n",
      " 74%|███████▍  | 1039/1400 [3:10:15<1:07:25, 11.21s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     170.33 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.77 ms /   216 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   257 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6318.03 ms /   339 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   241 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6245.86 ms /   717 tokens\n",
      " 74%|███████▍  | 1040/1400 [3:10:28<1:10:23, 11.73s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.90 ms /    89 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.41 ms /   217 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   281 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6957.92 ms /   363 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2975.07 ms /   586 tokens\n",
      " 74%|███████▍  | 1041/1400 [3:10:38<1:07:42, 11.32s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.53 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.65 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   254 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6264.71 ms /   315 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   139 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3710.10 ms /   595 tokens\n",
      " 74%|███████▍  | 1042/1400 [3:10:49<1:05:52, 11.04s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     170.41 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.46 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   184 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4506.68 ms /   242 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2099.74 ms /   527 tokens\n",
      " 74%|███████▍  | 1043/1400 [3:10:56<58:28,  9.83s/it]  Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.85 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.28 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   264 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6490.34 ms /   326 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   154 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4080.19 ms /   611 tokens\n",
      " 75%|███████▍  | 1044/1400 [3:11:07<1:00:23, 10.18s/it]Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.08 ms /    60 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 184 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   184 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.03 ms /   188 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    52 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   225 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5508.39 ms /   277 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 447 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   447 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    67 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1887.71 ms /   514 tokens\n",
      " 75%|███████▍  | 1045/1400 [3:11:14<55:59,  9.46s/it]  Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     138.41 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.17 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   356 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8828.99 ms /   425 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   109 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2955.77 ms /   573 tokens\n",
      " 75%|███████▍  | 1046/1400 [3:11:26<1:00:33, 10.26s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.29 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.85 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   277 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6829.67 ms /   331 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   161 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4215.33 ms /   610 tokens\n",
      " 75%|███████▍  | 1047/1400 [3:11:38<1:02:30, 10.63s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     227.43 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.32 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   390 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9720.08 ms /   466 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   158 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4212.32 ms /   628 tokens\n",
      " 75%|███████▍  | 1048/1400 [3:11:52<1:08:55, 11.75s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     139.03 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.79 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   281 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6909.82 ms /   338 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1754.04 ms /   512 tokens\n",
      " 75%|███████▍  | 1049/1400 [3:12:01<1:03:54, 10.92s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.11 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     215.30 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   208 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5103.27 ms /   281 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   127 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3441.15 ms /   595 tokens\n",
      " 75%|███████▌  | 1050/1400 [3:12:10<1:00:20, 10.34s/it]Llama.generate: 42 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   112 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.60 ms /   116 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 240 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   240 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     224.85 ms /   244 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   108 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   306 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7611.33 ms /   414 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 503 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   503 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   156 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4117.04 ms /   659 tokens\n",
      " 75%|███████▌  | 1051/1400 [3:12:22<1:03:23, 10.90s/it]Llama.generate: 42 prefix-match hit, remaining 141 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   141 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.35 ms /   144 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 269 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   269 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     222.28 ms /   272 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   136 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   300 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7517.30 ms /   436 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 532 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   532 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   359 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9302.98 ms /   891 tokens\n",
      " 75%|███████▌  | 1052/1400 [3:12:40<1:14:19, 12.82s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.17 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.66 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   199 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4885.68 ms /   264 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   158 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4173.59 ms /   618 tokens\n",
      " 75%|███████▌  | 1053/1400 [3:12:49<1:08:14, 11.80s/it]Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.39 ms /   104 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   228 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     224.03 ms /   232 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 97 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    97 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   309 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7680.62 ms /   406 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 491 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   491 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   137 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3654.52 ms /   628 tokens\n",
      " 75%|███████▌  | 1054/1400 [3:13:01<1:08:00, 11.79s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.68 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.89 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   191 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4711.19 ms /   247 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2382.27 ms /   537 tokens\n",
      " 75%|███████▌  | 1055/1400 [3:13:08<1:00:25, 10.51s/it]Llama.generate: 42 prefix-match hit, remaining 116 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   116 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     215.55 ms /   120 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 244 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   244 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     231.32 ms /   248 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 112 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   112 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   274 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6853.24 ms /   386 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 507 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   507 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   164 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4395.27 ms /   671 tokens\n",
      " 75%|███████▌  | 1056/1400 [3:13:20<1:02:20, 10.87s/it]Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.59 ms /    95 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.68 ms /   223 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   280 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6921.73 ms /   368 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 482 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   482 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   190 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4972.33 ms /   672 tokens\n",
      " 76%|███████▌  | 1057/1400 [3:13:33<1:04:40, 11.31s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     151.15 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.85 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   287 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7103.86 ms /   362 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   151 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4011.95 ms /   621 tokens\n",
      " 76%|███████▌  | 1058/1400 [3:13:44<1:04:46, 11.36s/it]Llama.generate: 42 prefix-match hit, remaining 97 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    97 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.36 ms /   100 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 225 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   225 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.79 ms /   228 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   330 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8192.20 ms /   424 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 488 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   488 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   224 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5793.79 ms /   712 tokens\n",
      " 76%|███████▌  | 1059/1400 [3:13:58<1:09:46, 12.28s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     132.34 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.07 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   351 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8716.08 ms /   415 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   177 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4623.59 ms /   637 tokens\n",
      " 76%|███████▌  | 1060/1400 [3:14:12<1:11:58, 12.70s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.72 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.67 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   214 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5259.77 ms /   274 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   121 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3237.03 ms /   575 tokens\n",
      " 76%|███████▌  | 1061/1400 [3:14:21<1:05:17, 11.55s/it]Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.69 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.45 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   264 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6547.36 ms /   345 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 477 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   477 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   140 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3722.14 ms /   617 tokens\n",
      " 76%|███████▌  | 1062/1400 [3:14:32<1:03:39, 11.30s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.50 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.95 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   293 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7270.39 ms /   377 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   243 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6286.76 ms /   721 tokens\n",
      " 76%|███████▌  | 1063/1400 [3:14:46<1:07:53, 12.09s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     169.74 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.43 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   277 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6837.56 ms /   342 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3299.46 ms /   582 tokens\n",
      " 76%|███████▌  | 1064/1400 [3:14:56<1:05:02, 11.62s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.47 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.81 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   231 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5711.93 ms /   305 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   109 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2968.96 ms /   578 tokens\n",
      " 76%|███████▌  | 1065/1400 [3:15:05<1:00:37, 10.86s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     145.38 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.66 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   276 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6804.80 ms /   341 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   132 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3545.88 ms /   591 tokens\n",
      " 76%|███████▌  | 1066/1400 [3:15:16<1:00:11, 10.81s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.58 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.68 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   253 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6248.33 ms /   314 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   170 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4494.71 ms /   626 tokens\n",
      " 76%|███████▌  | 1067/1400 [3:15:27<1:00:32, 10.91s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.09 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.57 ms /   209 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   209 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5133.22 ms /   281 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2693.09 ms /   565 tokens\n",
      " 76%|███████▋  | 1068/1400 [3:15:35<55:56, 10.11s/it]  Llama.generate: 42 prefix-match hit, remaining 140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   140 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     245.90 ms /   144 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 268 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   268 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     246.21 ms /   272 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   138 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   319 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7979.40 ms /   457 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 531 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   531 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   208 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5440.33 ms /   739 tokens\n",
      " 76%|███████▋  | 1069/1400 [3:15:49<1:02:06, 11.26s/it]Llama.generate: 42 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   127 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.54 ms /   130 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 255 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   255 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.73 ms /   258 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   124 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   292 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7259.52 ms /   416 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 518 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   518 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   239 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6246.48 ms /   757 tokens\n",
      " 76%|███████▋  | 1070/1400 [3:16:03<1:06:20, 12.06s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.59 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.25 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   331 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8197.60 ms /   408 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   153 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4043.82 ms /   623 tokens\n",
      " 76%|███████▋  | 1071/1400 [3:16:16<1:07:04, 12.23s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.34 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.84 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   279 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6894.06 ms /   343 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   190 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4945.13 ms /   648 tokens\n",
      " 77%|███████▋  | 1072/1400 [3:16:28<1:06:54, 12.24s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     133.51 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.17 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   297 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7354.93 ms /   363 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   115 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3104.75 ms /   575 tokens\n",
      " 77%|███████▋  | 1073/1400 [3:16:39<1:04:21, 11.81s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.32 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.85 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   222 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5445.91 ms /   282 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2621.01 ms /   551 tokens\n",
      " 77%|███████▋  | 1074/1400 [3:16:47<58:42, 10.80s/it]  Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.14 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.08 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   373 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9305.09 ms /   450 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   127 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3424.33 ms /   598 tokens\n",
      " 77%|███████▋  | 1075/1400 [3:17:00<1:02:17, 11.50s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     141.48 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.55 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   276 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6825.04 ms /   336 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   205 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5341.67 ms /   659 tokens\n",
      " 77%|███████▋  | 1076/1400 [3:17:13<1:03:44, 11.80s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.34 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.65 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   335 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8300.44 ms /   399 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   181 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4773.58 ms /   639 tokens\n",
      " 77%|███████▋  | 1077/1400 [3:17:26<1:06:14, 12.31s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.56 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.11 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   198 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4869.75 ms /   258 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2349.71 ms /   540 tokens\n",
      " 77%|███████▋  | 1078/1400 [3:17:34<58:30, 10.90s/it]  Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.76 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.04 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   231 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5683.31 ms /   293 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   118 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3168.48 ms /   574 tokens\n",
      " 77%|███████▋  | 1079/1400 [3:17:43<55:38, 10.40s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.46 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.91 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   257 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6346.69 ms /   327 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   135 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3634.37 ms /   599 tokens\n",
      " 77%|███████▋  | 1080/1400 [3:17:54<55:26, 10.40s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.88 ms /    89 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     225.07 ms /   217 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   299 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7418.08 ms /   380 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   116 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3125.85 ms /   592 tokens\n",
      " 77%|███████▋  | 1081/1400 [3:18:05<56:14, 10.58s/it]Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.57 ms /    98 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     221.15 ms /   227 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   285 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7115.60 ms /   376 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   486 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   230 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6011.20 ms /   716 tokens\n",
      " 77%|███████▋  | 1082/1400 [3:18:18<1:00:49, 11.48s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.48 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.16 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   298 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7380.68 ms /   364 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   191 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4965.80 ms /   651 tokens\n",
      " 77%|███████▋  | 1083/1400 [3:18:31<1:02:35, 11.85s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.21 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.63 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   227 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5581.99 ms /   289 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2149.25 ms /   533 tokens\n",
      " 77%|███████▋  | 1084/1400 [3:18:39<56:34, 10.74s/it]  Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.90 ms /    61 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     169.43 ms /   189 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 53 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    53 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   260 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6400.13 ms /   313 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1676.56 ms /   507 tokens\n",
      " 78%|███████▊  | 1085/1400 [3:18:48<52:45, 10.05s/it]Llama.generate: 42 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    93 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.90 ms /    97 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   221 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.66 ms /   224 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   228 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5618.78 ms /   317 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 484 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   484 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   123 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3311.93 ms /   607 tokens\n",
      " 78%|███████▊  | 1086/1400 [3:18:57<51:27,  9.83s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.91 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.86 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   317 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7824.87 ms /   382 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   105 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2858.52 ms /   565 tokens\n",
      " 78%|███████▊  | 1087/1400 [3:19:08<53:12, 10.20s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.28 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.80 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   319 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7931.19 ms /   402 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   172 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4530.81 ms /   650 tokens\n",
      " 78%|███████▊  | 1088/1400 [3:19:21<57:09, 10.99s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.02 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.22 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   285 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7044.50 ms /   352 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   142 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3778.01 ms /   604 tokens\n",
      " 78%|███████▊  | 1089/1400 [3:19:32<57:19, 11.06s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.39 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.82 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   187 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4610.29 ms /   242 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2400.87 ms /   536 tokens\n",
      " 78%|███████▊  | 1090/1400 [3:19:39<51:29,  9.97s/it]Llama.generate: 42 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   118 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     170.63 ms /   121 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 246 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   246 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.12 ms /   249 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 113 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   113 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   293 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7324.21 ms /   406 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 509 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   509 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   188 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4927.76 ms /   697 tokens\n",
      " 78%|███████▊  | 1091/1400 [3:19:52<55:28, 10.77s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.78 ms /    91 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.43 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   225 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5535.35 ms /   308 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   114 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3067.72 ms /   592 tokens\n",
      " 78%|███████▊  | 1092/1400 [3:20:01<52:35, 10.24s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     137.77 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.60 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   214 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5238.23 ms /   272 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   185 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4817.51 ms /   638 tokens\n",
      " 78%|███████▊  | 1093/1400 [3:20:12<52:39, 10.29s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.53 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.85 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   329 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8118.02 ms /   391 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   136 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3665.02 ms /   592 tokens\n",
      " 78%|███████▊  | 1094/1400 [3:20:24<55:26, 10.87s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.73 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.85 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6638.43 ms /   346 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3032.40 ms /   583 tokens\n",
      " 78%|███████▊  | 1095/1400 [3:20:34<54:01, 10.63s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.01 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.54 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5803.20 ms /   304 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2730.26 ms /   563 tokens\n",
      " 78%|███████▊  | 1096/1400 [3:20:43<51:18, 10.13s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     169.88 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.21 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   232 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5687.87 ms /   290 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   235 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6097.47 ms /   688 tokens\n",
      " 78%|███████▊  | 1097/1400 [3:20:55<54:13, 10.74s/it]Llama.generate: 42 prefix-match hit, remaining 246 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   246 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     234.19 ms /   250 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 374 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   374 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     262.84 ms /   377 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 242 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   242 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   276 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6989.40 ms /   518 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 637 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   637 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   313 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8239.00 ms /   950 tokens\n",
      " 78%|███████▊  | 1098/1400 [3:21:11<1:01:36, 12.24s/it]Llama.generate: 42 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    93 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.12 ms /    97 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   221 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.22 ms /   224 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   205 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5081.47 ms /   294 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 484 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   484 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   175 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4597.70 ms /   659 tokens\n",
      " 78%|███████▊  | 1099/1400 [3:21:21<58:11, 11.60s/it]  Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.29 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.00 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   254 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6247.92 ms /   314 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3057.11 ms /   567 tokens\n",
      " 79%|███████▊  | 1100/1400 [3:21:30<55:08, 11.03s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.16 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.06 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   265 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6559.99 ms /   331 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   176 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4614.63 ms /   635 tokens\n",
      " 79%|███████▊  | 1101/1400 [3:21:42<55:46, 11.19s/it]Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.68 ms /    99 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.94 ms /   227 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   151 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3733.22 ms /   242 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   486 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   142 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3779.24 ms /   628 tokens\n",
      " 79%|███████▊  | 1102/1400 [3:21:50<50:43, 10.21s/it]Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.67 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   211 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.67 ms /   214 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   264 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6520.10 ms /   343 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 474 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   474 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   223 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5762.24 ms /   697 tokens\n",
      " 79%|███████▉  | 1103/1400 [3:22:03<54:13, 10.96s/it]Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.22 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   218 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     217.76 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   248 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6116.63 ms /   334 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 481 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   481 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   125 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3391.01 ms /   606 tokens\n",
      " 79%|███████▉  | 1104/1400 [3:22:13<52:34, 10.66s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.83 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.32 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   169 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4137.77 ms /   228 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1792.58 ms /   515 tokens\n",
      " 79%|███████▉  | 1105/1400 [3:22:19<46:00,  9.36s/it]Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.47 ms /    89 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.31 ms /   217 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   279 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6924.73 ms /   361 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 477 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   477 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   234 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6066.78 ms /   711 tokens\n",
      " 79%|███████▉  | 1106/1400 [3:22:32<51:46, 10.57s/it]Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     149.39 ms /    92 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.92 ms /   220 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   296 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7312.82 ms /   382 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 480 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   480 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   157 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4159.60 ms /   637 tokens\n",
      " 79%|███████▉  | 1107/1400 [3:22:44<53:28, 10.95s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.81 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.10 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   264 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6467.82 ms /   329 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   119 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3200.96 ms /   578 tokens\n",
      " 79%|███████▉  | 1108/1400 [3:22:54<51:58, 10.68s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.13 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.02 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   286 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7050.82 ms /   351 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   166 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4389.74 ms /   626 tokens\n",
      " 79%|███████▉  | 1109/1400 [3:23:06<53:26, 11.02s/it]Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.73 ms /    98 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.92 ms /   226 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   342 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8524.93 ms /   434 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   486 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   241 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6258.64 ms /   727 tokens\n",
      " 79%|███████▉  | 1110/1400 [3:23:21<59:17, 12.27s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.62 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.46 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   277 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6851.91 ms /   347 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   202 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5277.76 ms /   666 tokens\n",
      " 79%|███████▉  | 1111/1400 [3:23:34<59:24, 12.33s/it]Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.73 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   218 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     217.44 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   282 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6959.43 ms /   369 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 481 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   481 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   141 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3745.73 ms /   622 tokens\n",
      " 79%|███████▉  | 1112/1400 [3:23:45<57:26, 11.97s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     170.27 ms /    91 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     218.09 ms /   219 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   195 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4791.11 ms /   278 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3278.79 ms /   600 tokens\n",
      " 80%|███████▉  | 1113/1400 [3:23:53<52:15, 10.92s/it]Llama.generate: 42 prefix-match hit, remaining 142 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   142 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     225.84 ms /   145 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 270 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   270 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     221.26 ms /   273 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 138 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   138 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   334 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8389.90 ms /   472 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 533 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   533 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   207 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5467.10 ms /   740 tokens\n",
      " 80%|███████▉  | 1114/1400 [3:24:08<56:56, 11.95s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.78 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.26 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   187 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4590.89 ms /   244 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2098.97 ms /   526 tokens\n",
      " 80%|███████▉  | 1115/1400 [3:24:15<49:49, 10.49s/it]Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.93 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.47 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   343 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8520.32 ms /   431 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 482 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   482 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   245 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6339.98 ms /   727 tokens\n",
      " 80%|███████▉  | 1116/1400 [3:24:30<56:23, 11.91s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.20 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.01 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   362 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8989.89 ms /   427 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   211 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5458.34 ms /   671 tokens\n",
      " 80%|███████▉  | 1117/1400 [3:24:45<1:00:20, 12.79s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.40 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.40 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   251 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6200.57 ms /   323 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   199 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5176.44 ms /   665 tokens\n",
      " 80%|███████▉  | 1118/1400 [3:24:57<58:41, 12.49s/it]  Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.50 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.09 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   265 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6537.23 ms /   335 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   207 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5377.76 ms /   671 tokens\n",
      " 80%|███████▉  | 1119/1400 [3:25:09<58:12, 12.43s/it]Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     170.03 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   218 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.55 ms /   221 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   265 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6533.60 ms /   352 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 481 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   481 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   152 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4058.88 ms /   633 tokens\n",
      " 80%|████████  | 1120/1400 [3:25:20<55:58, 11.99s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.14 ms /    86 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.50 ms /   214 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   315 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7815.85 ms /   393 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   182 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4762.60 ms /   655 tokens\n",
      " 80%|████████  | 1121/1400 [3:25:33<57:10, 12.30s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.45 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.26 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   263 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6461.70 ms /   325 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   250 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6431.58 ms /   707 tokens\n",
      " 80%|████████  | 1122/1400 [3:25:46<58:22, 12.60s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.65 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.20 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   226 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5531.35 ms /   284 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1631.29 ms /   507 tokens\n",
      " 80%|████████  | 1123/1400 [3:25:54<51:10, 11.08s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.33 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     223.66 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   246 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6052.07 ms /   317 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   116 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3180.48 ms /   581 tokens\n",
      " 80%|████████  | 1124/1400 [3:26:03<49:01, 10.66s/it]Llama.generate: 42 prefix-match hit, remaining 140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   140 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.97 ms /   144 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 268 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   268 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     250.19 ms /   272 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 137 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   137 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   411 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10279.71 ms /   548 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 531 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   531 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   208 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5479.33 ms /   739 tokens\n",
      " 80%|████████  | 1125/1400 [3:26:20<56:31, 12.33s/it]Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.90 ms /    92 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.88 ms /   220 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   351 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8702.19 ms /   435 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 480 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   480 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   168 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4406.03 ms /   648 tokens\n",
      " 80%|████████  | 1126/1400 [3:26:33<57:55, 12.68s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.64 ms /    89 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.33 ms /   216 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   347 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8607.09 ms /   429 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   174 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4581.82 ms /   650 tokens\n",
      " 80%|████████  | 1127/1400 [3:26:47<58:57, 12.96s/it]Llama.generate: 42 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   121 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     216.14 ms /   124 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 249 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   249 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.99 ms /   252 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   118 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   260 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6460.00 ms /   378 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 512 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   512 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   185 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4845.61 ms /   697 tokens\n",
      " 81%|████████  | 1128/1400 [3:26:58<57:06, 12.60s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     165.48 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.86 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   316 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7792.55 ms /   383 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   158 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4151.85 ms /   619 tokens\n",
      " 81%|████████  | 1129/1400 [3:27:11<56:31, 12.52s/it]Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.96 ms /    61 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.64 ms /   189 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 53 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    53 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   357 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8867.42 ms /   410 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 448 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   448 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2398.71 ms /   536 tokens\n",
      " 81%|████████  | 1130/1400 [3:27:22<55:12, 12.27s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.28 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.18 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   222 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5491.53 ms /   285 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3055.03 ms /   571 tokens\n",
      " 81%|████████  | 1131/1400 [3:27:31<50:32, 11.28s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.46 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.38 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   258 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6345.18 ms /   326 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3047.67 ms /   574 tokens\n",
      " 81%|████████  | 1132/1400 [3:27:41<48:25, 10.84s/it]Llama.generate: 42 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    93 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.38 ms /    97 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   221 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     226.57 ms /   225 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   279 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6885.07 ms /   367 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 484 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   484 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   134 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3578.62 ms /   618 tokens\n",
      " 81%|████████  | 1133/1400 [3:27:52<48:20, 10.86s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.25 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.86 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   291 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7214.26 ms /   359 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   176 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4607.29 ms /   639 tokens\n",
      " 81%|████████  | 1134/1400 [3:28:04<50:00, 11.28s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.89 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.42 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   185 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4552.61 ms /   241 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    56 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1657.65 ms /   507 tokens\n",
      " 81%|████████  | 1135/1400 [3:28:11<43:37,  9.88s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     156.08 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.18 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   261 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6466.33 ms /   327 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   189 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4925.40 ms /   650 tokens\n",
      " 81%|████████  | 1136/1400 [3:28:23<45:56, 10.44s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.30 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.99 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   375 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9305.36 ms /   430 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2409.49 ms /   538 tokens\n",
      " 81%|████████  | 1137/1400 [3:28:35<47:55, 10.94s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     151.66 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.50 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   231 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5689.85 ms /   288 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2266.10 ms /   533 tokens\n",
      " 81%|████████▏ | 1138/1400 [3:28:43<44:18, 10.15s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.02 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.87 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   288 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7059.11 ms /   344 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2445.69 ms /   539 tokens\n",
      " 81%|████████▏ | 1139/1400 [3:28:53<43:47, 10.07s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.24 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.06 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   256 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6314.51 ms /   332 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   159 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4207.40 ms /   630 tokens\n",
      " 81%|████████▏ | 1140/1400 [3:29:04<44:41, 10.31s/it]Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.89 ms /   100 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 224 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   224 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     215.37 ms /   228 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   206 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5112.61 ms /   298 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 487 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   487 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   128 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3441.30 ms /   615 tokens\n",
      " 82%|████████▏ | 1141/1400 [3:29:13<42:49,  9.92s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     169.83 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.16 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   233 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5744.88 ms /   291 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2315.54 ms /   536 tokens\n",
      " 82%|████████▏ | 1142/1400 [3:29:21<40:46,  9.48s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.50 ms /    83 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.47 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   259 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6373.56 ms /   335 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   182 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4760.18 ms /   653 tokens\n",
      " 82%|████████▏ | 1143/1400 [3:29:33<43:15, 10.10s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.86 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.74 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   316 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7839.14 ms /   381 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   257 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6608.07 ms /   716 tokens\n",
      " 82%|████████▏ | 1144/1400 [3:29:48<49:07, 11.51s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.42 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.10 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   326 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8028.17 ms /   381 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2718.86 ms /   550 tokens\n",
      " 82%|████████▏ | 1145/1400 [3:29:59<48:25, 11.39s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.21 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     215.47 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   214 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5278.88 ms /   285 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   114 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3089.38 ms /   580 tokens\n",
      " 82%|████████▏ | 1146/1400 [3:30:08<44:55, 10.61s/it]Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.61 ms /    94 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   218 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     222.37 ms /   222 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   273 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6760.44 ms /   359 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 481 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   481 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   120 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3218.22 ms /   601 tokens\n",
      " 82%|████████▏ | 1147/1400 [3:30:18<44:29, 10.55s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     169.05 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.74 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   226 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5554.06 ms /   283 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   128 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3439.71 ms /   581 tokens\n",
      " 82%|████████▏ | 1148/1400 [3:30:27<42:51, 10.20s/it]Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     145.12 ms /    99 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 224 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   224 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.94 ms /   227 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   305 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7542.33 ms /   397 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 487 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   487 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   172 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4501.73 ms /   659 tokens\n",
      " 82%|████████▏ | 1149/1400 [3:30:40<45:27, 10.87s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.74 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.41 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   310 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7629.30 ms /   375 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   215 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5585.93 ms /   675 tokens\n",
      " 82%|████████▏ | 1150/1400 [3:30:54<48:42, 11.69s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     137.43 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.22 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   321 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7951.83 ms /   390 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   171 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4463.68 ms /   635 tokens\n",
      " 82%|████████▏ | 1151/1400 [3:31:06<49:50, 12.01s/it]Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.38 ms /   103 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   228 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.50 ms /   231 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 97 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    97 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   278 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6924.84 ms /   375 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 491 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   491 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   243 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6299.89 ms /   734 tokens\n",
      " 82%|████████▏ | 1152/1400 [3:31:20<51:37, 12.49s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.97 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.03 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   263 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6469.92 ms /   321 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3024.30 ms /   565 tokens\n",
      " 82%|████████▏ | 1153/1400 [3:31:30<48:09, 11.70s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.63 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     218.75 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   272 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6724.80 ms /   339 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2845.97 ms /   565 tokens\n",
      " 82%|████████▏ | 1154/1400 [3:31:40<45:50, 11.18s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.72 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.03 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   262 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6445.02 ms /   327 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   137 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3637.90 ms /   597 tokens\n",
      " 82%|████████▎ | 1155/1400 [3:31:50<44:47, 10.97s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.87 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.91 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   267 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6566.20 ms /   336 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   135 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3593.19 ms /   598 tokens\n",
      " 83%|████████▎ | 1156/1400 [3:32:01<44:07, 10.85s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.82 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.99 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   259 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6391.76 ms /   323 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   157 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4177.87 ms /   616 tokens\n",
      " 83%|████████▎ | 1157/1400 [3:32:12<44:03, 10.88s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.24 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.48 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   220 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5404.24 ms /   277 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   195 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5088.93 ms /   648 tokens\n",
      " 83%|████████▎ | 1158/1400 [3:32:23<43:51, 10.87s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.70 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.52 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   187 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4585.86 ms /   250 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    95 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2634.80 ms /   553 tokens\n",
      " 83%|████████▎ | 1159/1400 [3:32:30<39:49,  9.91s/it]Llama.generate: 42 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   111 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.91 ms /   114 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 239 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   239 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     197.78 ms /   242 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   107 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   355 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8845.52 ms /   462 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 502 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   502 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   211 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5530.86 ms /   713 tokens\n",
      " 83%|████████▎ | 1160/1400 [3:32:45<45:29, 11.37s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.18 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.20 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5373.10 ms /   293 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   136 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3628.54 ms /   606 tokens\n",
      " 83%|████████▎ | 1161/1400 [3:32:54<42:54, 10.77s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.31 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.12 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   259 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6397.42 ms /   325 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   134 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3566.05 ms /   595 tokens\n",
      " 83%|████████▎ | 1162/1400 [3:33:05<42:12, 10.64s/it]Llama.generate: 42 prefix-match hit, remaining 139 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   139 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     256.79 ms /   143 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 267 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   267 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     223.21 ms /   270 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 135 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   135 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   244 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6105.82 ms /   379 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 530 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   530 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   209 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5545.06 ms /   739 tokens\n",
      " 83%|████████▎ | 1163/1400 [3:33:17<43:49, 11.10s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.12 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.57 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   289 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7131.10 ms /   353 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   140 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3704.31 ms /   598 tokens\n",
      " 83%|████████▎ | 1164/1400 [3:33:28<43:46, 11.13s/it]Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.42 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     221.91 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   209 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5140.65 ms /   292 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 477 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   477 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   132 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3523.80 ms /   609 tokens\n",
      " 83%|████████▎ | 1165/1400 [3:33:37<41:13, 10.52s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.66 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.37 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   286 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7038.18 ms /   349 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   152 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3998.04 ms /   610 tokens\n",
      " 83%|████████▎ | 1166/1400 [3:33:49<42:04, 10.79s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.22 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.06 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   247 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6081.54 ms /   312 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2659.27 ms /   557 tokens\n",
      " 83%|████████▎ | 1167/1400 [3:33:58<39:56, 10.29s/it]Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.86 ms /    95 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.92 ms /   223 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   297 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7328.03 ms /   386 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 483 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   483 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   201 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5239.97 ms /   684 tokens\n",
      " 83%|████████▎ | 1168/1400 [3:34:11<42:52, 11.09s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.45 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.52 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   200 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4930.51 ms /   271 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2981.01 ms /   576 tokens\n",
      " 84%|████████▎ | 1169/1400 [3:34:19<39:31, 10.27s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.70 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.40 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   240 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5892.58 ms /   296 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   129 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3431.75 ms /   580 tokens\n",
      " 84%|████████▎ | 1170/1400 [3:34:29<38:44, 10.10s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.81 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.40 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   273 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6741.97 ms /   343 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   226 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5852.84 ms /   691 tokens\n",
      " 84%|████████▎ | 1171/1400 [3:34:42<41:54, 10.98s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.63 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.89 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   317 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7862.98 ms /   390 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   136 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3637.96 ms /   603 tokens\n",
      " 84%|████████▎ | 1172/1400 [3:34:54<42:46, 11.25s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     157.13 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.82 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   324 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7994.71 ms /   394 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   145 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3849.67 ms /   608 tokens\n",
      " 84%|████████▍ | 1173/1400 [3:35:06<43:40, 11.54s/it]Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.56 ms /    93 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.39 ms /   221 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   340 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8448.78 ms /   425 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 480 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   480 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   194 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5094.00 ms /   674 tokens\n",
      " 84%|████████▍ | 1174/1400 [3:35:20<46:11, 12.26s/it]Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.95 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   211 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.94 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   251 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6198.33 ms /   330 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 474 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   474 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3050.63 ms /   586 tokens\n",
      " 84%|████████▍ | 1175/1400 [3:35:30<43:05, 11.49s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.52 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.03 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   270 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6652.98 ms /   334 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   142 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3768.50 ms /   601 tokens\n",
      " 84%|████████▍ | 1176/1400 [3:35:40<42:09, 11.29s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     158.49 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.67 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   293 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7235.02 ms /   353 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   127 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3428.24 ms /   582 tokens\n",
      " 84%|████████▍ | 1177/1400 [3:35:51<41:40, 11.21s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.46 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.72 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   301 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7450.70 ms /   361 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2751.88 ms /   554 tokens\n",
      " 84%|████████▍ | 1178/1400 [3:36:02<40:47, 11.02s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.88 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.37 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   231 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5691.71 ms /   299 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2990.30 ms /   572 tokens\n",
      " 84%|████████▍ | 1179/1400 [3:36:11<38:28, 10.44s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.24 ms /    85 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.65 ms /   213 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   262 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6479.41 ms /   340 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   181 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4745.51 ms /   653 tokens\n",
      " 84%|████████▍ | 1180/1400 [3:36:23<39:37, 10.81s/it]Llama.generate: 42 prefix-match hit, remaining 107 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   107 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.60 ms /   111 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 235 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   235 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     225.99 ms /   239 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   342 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8537.14 ms /   445 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 498 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   498 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   172 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4536.45 ms /   670 tokens\n",
      " 84%|████████▍ | 1181/1400 [3:36:36<42:25, 11.62s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.33 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.17 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   248 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6117.92 ms /   309 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2361.67 ms /   541 tokens\n",
      " 84%|████████▍ | 1182/1400 [3:36:45<39:12, 10.79s/it]Llama.generate: 42 prefix-match hit, remaining 136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   136 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     218.87 ms /   139 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 264 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   264 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     217.40 ms /   267 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 132 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   132 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   354 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8869.10 ms /   486 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 527 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   527 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   267 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6928.54 ms /   794 tokens\n",
      " 84%|████████▍ | 1183/1400 [3:37:01<44:57, 12.43s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.03 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.43 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   292 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7227.18 ms /   360 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   123 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3298.79 ms /   585 tokens\n",
      " 85%|████████▍ | 1184/1400 [3:37:12<43:07, 11.98s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.74 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.63 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6628.70 ms /   332 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2779.00 ms /   559 tokens\n",
      " 85%|████████▍ | 1185/1400 [3:37:22<40:36, 11.33s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.83 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.89 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   246 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6058.07 ms /   309 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   191 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4972.31 ms /   650 tokens\n",
      " 85%|████████▍ | 1186/1400 [3:37:34<40:30, 11.36s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.12 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.75 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   223 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5460.33 ms /   279 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3287.67 ms /   573 tokens\n",
      " 85%|████████▍ | 1187/1400 [3:37:43<37:58, 10.70s/it]Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.95 ms /    86 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   211 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.48 ms /   214 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   399 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9916.45 ms /   478 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 474 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   474 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   147 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3899.24 ms /   621 tokens\n",
      " 85%|████████▍ | 1188/1400 [3:37:57<41:30, 11.75s/it]Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     156.50 ms /    93 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   218 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.95 ms /   221 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   296 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7363.41 ms /   382 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 481 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   481 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   133 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3556.71 ms /   614 tokens\n",
      " 85%|████████▍ | 1189/1400 [3:38:08<40:50, 11.61s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.86 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.59 ms /   189 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   213 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5223.42 ms /   267 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2755.43 ms /   549 tokens\n",
      " 85%|████████▌ | 1190/1400 [3:38:17<37:14, 10.64s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.58 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.89 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   277 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6839.48 ms /   348 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   139 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3688.43 ms /   605 tokens\n",
      " 85%|████████▌ | 1191/1400 [3:38:27<37:21, 10.73s/it]Llama.generate: 42 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.00 ms /   188 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 313 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   313 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     242.53 ms /   316 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 182 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   182 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   304 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7643.50 ms /   486 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 576 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   576 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   302 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7939.44 ms /   878 tokens\n",
      " 85%|████████▌ | 1192/1400 [3:38:44<42:44, 12.33s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.27 ms /    85 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.39 ms /   213 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   204 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5084.88 ms /   281 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   108 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2961.46 ms /   580 tokens\n",
      " 85%|████████▌ | 1193/1400 [3:38:52<38:31, 11.17s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.29 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.34 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   292 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7218.49 ms /   357 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   148 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3903.25 ms /   608 tokens\n",
      " 85%|████████▌ | 1194/1400 [3:39:04<38:43, 11.28s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     145.55 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.79 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   264 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6515.91 ms /   331 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   170 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4494.38 ms /   632 tokens\n",
      " 85%|████████▌ | 1195/1400 [3:39:15<38:37, 11.31s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.20 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.49 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   209 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5141.41 ms /   268 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2382.01 ms /   539 tokens\n",
      " 85%|████████▌ | 1196/1400 [3:39:23<34:59, 10.29s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.62 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.28 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6616.68 ms /   335 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    98 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2725.59 ms /   558 tokens\n",
      " 86%|████████▌ | 1197/1400 [3:39:33<34:18, 10.14s/it]Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.51 ms /    99 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     217.10 ms /   227 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   293 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7320.21 ms /   384 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   486 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   187 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4909.23 ms /   673 tokens\n",
      " 86%|████████▌ | 1198/1400 [3:39:45<36:40, 10.89s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     141.72 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.69 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   296 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7310.98 ms /   367 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   177 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4624.37 ms /   641 tokens\n",
      " 86%|████████▌ | 1199/1400 [3:39:58<37:54, 11.32s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.85 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.83 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   248 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6123.92 ms /   313 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   179 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4697.71 ms /   639 tokens\n",
      " 86%|████████▌ | 1200/1400 [3:40:09<37:37, 11.29s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.78 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.41 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   216 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5315.00 ms /   284 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   105 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2881.10 ms /   568 tokens\n",
      " 86%|████████▌ | 1201/1400 [3:40:17<34:46, 10.49s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     151.13 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.26 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   325 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8081.75 ms /   408 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   153 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4056.94 ms /   631 tokens\n",
      " 86%|████████▌ | 1202/1400 [3:40:30<36:37, 11.10s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     170.61 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.71 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   301 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7417.73 ms /   366 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6107.52 ms /   695 tokens\n",
      " 86%|████████▌ | 1203/1400 [3:40:44<39:12, 11.94s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.51 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.92 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5361.99 ms /   281 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2373.09 ms /   542 tokens\n",
      " 86%|████████▌ | 1204/1400 [3:40:52<35:18, 10.81s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.21 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.14 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   225 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5505.50 ms /   283 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1693.06 ms /   509 tokens\n",
      " 86%|████████▌ | 1205/1400 [3:41:00<31:57,  9.84s/it]Llama.generate: 42 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.20 ms /   101 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 226 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   226 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.16 ms /   229 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   288 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7113.80 ms /   383 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 489 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   489 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   201 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5241.47 ms /   690 tokens\n",
      " 86%|████████▌ | 1206/1400 [3:41:12<34:39, 10.72s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     140.39 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.64 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   250 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6141.86 ms /   305 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   151 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3976.12 ms /   601 tokens\n",
      " 86%|████████▌ | 1207/1400 [3:41:23<34:14, 10.64s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.27 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     169.24 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   266 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6566.61 ms /   326 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2709.66 ms /   554 tokens\n",
      " 86%|████████▋ | 1208/1400 [3:41:32<33:05, 10.34s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.24 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.17 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   226 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5569.86 ms /   294 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2741.06 ms /   562 tokens\n",
      " 86%|████████▋ | 1209/1400 [3:41:41<31:22,  9.86s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.42 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.24 ms /   189 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   195 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4777.32 ms /   250 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    92 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2545.52 ms /   541 tokens\n",
      " 86%|████████▋ | 1210/1400 [3:41:49<29:10,  9.22s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     169.70 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.03 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   245 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5995.94 ms /   304 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   121 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3254.74 ms /   575 tokens\n",
      " 86%|████████▋ | 1211/1400 [3:41:59<29:24,  9.34s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.25 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.60 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   231 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5686.01 ms /   287 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2733.51 ms /   551 tokens\n",
      " 87%|████████▋ | 1212/1400 [3:42:07<28:45,  9.18s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     140.35 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.31 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   369 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9151.95 ms /   432 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   142 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3788.90 ms /   600 tokens\n",
      " 87%|████████▋ | 1213/1400 [3:42:21<32:27, 10.41s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     165.87 ms /    61 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.70 ms /   189 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   222 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5470.59 ms /   276 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1600.69 ms /   503 tokens\n",
      " 87%|████████▋ | 1214/1400 [3:42:28<29:31,  9.52s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.11 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.18 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   192 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4744.88 ms /   259 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2616.96 ms /   555 tokens\n",
      " 87%|████████▋ | 1215/1400 [3:42:36<27:44,  9.00s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.13 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.48 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   256 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6309.95 ms /   319 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   180 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4710.15 ms /   638 tokens\n",
      " 87%|████████▋ | 1216/1400 [3:42:47<29:48,  9.72s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.53 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.76 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   213 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5256.04 ms /   291 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   220 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5722.25 ms /   692 tokens\n",
      " 87%|████████▋ | 1217/1400 [3:42:59<31:09, 10.22s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.51 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.33 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   285 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7027.79 ms /   355 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   101 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2746.67 ms /   565 tokens\n",
      " 87%|████████▋ | 1218/1400 [3:43:09<30:58, 10.21s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     144.81 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.58 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   263 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6571.24 ms /   347 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   233 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6031.31 ms /   711 tokens\n",
      " 87%|████████▋ | 1219/1400 [3:43:22<33:18, 11.04s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     147.70 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.49 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   298 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7384.82 ms /   368 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   198 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5132.26 ms /   662 tokens\n",
      " 87%|████████▋ | 1220/1400 [3:43:35<34:46, 11.59s/it]Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.76 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 206 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   206 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.80 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   254 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6292.53 ms /   328 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 469 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   469 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   130 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3496.04 ms /   599 tokens\n",
      " 87%|████████▋ | 1221/1400 [3:43:45<33:20, 11.17s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.89 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.37 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   332 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8212.99 ms /   395 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   150 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3989.40 ms /   608 tokens\n",
      " 87%|████████▋ | 1222/1400 [3:43:57<34:25, 11.61s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.61 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.67 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   267 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6586.24 ms /   340 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   178 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4696.64 ms /   646 tokens\n",
      " 87%|████████▋ | 1223/1400 [3:44:09<34:17, 11.63s/it]Llama.generate: 42 prefix-match hit, remaining 104 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   104 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     156.79 ms /   107 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 232 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   232 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.95 ms /   235 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   360 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8965.87 ms /   460 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 495 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   495 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   175 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4599.75 ms /   670 tokens\n",
      " 87%|████████▋ | 1224/1400 [3:44:23<36:08, 12.32s/it]Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.48 ms /    92 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 216 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   216 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.69 ms /   220 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   279 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6927.54 ms /   363 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 479 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   479 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   206 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5348.04 ms /   685 tokens\n",
      " 88%|████████▊ | 1225/1400 [3:44:36<36:16, 12.44s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     146.67 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.21 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   265 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6530.32 ms /   330 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2905.18 ms /   566 tokens\n",
      " 88%|████████▊ | 1226/1400 [3:44:46<33:47, 11.65s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.20 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.77 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   124 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3057.01 ms /   181 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2142.89 ms /   527 tokens\n",
      " 88%|████████▊ | 1227/1400 [3:44:51<28:22,  9.84s/it]Llama.generate: 42 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   117 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.69 ms /   120 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 245 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   245 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.32 ms /   248 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 114 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   114 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   305 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7644.00 ms /   419 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 508 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   508 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   192 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5012.26 ms /   700 tokens\n",
      " 88%|████████▊ | 1228/1400 [3:45:04<30:59, 10.81s/it]Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.35 ms /    86 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   211 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.34 ms /   214 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   323 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8027.20 ms /   402 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 474 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   474 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   179 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4757.57 ms /   653 tokens\n",
      " 88%|████████▊ | 1229/1400 [3:45:18<32:50, 11.52s/it]Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     219.46 ms /   103 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   227 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     231.80 ms /   231 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   271 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6688.43 ms /   367 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 490 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   490 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   172 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4504.89 ms /   662 tokens\n",
      " 88%|████████▊ | 1230/1400 [3:45:29<32:46, 11.57s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.11 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.46 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   212 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5198.22 ms /   270 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    87 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2416.42 ms /   539 tokens\n",
      " 88%|████████▊ | 1231/1400 [3:45:37<29:36, 10.51s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.03 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.34 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   297 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7335.15 ms /   357 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    95 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2630.80 ms /   550 tokens\n",
      " 88%|████████▊ | 1232/1400 [3:45:48<29:17, 10.46s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     158.47 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.46 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   199 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4885.38 ms /   255 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   125 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3341.40 ms /   576 tokens\n",
      " 88%|████████▊ | 1233/1400 [3:45:56<27:34,  9.91s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.05 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.71 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   204 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4988.38 ms /   266 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2506.91 ms /   548 tokens\n",
      " 88%|████████▊ | 1234/1400 [3:46:04<25:46,  9.31s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.76 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.53 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   212 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5207.05 ms /   270 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    58 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1707.47 ms /   511 tokens\n",
      " 88%|████████▊ | 1235/1400 [3:46:11<23:55,  8.70s/it]Llama.generate: 42 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    93 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.62 ms /    97 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   221 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.63 ms /   224 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   212 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5208.91 ms /   301 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 484 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   484 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   168 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4447.00 ms /   652 tokens\n",
      " 88%|████████▊ | 1236/1400 [3:46:21<24:55,  9.12s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.59 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.10 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   308 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7609.59 ms /   371 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   193 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5026.95 ms /   651 tokens\n",
      " 88%|████████▊ | 1237/1400 [3:46:35<27:57, 10.29s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.83 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.44 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   316 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7854.70 ms /   391 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   169 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4423.16 ms /   639 tokens\n",
      " 88%|████████▊ | 1238/1400 [3:46:47<29:41, 11.00s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     132.21 ms /    61 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.68 ms /   189 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   227 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5608.78 ms /   282 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    54 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1642.11 ms /   503 tokens\n",
      " 88%|████████▊ | 1239/1400 [3:46:55<26:46,  9.98s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.44 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.91 ms /   207 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   191 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4686.52 ms /   262 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   139 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3755.37 ms /   605 tokens\n",
      " 89%|████████▊ | 1240/1400 [3:47:04<25:43,  9.65s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     170.28 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.65 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   246 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6049.40 ms /   303 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3066.56 ms /   564 tokens\n",
      " 89%|████████▊ | 1241/1400 [3:47:13<25:26,  9.60s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     135.54 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.60 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   416 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10388.61 ms /   493 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   148 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3928.45 ms /   620 tokens\n",
      " 89%|████████▊ | 1242/1400 [3:47:28<29:17, 11.12s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.52 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.20 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   297 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7372.79 ms /   368 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   137 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3655.00 ms /   603 tokens\n",
      " 89%|████████▉ | 1243/1400 [3:47:39<29:20, 11.21s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.54 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.57 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   248 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6101.88 ms /   304 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   108 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2951.80 ms /   559 tokens\n",
      " 89%|████████▉ | 1244/1400 [3:47:49<27:46, 10.69s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     138.27 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.91 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   274 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6797.63 ms /   332 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   156 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4117.18 ms /   609 tokens\n",
      " 89%|████████▉ | 1245/1400 [3:48:00<28:02, 10.86s/it]Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     224.76 ms /   103 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   227 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     231.17 ms /   231 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   351 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8744.08 ms /   446 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 490 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   490 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   177 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4665.66 ms /   667 tokens\n",
      " 89%|████████▉ | 1246/1400 [3:48:14<30:12, 11.77s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.78 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.85 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   220 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5467.68 ms /   285 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   215 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5616.53 ms /   675 tokens\n",
      " 89%|████████▉ | 1247/1400 [3:48:25<29:47, 11.68s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.25 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.33 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   225 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5546.82 ms /   280 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2443.41 ms /   539 tokens\n",
      " 89%|████████▉ | 1248/1400 [3:48:34<27:06, 10.70s/it]Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.05 ms /    79 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 203 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   203 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.84 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   321 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7968.49 ms /   392 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 466 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   466 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   146 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3869.99 ms /   612 tokens\n",
      " 89%|████████▉ | 1249/1400 [3:48:46<28:04, 11.16s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.51 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.23 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   252 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6226.61 ms /   316 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2851.59 ms /   562 tokens\n",
      " 89%|████████▉ | 1250/1400 [3:48:55<26:37, 10.65s/it]Llama.generate: 42 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   121 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.62 ms /   125 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 249 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   249 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     225.82 ms /   253 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 118 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   118 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   309 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7719.05 ms /   427 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 512 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   512 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   174 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4616.22 ms /   686 tokens\n",
      " 89%|████████▉ | 1251/1400 [3:49:08<28:02, 11.29s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.46 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.61 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   215 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5288.70 ms /   280 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3062.62 ms /   573 tokens\n",
      " 89%|████████▉ | 1252/1400 [3:49:17<25:58, 10.53s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.16 ms /    86 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.94 ms /   213 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   325 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8068.26 ms /   403 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   166 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4347.49 ms /   639 tokens\n",
      " 90%|████████▉ | 1253/1400 [3:49:30<27:28, 11.22s/it]Llama.generate: 42 prefix-match hit, remaining 126 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   126 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     149.08 ms /   129 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 254 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   254 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.18 ms /   257 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   123 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   347 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8668.36 ms /   470 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 517 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   517 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   301 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7825.67 ms /   818 tokens\n",
      " 90%|████████▉ | 1254/1400 [3:49:47<31:26, 12.92s/it]Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.23 ms /    82 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 207 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.47 ms /   210 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    75 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   348 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8659.63 ms /   423 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 470 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   470 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   252 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6499.20 ms /   722 tokens\n",
      " 90%|████████▉ | 1255/1400 [3:50:02<33:06, 13.70s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     147.09 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.69 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   385 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9571.35 ms /   443 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    55 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1628.62 ms /   507 tokens\n",
      " 90%|████████▉ | 1256/1400 [3:50:14<31:20, 13.06s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.18 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.64 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   313 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7750.66 ms /   383 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   155 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4085.58 ms /   619 tokens\n",
      " 90%|████████▉ | 1257/1400 [3:50:26<30:32, 12.82s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.99 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.12 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   212 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5189.78 ms /   266 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3020.11 ms /   562 tokens\n",
      " 90%|████████▉ | 1258/1400 [3:50:35<27:20, 11.55s/it]Llama.generate: 42 prefix-match hit, remaining 102 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   102 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.70 ms /   105 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 230 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   230 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.21 ms /   233 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   278 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6933.52 ms /   376 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 493 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   493 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   202 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5281.73 ms /   695 tokens\n",
      " 90%|████████▉ | 1259/1400 [3:50:47<27:54, 11.88s/it]Llama.generate: 42 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    86 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.70 ms /    90 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 214 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   214 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     216.84 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   265 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6552.13 ms /   347 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 477 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   477 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   134 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3605.54 ms /   611 tokens\n",
      " 90%|█████████ | 1260/1400 [3:50:58<26:47, 11.49s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.20 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.00 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   184 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4560.04 ms /   249 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   111 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2996.98 ms /   572 tokens\n",
      " 90%|█████████ | 1261/1400 [3:51:06<24:11, 10.44s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.90 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.13 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   218 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5367.41 ms /   278 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    77 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2189.99 ms /   531 tokens\n",
      " 90%|█████████ | 1262/1400 [3:51:14<22:19,  9.71s/it]Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.66 ms /    98 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.38 ms /   226 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   268 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6651.83 ms /   359 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   486 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   168 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4420.23 ms /   654 tokens\n",
      " 90%|█████████ | 1263/1400 [3:51:25<23:21, 10.23s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.59 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.65 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   199 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4913.58 ms /   267 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    95 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2635.76 ms /   558 tokens\n",
      " 90%|█████████ | 1264/1400 [3:51:33<21:38,  9.55s/it]Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.02 ms /    91 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 215 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   215 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.84 ms /   218 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   284 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7075.75 ms /   368 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 478 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   478 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   142 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3804.57 ms /   620 tokens\n",
      " 90%|█████████ | 1265/1400 [3:51:44<22:40, 10.07s/it]Llama.generate: 42 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   120 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     218.51 ms /   124 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 248 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   248 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     227.50 ms /   252 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 117 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   117 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   248 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6176.83 ms /   365 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 511 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   511 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   153 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4062.63 ms /   664 tokens\n",
      " 90%|█████████ | 1266/1400 [3:51:55<22:55, 10.27s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     219.21 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     216.28 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   182 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4462.62 ms /   243 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2375.96 ms /   541 tokens\n",
      " 90%|█████████ | 1267/1400 [3:52:03<20:47,  9.38s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     136.02 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.89 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   211 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5240.37 ms /   269 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    68 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1925.99 ms /   520 tokens\n",
      " 91%|█████████ | 1268/1400 [3:52:10<19:23,  8.82s/it]Llama.generate: 42 prefix-match hit, remaining 100 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   100 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.19 ms /   103 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 228 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   228 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.81 ms /   231 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 97 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    97 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   353 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8845.55 ms /   450 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 491 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   491 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   225 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5848.89 ms /   716 tokens\n",
      " 91%|█████████ | 1269/1400 [3:52:25<23:21, 10.70s/it]Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.32 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   211 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.59 ms /   214 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   304 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7544.64 ms /   384 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 474 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   474 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   145 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3844.27 ms /   619 tokens\n",
      " 91%|█████████ | 1270/1400 [3:52:37<23:53, 11.02s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.02 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.05 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   275 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6782.50 ms /   333 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2435.69 ms /   541 tokens\n",
      " 91%|█████████ | 1271/1400 [3:52:46<22:46, 10.59s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.44 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.79 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   263 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6487.79 ms /   325 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   118 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3183.48 ms /   574 tokens\n",
      " 91%|█████████ | 1272/1400 [3:52:57<22:15, 10.43s/it]Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     217.51 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 316 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   316 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     246.78 ms /   319 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   374 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9425.61 ms /   559 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 579 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   579 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7077.92 ms /   848 tokens\n",
      " 91%|█████████ | 1273/1400 [3:53:14<26:15, 12.40s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.17 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.26 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   188 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4615.21 ms /   245 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    82 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2266.30 ms /   533 tokens\n",
      " 91%|█████████ | 1274/1400 [3:53:21<22:50, 10.87s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.67 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.73 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   312 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7732.02 ms /   378 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2826.64 ms /   563 tokens\n",
      " 91%|█████████ | 1275/1400 [3:53:32<22:42, 10.90s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.97 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.48 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   239 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5878.76 ms /   310 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   109 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2958.10 ms /   573 tokens\n",
      " 91%|█████████ | 1276/1400 [3:53:41<21:31, 10.41s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     137.70 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.87 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   227 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5595.38 ms /   293 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2633.78 ms /   556 tokens\n",
      " 91%|█████████ | 1277/1400 [3:53:50<20:13,  9.87s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.33 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.74 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   240 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5969.70 ms /   316 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   114 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3092.10 ms /   585 tokens\n",
      " 91%|█████████▏| 1278/1400 [3:53:59<19:50,  9.76s/it]Llama.generate: 42 prefix-match hit, remaining 98 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    98 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.46 ms /   102 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 226 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   226 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     227.56 ms /   230 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   244 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6053.23 ms /   338 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 489 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   489 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   183 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4816.00 ms /   672 tokens\n",
      " 91%|█████████▏| 1279/1400 [3:54:10<20:36, 10.22s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     154.15 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.81 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   253 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6251.97 ms /   313 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   228 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5924.25 ms /   682 tokens\n",
      " 91%|█████████▏| 1280/1400 [3:54:23<21:50, 10.92s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     143.43 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.36 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   283 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6981.61 ms /   342 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    95 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2657.61 ms /   549 tokens\n",
      " 92%|█████████▏| 1281/1400 [3:54:33<21:05, 10.64s/it]Llama.generate: 42 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   105 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.37 ms /   109 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 233 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   233 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     218.75 ms /   237 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 101 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   101 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   309 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7707.26 ms /   410 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 496 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   496 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   171 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4538.21 ms /   667 tokens\n",
      " 92%|█████████▏| 1282/1400 [3:54:46<22:07, 11.25s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.46 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.95 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   214 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5294.01 ms /   272 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    75 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2124.94 ms /   527 tokens\n",
      " 92%|█████████▏| 1283/1400 [3:54:54<19:56, 10.23s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.59 ms /    89 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.73 ms /   216 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   331 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8232.08 ms /   413 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   138 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3698.18 ms /   614 tokens\n",
      " 92%|█████████▏| 1284/1400 [3:55:06<21:00, 10.86s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.03 ms /    85 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.98 ms /   213 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   260 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6423.96 ms /   338 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   199 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5206.62 ms /   672 tokens\n",
      " 92%|█████████▏| 1285/1400 [3:55:18<21:28, 11.21s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.83 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.96 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   270 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6691.21 ms /   334 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3261.23 ms /   581 tokens\n",
      " 92%|█████████▏| 1286/1400 [3:55:28<20:48, 10.95s/it]Llama.generate: 42 prefix-match hit, remaining 178 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   178 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     244.27 ms /   182 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 306 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   306 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     273.85 ms /   310 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 174 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   174 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   337 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8481.25 ms /   511 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 569 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   569 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   259 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6824.24 ms /   828 tokens\n",
      " 92%|█████████▏| 1287/1400 [3:55:44<23:23, 12.42s/it]Llama.generate: 42 prefix-match hit, remaining 143 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   143 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     215.20 ms /   146 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 271 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   271 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     227.08 ms /   274 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 139 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   139 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   369 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9271.49 ms /   508 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 534 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   534 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   261 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6845.65 ms /   795 tokens\n",
      " 92%|█████████▏| 1288/1400 [3:56:01<25:31, 13.67s/it]Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.77 ms /    96 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.89 ms /   223 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   295 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7333.40 ms /   383 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 483 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   483 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   179 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4721.57 ms /   662 tokens\n",
      " 92%|█████████▏| 1289/1400 [3:56:13<24:37, 13.31s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.01 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.88 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   297 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7348.40 ms /   355 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3064.87 ms /   566 tokens\n",
      " 92%|█████████▏| 1290/1400 [3:56:24<23:00, 12.55s/it]Llama.generate: 42 prefix-match hit, remaining 111 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   111 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.74 ms /   114 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 239 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   239 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.16 ms /   242 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 106 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   106 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   359 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8956.73 ms /   465 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 502 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   502 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   179 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4690.54 ms /   681 tokens\n",
      " 92%|█████████▏| 1291/1400 [3:56:38<23:37, 13.01s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.49 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.28 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   439 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10947.96 ms /   509 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   237 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6138.69 ms /   701 tokens\n",
      " 92%|█████████▏| 1292/1400 [3:56:55<25:49, 14.34s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.30 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.49 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   201 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4948.85 ms /   264 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   109 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2961.64 ms /   566 tokens\n",
      " 92%|█████████▏| 1293/1400 [3:57:04<22:21, 12.53s/it]Llama.generate: 42 prefix-match hit, remaining 124 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   124 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.79 ms /   128 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 252 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   252 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.52 ms /   255 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 120 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   120 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6710.01 ms /   389 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 515 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   515 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   252 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6598.25 ms /   767 tokens\n",
      " 92%|█████████▏| 1294/1400 [3:57:18<22:47, 12.90s/it]Llama.generate: 42 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     160.69 ms /    97 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 222 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   222 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     191.73 ms /   225 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   293 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7374.19 ms /   383 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 485 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   485 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   155 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4123.20 ms /   640 tokens\n",
      " 92%|█████████▎| 1295/1400 [3:57:29<22:02, 12.59s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     165.77 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.60 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   215 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5276.77 ms /   272 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    94 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2585.08 ms /   546 tokens\n",
      " 93%|█████████▎| 1296/1400 [3:57:38<19:33, 11.28s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.19 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.62 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   202 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5030.72 ms /   265 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    96 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2671.89 ms /   553 tokens\n",
      " 93%|█████████▎| 1297/1400 [3:57:46<17:45, 10.34s/it]Llama.generate: 42 prefix-match hit, remaining 162 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   162 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.26 ms /   165 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 290 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   290 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     242.66 ms /   293 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 159 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   159 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   363 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9125.79 ms /   522 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 553 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   553 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   232 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6116.59 ms /   785 tokens\n",
      " 93%|█████████▎| 1298/1400 [3:58:01<20:19, 11.95s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     170.57 ms /    67 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.18 ms /   195 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   250 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6173.50 ms /   311 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   107 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2934.75 ms /   562 tokens\n",
      " 93%|█████████▎| 1299/1400 [3:58:11<18:52, 11.21s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.87 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.16 ms /   206 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   222 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5474.75 ms /   292 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   105 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2906.01 ms /   570 tokens\n",
      " 93%|█████████▎| 1300/1400 [3:58:20<17:28, 10.49s/it]Llama.generate: 42 prefix-match hit, remaining 177 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   177 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     250.75 ms /   180 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 305 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   305 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     239.20 ms /   308 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 173 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   173 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   364 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9195.66 ms /   537 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 568 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   568 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7080.04 ms /   837 tokens\n",
      " 93%|█████████▎| 1301/1400 [3:58:37<20:25, 12.38s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     130.66 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.36 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   238 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5826.71 ms /   297 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    91 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2508.92 ms /   544 tokens\n",
      " 93%|█████████▎| 1302/1400 [3:58:45<18:24, 11.27s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.34 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.29 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   259 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6408.36 ms /   330 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   166 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4371.43 ms /   631 tokens\n",
      " 93%|█████████▎| 1303/1400 [3:58:56<18:10, 11.25s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.11 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     215.83 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   269 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6635.63 ms /   342 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   203 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5308.12 ms /   670 tokens\n",
      " 93%|█████████▎| 1304/1400 [3:59:09<18:31, 11.58s/it]Llama.generate: 42 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    93 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.93 ms /    97 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 221 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   221 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.90 ms /   224 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   317 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7904.72 ms /   406 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 484 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   484 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   168 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4435.99 ms /   652 tokens\n",
      " 93%|█████████▎| 1305/1400 [3:59:22<18:54, 11.94s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.41 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.64 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   294 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7260.27 ms /   351 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   154 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4060.80 ms /   606 tokens\n",
      " 93%|█████████▎| 1306/1400 [3:59:33<18:35, 11.87s/it]Llama.generate: 42 prefix-match hit, remaining 139 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   139 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.77 ms /   142 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 267 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   267 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     222.75 ms /   270 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   136 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   262 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6585.83 ms /   398 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 530 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   530 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   230 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6042.15 ms /   760 tokens\n",
      " 93%|█████████▎| 1307/1400 [3:59:46<18:57, 12.23s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.48 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.84 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   267 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6582.60 ms /   326 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    50 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1524.97 ms /   504 tokens\n",
      " 93%|█████████▎| 1308/1400 [3:59:55<17:02, 11.11s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.15 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.33 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   206 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5044.88 ms /   269 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2828.22 ms /   561 tokens\n",
      " 94%|█████████▎| 1309/1400 [4:00:03<15:33, 10.26s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     170.61 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.21 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   308 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7594.83 ms /   364 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1854.77 ms /   513 tokens\n",
      " 94%|█████████▎| 1310/1400 [4:00:13<15:11, 10.13s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.27 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.20 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   258 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6371.16 ms /   323 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2830.20 ms /   562 tokens\n",
      " 94%|█████████▎| 1311/1400 [4:00:23<14:46,  9.97s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.47 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.06 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   189 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4656.94 ms /   248 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2404.89 ms /   541 tokens\n",
      " 94%|█████████▎| 1312/1400 [4:00:30<13:30,  9.21s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     150.48 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.25 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   293 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7224.30 ms /   350 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   131 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3518.76 ms /   583 tokens\n",
      " 94%|█████████▍| 1313/1400 [4:00:41<14:10,  9.78s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.83 ms /    86 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.40 ms /   213 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   282 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6971.99 ms /   360 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5968.14 ms /   702 tokens\n",
      " 94%|█████████▍| 1314/1400 [4:00:54<15:33, 10.85s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.97 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.28 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   253 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6278.37 ms /   310 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   120 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3219.83 ms /   572 tokens\n",
      " 94%|█████████▍| 1315/1400 [4:01:04<14:57, 10.56s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.58 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.46 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   274 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6760.66 ms /   354 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   142 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3805.20 ms /   617 tokens\n",
      " 94%|█████████▍| 1316/1400 [4:01:15<14:56, 10.68s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     153.30 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.41 ms /   190 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   262 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6479.90 ms /   317 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    63 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1829.86 ms /   513 tokens\n",
      " 94%|█████████▍| 1317/1400 [4:01:24<13:56, 10.07s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     152.85 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     173.10 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   219 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5359.34 ms /   276 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    50 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1495.29 ms /   502 tokens\n",
      " 94%|█████████▍| 1318/1400 [4:01:31<12:35,  9.21s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     155.83 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.31 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   291 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7202.00 ms /   357 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   144 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3834.72 ms /   605 tokens\n",
      " 94%|█████████▍| 1319/1400 [4:01:43<13:19,  9.87s/it]Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     207.26 ms /    76 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 200 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   200 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.13 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   209 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5195.83 ms /   278 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 463 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   463 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2793.79 ms /   565 tokens\n",
      " 94%|█████████▍| 1320/1400 [4:01:51<12:35,  9.44s/it]Llama.generate: 42 prefix-match hit, remaining 119 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   119 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.69 ms /   122 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 247 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   247 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.96 ms /   250 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 115 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   115 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   252 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6247.97 ms /   367 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 510 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   510 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   201 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5258.87 ms /   711 tokens\n",
      " 94%|█████████▍| 1321/1400 [4:02:03<13:24, 10.19s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.93 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     200.03 ms /   199 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   291 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7223.41 ms /   356 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   149 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3932.57 ms /   608 tokens\n",
      " 94%|█████████▍| 1322/1400 [4:02:14<13:46, 10.60s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     170.29 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.60 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   234 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5763.20 ms /   301 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   127 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3432.89 ms /   588 tokens\n",
      " 94%|█████████▍| 1323/1400 [4:02:24<13:13, 10.30s/it]Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     136.38 ms /    60 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 185 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   185 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.52 ms /   188 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   359 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8921.29 ms /   413 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 448 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   448 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   151 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3969.88 ms /   599 tokens\n",
      " 95%|█████████▍| 1324/1400 [4:02:37<14:09, 11.18s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.48 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.90 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   224 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5540.96 ms /   291 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   128 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3425.90 ms /   590 tokens\n",
      " 95%|█████████▍| 1325/1400 [4:02:47<13:18, 10.65s/it]Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     168.76 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 197 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   197 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.59 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   235 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5832.20 ms /   301 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 460 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   460 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   127 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3435.90 ms /   587 tokens\n",
      " 95%|█████████▍| 1326/1400 [4:02:56<12:45, 10.35s/it]Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     217.83 ms /   103 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 227 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   227 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     212.31 ms /   230 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 96 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    96 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   426 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10719.11 ms /   522 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 490 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   490 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   234 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6094.87 ms /   724 tokens\n",
      " 95%|█████████▍| 1327/1400 [4:03:14<15:07, 12.42s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.14 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.79 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   235 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5781.97 ms /   297 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   197 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5191.46 ms /   654 tokens\n",
      " 95%|█████████▍| 1328/1400 [4:03:25<14:31, 12.10s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     135.06 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.81 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   244 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6042.21 ms /   302 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   138 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3664.46 ms /   591 tokens\n",
      " 95%|█████████▍| 1329/1400 [4:03:35<13:35, 11.49s/it]Llama.generate: 42 prefix-match hit, remaining 65 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    65 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     202.97 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 193 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   193 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.48 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   192 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4762.72 ms /   253 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 456 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   456 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    84 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2375.30 ms /   540 tokens\n",
      " 95%|█████████▌| 1330/1400 [4:03:43<12:01, 10.31s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.88 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.02 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   267 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6607.31 ms /   325 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3295.73 ms /   575 tokens\n",
      " 95%|█████████▌| 1331/1400 [4:03:53<11:51, 10.32s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.82 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     217.84 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   210 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5194.98 ms /   276 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3003.22 ms /   571 tokens\n",
      " 95%|█████████▌| 1332/1400 [4:04:02<11:06,  9.81s/it]Llama.generate: 42 prefix-match hit, remaining 141 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   141 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     205.49 ms /   144 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 269 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   269 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     224.95 ms /   272 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 137 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   137 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   323 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8128.87 ms /   460 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 532 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   532 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   227 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5981.06 ms /   759 tokens\n",
      " 95%|█████████▌| 1333/1400 [4:04:16<12:32, 11.24s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.78 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.16 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   205 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5052.65 ms /   263 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    80 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2245.83 ms /   533 tokens\n",
      " 95%|█████████▌| 1334/1400 [4:04:24<11:11, 10.17s/it]Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.20 ms /    63 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 187 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   187 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.55 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   197 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4834.08 ms /   252 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 450 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   450 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    85 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2400.67 ms /   535 tokens\n",
      " 95%|█████████▌| 1335/1400 [4:04:31<10:12,  9.42s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.59 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     220.11 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   238 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5906.33 ms /   310 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3304.72 ms /   589 tokens\n",
      " 95%|█████████▌| 1336/1400 [4:04:41<10:07,  9.49s/it]Llama.generate: 42 prefix-match hit, remaining 95 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    95 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.18 ms /    99 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 223 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   223 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     218.43 ms /   227 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   340 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8477.59 ms /   432 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 486 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   486 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   203 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5312.47 ms /   689 tokens\n",
      " 96%|█████████▌| 1337/1400 [4:04:55<11:27, 10.91s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.31 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.25 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   235 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5853.68 ms /   298 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2813.13 ms /   560 tokens\n",
      " 96%|█████████▌| 1338/1400 [4:05:04<10:42, 10.36s/it]Llama.generate: 42 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    91 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     223.67 ms /    95 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 219 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   219 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.43 ms /   223 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   385 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9644.90 ms /   472 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 482 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   482 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   237 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6167.41 ms /   719 tokens\n",
      " 96%|█████████▌| 1339/1400 [4:05:21<12:20, 12.14s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.36 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.78 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   233 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5764.23 ms /   291 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    88 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2435.48 ms /   541 tokens\n",
      " 96%|█████████▌| 1340/1400 [4:05:29<11:05, 11.08s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.73 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.46 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   180 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4509.48 ms /   241 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    90 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2515.28 ms /   545 tokens\n",
      " 96%|█████████▌| 1341/1400 [4:05:37<09:49,  9.99s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     137.83 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.71 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   230 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5680.95 ms /   296 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   216 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5647.52 ms /   677 tokens\n",
      " 96%|█████████▌| 1342/1400 [4:05:48<10:08, 10.50s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.42 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     204.64 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5819.21 ms /   294 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3041.93 ms /   565 tokens\n",
      " 96%|█████████▌| 1343/1400 [4:05:58<09:37, 10.14s/it]Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     177.56 ms /    85 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 209 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   209 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.84 ms /   213 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   278 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6905.63 ms /   355 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 472 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   472 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   113 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3098.31 ms /   585 tokens\n",
      " 96%|█████████▌| 1344/1400 [4:06:08<09:32, 10.22s/it]Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.43 ms /    95 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.22 ms /   223 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5859.96 ms /   325 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 483 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   483 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   235 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6136.15 ms /   718 tokens\n",
      " 96%|█████████▌| 1345/1400 [4:06:21<09:58, 10.88s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.42 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.89 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   265 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6547.03 ms /   325 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   195 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5113.20 ms /   650 tokens\n",
      " 96%|█████████▌| 1346/1400 [4:06:33<10:06, 11.24s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.14 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.29 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   307 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7634.18 ms /   365 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   111 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3029.02 ms /   564 tokens\n",
      " 96%|█████████▌| 1347/1400 [4:06:44<09:52, 11.18s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.31 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.01 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   232 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5724.82 ms /   295 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   168 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4418.88 ms /   626 tokens\n",
      " 96%|█████████▋| 1348/1400 [4:06:54<09:31, 10.99s/it]Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.60 ms /    78 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 202 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   202 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.12 ms /   205 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   317 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7877.22 ms /   387 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 465 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   465 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   235 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6147.61 ms /   700 tokens\n",
      " 96%|█████████▋| 1349/1400 [4:07:09<10:12, 12.02s/it]Llama.generate: 42 prefix-match hit, remaining 127 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   127 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     167.67 ms /   130 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 255 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   255 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.06 ms /   258 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 123 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   123 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   317 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7982.84 ms /   440 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 518 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   518 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   208 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5463.27 ms /   726 tokens\n",
      " 96%|█████████▋| 1350/1400 [4:07:23<10:28, 12.57s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.71 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     203.99 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   171 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4232.56 ms /   237 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2686.98 ms /   558 tokens\n",
      " 96%|█████████▋| 1351/1400 [4:07:30<08:59, 11.00s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.02 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     179.25 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   307 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7606.89 ms /   363 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   134 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3603.24 ms /   585 tokens\n",
      " 97%|█████████▋| 1352/1400 [4:07:41<08:56, 11.18s/it]Llama.generate: 42 prefix-match hit, remaining 129 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   129 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     231.20 ms /   133 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 257 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   257 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     225.39 ms /   260 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   125 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   241 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6019.31 ms /   366 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 520 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   520 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   175 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4684.82 ms /   695 tokens\n",
      " 97%|█████████▋| 1353/1400 [4:07:53<08:45, 11.19s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     193.30 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     174.62 ms /   191 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   250 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6167.50 ms /   307 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2810.34 ms /   554 tokens\n",
      " 97%|█████████▋| 1354/1400 [4:08:02<08:09, 10.64s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     149.46 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.53 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 56 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    56 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   216 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5337.54 ms /   272 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   110 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2958.21 ms /   562 tokens\n",
      " 97%|█████████▋| 1355/1400 [4:08:11<07:31, 10.04s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.33 ms /    85 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.74 ms /   213 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    79 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   268 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6708.91 ms /   347 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   123 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3349.19 ms /   596 tokens\n",
      " 97%|█████████▋| 1356/1400 [4:08:21<07:27, 10.16s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     165.68 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     210.99 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   236 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5860.46 ms /   303 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   161 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4268.61 ms /   622 tokens\n",
      " 97%|█████████▋| 1357/1400 [4:08:32<07:21, 10.27s/it]Llama.generate: 42 prefix-match hit, remaining 108 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   108 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.54 ms /   112 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 236 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   236 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     229.42 ms /   240 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 105 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   105 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   345 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8627.14 ms /   450 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 499 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   499 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   162 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4279.34 ms /   661 tokens\n",
      " 97%|█████████▋| 1358/1400 [4:08:45<07:50, 11.20s/it]Llama.generate: 42 prefix-match hit, remaining 140 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   140 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.03 ms /   144 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 268 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   268 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     221.07 ms /   271 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 136 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   136 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   215 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5408.70 ms /   351 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 531 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   531 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   179 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4800.17 ms /   710 tokens\n",
      " 97%|█████████▋| 1359/1400 [4:08:56<07:32, 11.04s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     183.34 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     199.45 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   235 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5811.28 ms /   295 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    97 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2708.25 ms /   552 tokens\n",
      " 97%|█████████▋| 1360/1400 [4:09:05<06:56, 10.40s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.64 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.66 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 81 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    81 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   285 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7111.91 ms /   366 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   121 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3289.11 ms /   596 tokens\n",
      " 97%|█████████▋| 1361/1400 [4:09:15<06:50, 10.52s/it]Llama.generate: 42 prefix-match hit, remaining 166 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   166 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     247.46 ms /   170 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 294 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   294 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     271.53 ms /   298 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 163 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   163 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   254 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6395.53 ms /   417 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 557 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   557 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   222 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5831.58 ms /   779 tokens\n",
      " 97%|█████████▋| 1362/1400 [4:09:28<07:05, 11.20s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.43 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.99 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   291 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7231.65 ms /   354 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   123 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3336.50 ms /   581 tokens\n",
      " 97%|█████████▋| 1363/1400 [4:09:39<06:51, 11.13s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     206.93 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.68 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   222 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5486.85 ms /   289 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   170 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4504.31 ms /   632 tokens\n",
      " 97%|█████████▋| 1364/1400 [4:09:50<06:33, 10.92s/it]Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     165.21 ms /    85 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 210 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   210 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.38 ms /   213 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    78 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   245 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6121.02 ms /   323 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 473 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   473 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   275 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7108.66 ms /   748 tokens\n",
      " 98%|█████████▊| 1365/1400 [4:10:03<06:50, 11.73s/it]Llama.generate: 42 prefix-match hit, remaining 272 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   272 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     256.79 ms /   276 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 400 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   400 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     303.62 ms /   403 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 267 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   267 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   380 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9704.70 ms /   647 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 663 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   663 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   309 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8162.90 ms /   972 tokens\n",
      " 98%|█████████▊| 1366/1400 [4:10:22<07:47, 13.75s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     198.05 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     216.10 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   256 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6364.71 ms /   318 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   128 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3471.43 ms /   585 tokens\n",
      " 98%|█████████▊| 1367/1400 [4:10:32<06:59, 12.71s/it]Llama.generate: 42 prefix-match hit, remaining 83 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    83 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     244.35 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 211 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   211 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     214.36 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   195 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4818.70 ms /   275 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 474 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   474 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   134 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3630.20 ms /   608 tokens\n",
      " 98%|█████████▊| 1368/1400 [4:10:41<06:10, 11.57s/it]Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.79 ms /    93 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 218 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   218 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.83 ms /   221 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 87 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    87 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   229 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5686.31 ms /   316 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 481 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   481 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   137 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3657.48 ms /   618 tokens\n",
      " 98%|█████████▊| 1369/1400 [4:10:51<05:41, 11.02s/it]Llama.generate: 42 prefix-match hit, remaining 63 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    63 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     132.79 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 191 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     172.36 ms /   194 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 59 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    59 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   283 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6994.13 ms /   342 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 454 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   454 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    86 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2423.69 ms /   540 tokens\n",
      " 98%|█████████▊| 1370/1400 [4:11:00<05:19, 10.65s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.34 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.16 ms /   211 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   243 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6003.06 ms /   319 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   201 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5266.60 ms /   672 tokens\n",
      " 98%|█████████▊| 1371/1400 [4:11:12<05:17, 10.95s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.98 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.02 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   339 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8455.79 ms /   412 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   191 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4997.56 ms /   659 tokens\n",
      " 98%|█████████▊| 1372/1400 [4:11:26<05:30, 11.82s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.98 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     178.04 ms /   202 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   289 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7178.52 ms /   357 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   105 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2871.62 ms /   567 tokens\n",
      " 98%|█████████▊| 1373/1400 [4:11:36<05:07, 11.40s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.07 ms /    74 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     192.12 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   305 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7580.76 ms /   371 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   149 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3962.34 ms /   610 tokens\n",
      " 98%|█████████▊| 1374/1400 [4:11:48<05:00, 11.57s/it]Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.93 ms /    88 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 213 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   213 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.66 ms /   216 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    82 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   260 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6455.55 ms /   342 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 476 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   476 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   130 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3521.89 ms /   606 tokens\n",
      " 98%|█████████▊| 1375/1400 [4:11:59<04:40, 11.21s/it]Llama.generate: 42 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   125 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     188.57 ms /   128 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 253 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   253 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.97 ms /   256 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 121 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   121 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   375 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    9416.41 ms /   496 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 516 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   516 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   267 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7043.55 ms /   783 tokens\n",
      " 98%|█████████▊| 1376/1400 [4:12:16<05:10, 12.92s/it]Llama.generate: 42 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    92 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     216.26 ms /    96 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 220 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   220 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     218.13 ms /   224 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 88 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    88 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   252 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6250.49 ms /   340 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 483 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   483 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   195 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5094.68 ms /   678 tokens\n",
      " 98%|█████████▊| 1377/1400 [4:12:27<04:49, 12.58s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.99 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     209.24 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   182 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4490.76 ms /   244 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    89 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2495.47 ms /   546 tokens\n",
      " 98%|█████████▊| 1378/1400 [4:12:35<04:02, 11.03s/it]Llama.generate: 42 prefix-match hit, remaining 172 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   172 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     225.75 ms /   176 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 300 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   300 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     244.55 ms /   303 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 169 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   169 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   284 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7179.56 ms /   453 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 563 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   563 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   244 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6439.47 ms /   807 tokens\n",
      " 98%|█████████▊| 1379/1400 [4:12:49<04:11, 11.96s/it]Llama.generate: 42 prefix-match hit, remaining 103 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   103 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     164.59 ms /   106 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 231 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   231 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     194.43 ms /   234 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 99 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    99 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   303 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7559.09 ms /   402 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 494 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   494 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   180 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4734.13 ms /   674 tokens\n",
      " 99%|█████████▊| 1380/1400 [4:13:02<04:03, 12.17s/it]Llama.generate: 42 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    76 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     175.77 ms /    80 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 204 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   204 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     217.86 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    72 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   322 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8018.92 ms /   394 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 467 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   467 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   102 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2797.08 ms /   569 tokens\n",
      " 99%|█████████▊| 1381/1400 [4:13:13<03:45, 11.89s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     223.19 ms /    71 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.12 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   261 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6472.99 ms /   325 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3066.96 ms /   570 tokens\n",
      " 99%|█████████▊| 1382/1400 [4:13:23<03:23, 11.32s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.16 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.30 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   226 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5556.72 ms /   288 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   196 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5130.47 ms /   653 tokens\n",
      " 99%|█████████▉| 1383/1400 [4:13:34<03:11, 11.24s/it]Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     166.18 ms /    68 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 192 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   192 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     201.27 ms /   196 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   290 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7200.90 ms /   350 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 455 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   455 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   103 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2816.71 ms /   558 tokens\n",
      " 99%|█████████▉| 1384/1400 [4:13:44<02:55, 10.99s/it]Llama.generate: 42 prefix-match hit, remaining 61 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    61 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     208.72 ms /    65 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 189 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   189 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.18 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   168 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4167.49 ms /   225 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 452 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   452 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   141 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3777.24 ms /   593 tokens\n",
      " 99%|█████████▉| 1385/1400 [4:13:53<02:33, 10.21s/it]Llama.generate: 42 prefix-match hit, remaining 60 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    60 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     163.64 ms /    64 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 188 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   188 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.67 ms /   192 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 55 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    55 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   280 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6934.86 ms /   335 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 451 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   451 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   112 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3012.49 ms /   563 tokens\n",
      " 99%|█████████▉| 1386/1400 [4:14:03<02:23, 10.25s/it]Llama.generate: 42 prefix-match hit, remaining 66 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    66 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     147.14 ms /    69 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 194 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   194 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.47 ms /   197 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   347 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    8666.69 ms /   409 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 457 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   457 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   154 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4096.59 ms /   611 tokens\n",
      " 99%|█████████▉| 1387/1400 [4:14:16<02:24, 11.11s/it]Llama.generate: 42 prefix-match hit, remaining 70 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    70 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     176.26 ms /    73 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 198 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   198 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     189.52 ms /   201 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   280 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6957.31 ms /   347 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 461 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   461 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   177 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4649.46 ms /   638 tokens\n",
      " 99%|█████████▉| 1388/1400 [4:14:28<02:16, 11.38s/it]Llama.generate: 42 prefix-match hit, remaining 89 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    89 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     161.29 ms /    92 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 217 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   217 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     190.41 ms /   220 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 85 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    85 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   414 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   10403.94 ms /   499 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 480 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   480 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   244 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6370.66 ms /   724 tokens\n",
      " 99%|█████████▉| 1389/1400 [4:14:45<02:24, 13.11s/it]Llama.generate: 42 prefix-match hit, remaining 58 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    58 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.86 ms /    62 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 186 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   186 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     171.55 ms /   189 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 54 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    54 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   194 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4755.48 ms /   248 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 449 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   449 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   118 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3196.30 ms /   567 tokens\n",
      " 99%|█████████▉| 1390/1400 [4:14:54<01:56, 11.68s/it]Llama.generate: 42 prefix-match hit, remaining 71 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    71 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     181.34 ms /    75 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 199 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   199 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     213.44 ms /   203 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   196 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    4855.08 ms /   264 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 462 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   462 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   143 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3785.91 ms /   605 tokens\n",
      " 99%|█████████▉| 1391/1400 [4:15:03<01:38, 10.89s/it]Llama.generate: 42 prefix-match hit, remaining 67 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    67 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     148.89 ms /    70 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 195 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   195 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     187.13 ms /   198 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   207 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5096.88 ms /   269 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 458 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   458 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2733.71 ms /   557 tokens\n",
      " 99%|█████████▉| 1392/1400 [4:15:11<01:20, 10.08s/it]Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     159.49 ms /    81 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 205 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   205 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.57 ms /   208 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    74 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   294 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7306.63 ms /   368 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 468 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   122 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3327.89 ms /   590 tokens\n",
      "100%|█████████▉| 1393/1400 [4:15:22<01:12, 10.36s/it]Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     182.77 ms /    84 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 208 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   208 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     215.84 ms /   212 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   212 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5274.82 ms /   289 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 471 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   471 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   130 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3518.84 ms /   601 tokens\n",
      "100%|█████████▉| 1394/1400 [4:15:31<01:00, 10.02s/it]Llama.generate: 42 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    94 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     184.48 ms /    98 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 222 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   222 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     220.34 ms /   226 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    90 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   231 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5763.59 ms /   321 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 485 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   485 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   150 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3994.85 ms /   635 tokens\n",
      "100%|█████████▉| 1395/1400 [4:15:41<00:50, 10.07s/it]Llama.generate: 42 prefix-match hit, remaining 68 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    68 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     156.38 ms /    72 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 196 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   196 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     215.36 ms /   200 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 64 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    64 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   273 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6770.66 ms /   337 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 459 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   459 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   145 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3854.72 ms /   604 tokens\n",
      "100%|█████████▉| 1396/1400 [4:15:52<00:41, 10.36s/it]Llama.generate: 42 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    73 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     196.82 ms /    77 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 201 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   201 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     186.02 ms /   204 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 69 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    69 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   257 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6419.29 ms /   326 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 464 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   464 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   137 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3673.51 ms /   601 tokens\n",
      "100%|█████████▉| 1397/1400 [4:16:03<00:31, 10.40s/it]Llama.generate: 42 prefix-match hit, remaining 164 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   164 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     255.16 ms /   168 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 292 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   292 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     269.92 ms /   296 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 160 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   160 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   244 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6212.28 ms /   404 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 555 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   555 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   200 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    5313.69 ms /   755 tokens\n",
      "100%|█████████▉| 1398/1400 [4:16:15<00:21, 10.91s/it]Llama.generate: 42 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     162.74 ms /    87 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 212 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   212 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     185.72 ms /   215 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    80 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   315 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    7882.18 ms /   395 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 475 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   475 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   138 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3742.15 ms /   613 tokens\n",
      "100%|█████████▉| 1399/1400 [4:16:27<00:11, 11.23s/it]Llama.generate: 42 prefix-match hit, remaining 62 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    62 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     4 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     195.89 ms /    66 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 190 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   190 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     3 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     180.61 ms /   193 tokens\n",
      "Llama.generate: 42 prefix-match hit, remaining 57 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    57 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   250 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6176.72 ms /   307 tokens\n",
      "Llama.generate: 60 prefix-match hit, remaining 453 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     750.64 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   453 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   139 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    3699.82 ms /   592 tokens\n",
      "100%|██████████| 1400/1400 [4:16:37<00:00, 11.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "FINAL RESULTS REPORTs\n",
      "==============================\n",
      "\n",
      "Technique: Zero-Shot\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Not Offensive (0)       0.79      0.92      0.85       700\n",
      "    Offensive (1)       0.91      0.75      0.82       700\n",
      "\n",
      "         accuracy                           0.84      1400\n",
      "        macro avg       0.85      0.84      0.84      1400\n",
      "     weighted avg       0.85      0.84      0.84      1400\n",
      "\n",
      "Accuracy: 0.8364\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Technique: Few-Shot\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Not Offensive (0)       0.95      0.73      0.83       700\n",
      "    Offensive (1)       0.78      0.96      0.86       700\n",
      "\n",
      "         accuracy                           0.84      1400\n",
      "        macro avg       0.86      0.84      0.84      1400\n",
      "     weighted avg       0.86      0.84      0.84      1400\n",
      "\n",
      "Accuracy: 0.8450\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Technique: CoT-Zero-Shot\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Not Offensive (0)       0.62      0.70      0.66       700\n",
      "    Offensive (1)       0.66      0.57      0.61       700\n",
      "\n",
      "         accuracy                           0.63      1400\n",
      "        macro avg       0.64      0.63      0.63      1400\n",
      "     weighted avg       0.64      0.63      0.63      1400\n",
      "\n",
      "Accuracy: 0.6343\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Technique: CoT-Few-Shot\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Not Offensive (0)       0.72      0.96      0.83       700\n",
      "    Offensive (1)       0.95      0.63      0.76       700\n",
      "\n",
      "         accuracy                           0.80      1400\n",
      "        macro avg       0.83      0.80      0.79      1400\n",
      "     weighted avg       0.83      0.80      0.79      1400\n",
      "\n",
      "Accuracy: 0.7979\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Execution Loop\n",
    "\n",
    "LLM = llama\n",
    "N_SAMPLES = 1400\n",
    "subset_test = test_df.sample(n=N_SAMPLES, random_state=42)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "results = {\n",
    "    \"Zero-Shot\": [],\n",
    "    \"Few-Shot\": [],\n",
    "    \"CoT-Zero-Shot\": [],\n",
    "    \"CoT-Few-Shot\": []\n",
    "}\n",
    "true_labels = subset_test['label'].tolist()\n",
    "\n",
    "for index, row in tqdm(subset_test.iterrows(), total=N_SAMPLES):\n",
    "    text = row['text']\n",
    "    \n",
    "    # 1. Zero Shot\n",
    "    r_zero = query_llm(create_zero_shot_prompt(text), LLM)\n",
    "    results[\"Zero-Shot\"].append(parse_response(r_zero))\n",
    "    \n",
    "    # 2. Few Shot\n",
    "    r_few = query_llm(create_few_shot_prompt(text, train_df), LLM)\n",
    "    results[\"Few-Shot\"].append(parse_response(r_few))\n",
    "\n",
    "    # 3. CoT Zero Shot\n",
    "    r_cot_zs = query_llm(create_cot_prompt_zs(text), LLM)\n",
    "    results[\"CoT-Zero-Shot\"].append(parse_response(r_cot_zs))\n",
    "\n",
    "    # 4. CoT Few Shot\n",
    "    r_cot_fs = query_llm(create_cot_prompt_fs(text), LLM)\n",
    "    results[\"CoT-Few-Shot\"].append(parse_response(r_cot_fs))\n",
    "\n",
    "\n",
    "# Final Report\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"FINAL RESULTS REPORTs\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "target_names = ['Not Offensive (0)', 'Offensive (1)']\n",
    "\n",
    "for technique, preds in results.items():\n",
    "    print(f\"\\nTechnique: {technique}\")\n",
    "    \n",
    "    # Handle parsing errors (-1) by treating them as errors (mismatches)\n",
    "    clean_preds = [p if p != -1 else (1 if true_labels[i] == 0 else 0) for i, p in enumerate(preds)]\n",
    "    \n",
    "    print(classification_report(true_labels, clean_preds, target_names=target_names))\n",
    "    print(f\"Accuracy: {accuracy_score(true_labels, clean_preds):.4f}\")\n",
    "\n",
    "    # Confusion Matrix Heatmap for each technique report\n",
    "    cm = confusion_matrix(true_labels, clean_preds)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=target_names,\n",
    "                yticklabels=target_names)\n",
    "\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(f\"Final Confusion Matrix: {technique}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
